{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topic Modeling Algorithms\n",
    "\n",
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook provides a table of contents with a summarized overview of each baseline topic modeling algorithm.  They are organized in descending chronological order of which topic modeling algorithm was published earliest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is a generative probabilistic model that assigns a set of topics to a corpus of documents using Dirichlet distributions as priors.  The model performs well on documents with formal grammatical style and long text lengths but generally performs poorly on short texts with inconsistent grammatical style.  This is the baseline algorithm for topic extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further details, please refer to the following Jupyter Notebook files:\n",
    "\n",
    "[LDA Introduction](notebooks/slo-lda-introduction.ipynb#bookmark) - presents the plate notation diagram and pseudocode for the algorithm as well as providing a simple hand-worked example of a single iteration of the algorithm.\n",
    "\n",
    "[LDA Implementation](notebooks/slo-lda-implementation.ipynb#bookmark) - presents the codebase and implementation of the algorithm on our Twitter dataset.\n",
    "\n",
    "[LDA Grid Search](notebooks/slo-lda-grid-search.ipynb#bookmark) - presents the codebase for implementing an exhaustive grid search for hyperparameter tuning for the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources Referenced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://delivery.acm.org/10.1145/950000/944937/3-993-blei.pdf?ip=153.106.81.45&id=944937&acc=OPEN&key=B5D9E165A72B697C%2E89FDE39119A3A240%2E4D4702B0C3E38B35%2E6D218144511F3437&__acm__=1563211889_7657963b7fdc3ceb86b5b1155986ddcf\n",
    "    - original paper detailing LDA.\n",
    "    \n",
    "    \n",
    "- https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158<br>\n",
    "    - Utilized two diagrams, formula, and explanation of associated notation on LDA's.<br>\n",
    "\n",
    "\n",
    "- https://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/<br>\n",
    "    - Utilized blog's example as the basis for the explanation of the LDA algorithm pseudocode.<br>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HLDA is an extension of the LDA algorithm.  LDA is implemented on the original corpus of documents resulting in the usual document-topic and word-topic assignments.  Then, \"synthetic\" documents are created from the word-topic assignments for each document-topic assignment and are grouped into \"synthetic\" corpuses by topic.  LDA is implemented recursively on the \"synthetic\" corpuses until \"synthetic\" documents and corpuses are no longer able to be generated.  For each loop of the recursion, a hierarchy of topic distributions are generated, resulting in a tree-like structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further details, please refer to the following Jupyter Notebook files:\n",
    "\n",
    "[HLDA Introduction](notebooks/slo-hlda-introduction.ipynb#bookmark)  - presents the plate notation diagram and pseudocode for the algorithm as well as providing a simple hand-worked example of a single iteration of the algorithm.\n",
    "\n",
    "[HLDA Implementation](notebooks/slo-hlda-implementation.ipynb#bookmark) - presents the codebase and implementation of the algorithm on our Twitter dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources Referenced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://dl.acm.org/citation.cfm?id=2981348\n",
    "    - provides publication date of article.\n",
    "\n",
    "\n",
    "- https://papers.nips.cc/paper/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.pdf\n",
    "    - original paper by on Hierarchical Topic Modeling by Blei.\n",
    "\n",
    "\n",
    "- https://www.aclweb.org/anthology/W14-3111\n",
    "    - has a nice overview of the HLDA algorithm with image.\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author-Topic Model (a.k.a. LDA-u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author-topic model is an extension of the LDA algorithm.  It combines both the Author model and the LDA topic model into the Author-Topic (LDA) model.  Whereas normal LDA generates a document-topic and word-topic distribution, the author-topic model generates a author-topic and word-topic distribution (there is no longer a document-topic distribution in use).  The result is a probabilistic model that assigns topics to authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further details, please refer to the following Jupyter Notebook files:\n",
    "\n",
    "[Author-Topic Introduction](notebooks/slo-author-topic-introduction.ipynb#bookmark)  - presents the plate notation diagram and pseudocode for the algorithm as well as providing a simple hand-worked example of a single iteration of the algorithm.\n",
    "\n",
    "[Author-Topic Implementation](notebooks/slo-author-topic-implementation.ipynb#bookmark) - presents the codebase and implementation of the algorithm on our Twitter dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources Referenced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://dl.acm.org/citation.cfm?id=1036902\n",
    "    - provides publication date of article.\n",
    "\n",
    "\n",
    "- https://mimno.infosci.cornell.edu/info6150/readings/398.pdf\n",
    "    - original paper on the author-topic model.\n",
    "    \n",
    "    \n",
    "- https://www.slideshare.net/FREEZ7/author-topic-model?from_action=save\n",
    "    - nice slides explaining the author-topic model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Dirichlet Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDP is a generative probabilistic model that is similar to LDA except that the number of assigned topics is not a hyperparameter.  The set of topics themselves is a random (latent) variable that is generated via Dirichlet processes and there is no upper bound on the number of generated topics.  The \"hierarchical\" part of the name refers to a global set of topics shared among the entire corpus of documents from which the local set of topics assigned to each document is drawn from.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further details, please refer to the following Jupyter Notebook files:\n",
    "\n",
    "[HDP Introduction](notebooks/slo-hdp-introduction.ipynb#bookmark)  - presents the plate notation diagram and pseudocode for the algorithm as well as providing a simple hand-worked example of a single iteration of the algorithm.\n",
    "\n",
    "[HDP Implementation](notebooks/slo-hdp-implementation.ipynb#bookmark) - presents the codebase and implementation of the algorithm on our Twitter dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources Referenced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://amstat.tandfonline.com/doi/abs/10.1198/016214506000000302\n",
    "    - online citation of the original paper on HDP.\n",
    "\n",
    "\n",
    "- https://people.eecs.berkeley.edu/~jordan/papers/hdp.pdf\n",
    "    - original paper on HDP.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biterm Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biterms are unordered word-pair combinations within a text.  The biterm model resolves the data sparsity issue with short texts by modeling biterms across the entire corpus of documents instead of individual words within a document.  Each biterm is associated with a single topic whereas in LDA each word can be associated with multiple topics.  The model infers the topic of each document using the topics its biterms are associated with.  Hypothetically, topic extraction is easier with this model as inferring the topic of a biterm is easier with the added context provided using word co-occurrences over individual words with no context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further details, please refer to the following Jupyter Notebook files:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Biterm Introduction](notebooks/slo-biterm-introduction.ipynb#bookmark)  - presents the plate notation diagram and pseudocode for the algorithm as well as providing a simple hand-worked example of a single iteration of the algorithm.\n",
    "\n",
    "[Biterm Implementation](notebooks/slo-biterm-implementation.ipynb#bookmark) - presents the codebase and implementation of the algorithm on our Twitter dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources Referenced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://dl.acm.org/citation.cfm?id=2488514\n",
    "    - provides publication date for article.\n",
    "\n",
    "\n",
    "- https://www.researchgate.net/publication/262244963_A_biterm_topic_model_for_short_texts\n",
    "    - original paper on Biterm model.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
