{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topic Modeling Algorithms\n",
    "\n",
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook provides a table of contents with a summarized overview of each baseline topic modeling algorithm.  They are organized in descending chronological order of which topic modeling algorithm was published earliest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "LDA is a generative probabilistic model that assigns a set of topics to a corpus \n",
    "of documents using Dirichlet distributions as priors.  It is the baseline \n",
    "algorithm for topic extraction. \n",
    "\n",
    "For further details, please refer to the following Jupyter Notebook files:\n",
    "\n",
    "- [LDA Introduction](notebooks/slo-lda-introduction.ipynb#bookmark) - presents the plate notation diagram and pseudocode for the algorithm as well as providing a simple hand-worked example of a single iteration of the algorithm.\n",
    "\n",
    "- [LDA Implementation](notebooks/slo-lda-implementation.ipynb#bookmark) - presents the codebase and implementation of the algorithm on our Twitter dataset.\n",
    "\n",
    "- [LDA Grid Search](notebooks/slo-lda-grid-search.ipynb#bookmark) - presents the codebase for implementing an exhaustive grid search for hyperparameter tuning for the algorithm. This currently doesn't work and would require configuring a faster machine to run the full search.\n",
    "\n",
    "The model performs well on documents with formal \n",
    "grammatical style and long text lengths but generally performs poorly on short \n",
    "texts with inconsistent grammatical style and, in particular, on tweets in the \n",
    " SLO dataset.\n",
    "\n",
    "### Resources Referenced\n",
    "\n",
    "- http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf\n",
    "    - original paper detailing LDA.\n",
    "    \n",
    "- https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158<br>\n",
    "    - Utilized two diagrams, formula, and explanation of associated notation on LDA's.<br>\n",
    "\n",
    "- https://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/<br>\n",
    "    - Utilized blog's example as the basis for the explanation of the LDA algorithm pseudocode.<br>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Latent Dirichlet Allocation\n",
    "\n",
    "HLDA is a recursive extension of the LDA algorithm that produces a hierarchy of topics and sub-topics.  LDA is implemented on the original corpus of documents resulting in the usual document-topic and word-topic assignments.  Then, \"synthetic\" documents are created from the word-topic assignments for each document-topic assignment and are grouped into \"synthetic\" corpora by topic.  LDA is implemented recursively on the \"synthetic\" corpora until \"synthetic\" documents and corpora are no longer able to be generated.  For each level of the recursion, a hierarchy of topic distributions are generated, resulting in a tree-like structure. \n",
    "\n",
    "For further details, please refer to the following Jupyter Notebook files:\n",
    "\n",
    "- [HLDA Introduction](notebooks/slo-hlda-introduction.ipynb#bookmark)  - presents the plate notation diagram and pseudocode for the algorithm as well as providing a simple hand-worked example of a single iteration of the algorithm.\n",
    "\n",
    "- [HLDA Implementation](notebooks/slo-hlda-implementation.ipynb#bookmark) - presents the codebase and implementation of the algorithm on our Twitter dataset. This currently doesn't work on the full SLO dataset beyond the first level of recursion due to memory overflow.\n",
    "\n",
    "This model performs well on the same tasks as LDA and has the same problems. It's also unclear how valuable a hierarchy of topics would be in the SLO application.\n",
    "\n",
    "### Resources Referenced\n",
    "\n",
    "- https://dl.acm.org/citation.cfm?id=2981348\n",
    "    - provides publication date of article.\n",
    "\n",
    "- https://papers.nips.cc/paper/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.pdf\n",
    "    - original paper by on Hierarchical Topic Modeling by Blei.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author-Topic Model (a.k.a. LDA-u)\n",
    "\n",
    "The author-topic model is an extension of the LDA algorithm. Whereas \n",
    "the normal LDA comprises a document-topic distribution and a word-topic \n",
    "distribution, the \n",
    "author-topic model replaces the document-topic distribution with an author-topic \n",
    "distribution.  The result is a \n",
    "probabilistic model that assigns topics to authors.\n",
    "\n",
    "For further details, please refer to the following Jupyter Notebook files:\n",
    "\n",
    "- [Author-Topic Introduction](notebooks/slo-author-topic-introduction.ipynb#bookmark)  - presents the plate notation diagram and pseudocode for the algorithm as well as providing a simple hand-worked example of a single iteration of the algorithm.\n",
    "\n",
    "- [Author-Topic Implementation](notebooks/slo-author-topic-implementation.ipynb#bookmark) - presents the codebase and implementation of the algorithm on our Twitter dataset.\n",
    "\n",
    "This model produces somewhat better word custers for topics, but they are still \n",
    "difficult to understand. It's possible that one could focus more on authors than on \n",
    "documents, which would give a better estimate of the number of people with particular\n",
    "views rather than the number of tweets. \n",
    "\n",
    "### Resources Referenced\n",
    "\n",
    "- https://dl.acm.org/citation.cfm?id=1036902\n",
    "    - provides publication date of article.\n",
    "\n",
    "- https://mimno.infosci.cornell.edu/info6150/readings/398.pdf\n",
    "    - original paper on the author-topic model.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Dirichlet Process\n",
    "\n",
    "HDP is a generative probabilistic model that is similar to LDA except that the number of assigned topics is not a hyperparameter.  The set of topics themselves is a random (latent) variable that is generated via Dirichlet processes and there is no upper bound on the number of generated topics.  The \"hierarchical\" part of the name refers to a global set of topics shared among the entire corpus of documents from which the local set of topics assigned to each document is drawn from.  \n",
    "\n",
    "For further details, please refer to the following Jupyter Notebook files:\n",
    "\n",
    "- [HDP Introduction](notebooks/slo-hdp-introduction.ipynb#bookmark)  - presents the plate notation diagram and pseudocode for the algorithm as well as providing a simple hand-worked example of a single iteration of the algorithm.\n",
    "\n",
    "- [HDP Implementation](notebooks/slo-hdp-implementation.ipynb#bookmark) - presents the codebase and implementation of the algorithm on our Twitter dataset.\n",
    "\n",
    "### Resources Referenced\n",
    "\n",
    "- https://amstat.tandfonline.com/doi/abs/10.1198/016214506000000302\n",
    "    - online citation of the original paper on HDP.\n",
    "\n",
    "- https://people.eecs.berkeley.edu/~jordan/papers/hdp.pdf\n",
    "    - original paper on HDP.\n",
    "\n",
    "- http://proceedings.mlr.press/v15/wang11a/wang11a.pdf\n",
    "    - Gensim model is based off this article specifically.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biterm Model\n",
    "\n",
    "Biterms are unordered word-pair combinations within a text.  The biterm model resolves the data sparsity issue with short texts by modeling biterms across the entire corpus of documents instead of individual words within a document.  Each biterm is associated with a single topic whereas in LDA each word can be associated with multiple topics.  The model infers the topic of each document using the topics its biterms are associated with.  Hypothetically, topic extraction is easier with this model as inferring the topic of a biterm is easier with the added context provided using word co-occurrences over individual words with no context.\n",
    "\n",
    "For further details, please refer to the following Jupyter Notebook files:\n",
    "\n",
    "- [Biterm Introduction](notebooks/slo-biterm-introduction.ipynb#bookmark)  - presents the plate notation diagram and pseudocode for the algorithm as well as providing a simple hand-worked example of a single iteration of the algorithm.\n",
    "\n",
    "- [Biterm Implementation](notebooks/slo-biterm-implementation.ipynb#bookmark) - presents the codebase and implementation of the algorithm on our Twitter dataset.\n",
    "\n",
    "### Resources Referenced\n",
    "\n",
    "- https://dl.acm.org/citation.cfm?id=2488514\n",
    "    - provides publication date for article.\n",
    "\n",
    "- https://www.researchgate.net/publication/262244963_A_biterm_topic_model_for_short_texts\n",
    "    - original paper on Biterm model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization\n",
    "\n",
    "Non-negative matrix factorization is a general group of algorithms used in multi-variate analysis.  The \"non-negative\" part of the name refers to the condition that no negative terms are permitted for any element of the matrices.  Given a matrix \"V\", V is factorized into two matrices \"W\" and \"H\".  This factorization is repeated and it is the goal for the reuslting matrices to have significantly reduced dimensionality.  According to Wikipedia, NMF has an inherent clustering property and cluster membership is automatically assigned during the computation process.  Hence, NMF can be used for topic modeling of textual data.\n",
    "\n",
    "For further details, please refer to the following Jupyter Notebook files:\n",
    "\n",
    "- [NMF Introduction](notebooks/slo-nmf-introduction.ipynb#bookmark)  - presents the plate notation diagram and pseudocode for the algorithm as well as providing a simple hand-worked example of a single iteration of the algorithm.\n",
    "\n",
    "- [NMF Implementation](notebooks/slo-nmf-implementation.ipynb#bookmark) - presents the codebase and implementation of the algorithm on our Twitter dataset.\n",
    "\n",
    "### Resources Referenced\n",
    "\n",
    "- https://arxiv.org/abs/1604.02634\n",
    "    - provides publication date of article.\n",
    "\n",
    "- https://arxiv.org/pdf/1604.02634.pdf\n",
    "    - Gensim NMF Class is based off of this particular paper's algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}