{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "</p>Latent Dirichlet Allocation is a probabilistic method of text analysis for topic modeling.  This method identifies the topics that exist within a set of documents and maps those documents to their associated topics.  The process uses a bag-of-words feature representation for the documents of interest.  In LDA's, each document is described by a distribution of topics and each topic is described by a distribution of words.  There are two primary components to LDA's.  The observed layers are the documents (also called composites) and the words that comprise those documents (the parts).  The hidden (or latent) layer consists of the topics (also called categories) as well as the various variables utilized by the algorithm.  The output of the algorithm is a list of the topics associated with the entire set of documents and the top words associated with each topic.  These topics are indexed values assigned integer values to which they are later assigned English descriptors to describe those topics.</p>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plate Notation for the LDA Algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "</p>Plate notation is a useful method for displaying variables that repeat via a graphical model. (\"Plate notation\" 2019)  The LDA algorithm contains variables that are repeated within 3 nested for loops so we find it useful to depict them in this form.  Each rectangle (plate) is a subgraph of grouped variables that indicate that those variables are repeated simultaneously.  The variables are represented as circles while the notation in the lower right hand corner of each rectangle (plate) indicate that it is repeated \"M\" or \"k\" times, for example.  In this case, the entire diagram represents the observed/latent variables and parameters utilized in LDA and their interdependency as indicated by the directed edges.  The list below describes the purpose of each variable and notation in the plate diagram. (Ganegedara & Ganegedara \"Intuitive Guide to Latent Dirichlet Allocation\" 2018)</p><br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lda](images/lda_model.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "k — Number of topics a document belongs to (a fixed number).<br>\n",
    "\n",
    "V — Size of the vocabulary.<br>\n",
    "\n",
    "M — Number of documents.<br>\n",
    "\n",
    "N — Number of words in each document.<br>\n",
    "\n",
    "w — A word in a document. This is represented as one hot encoded vector of size V (i.e. V — vocabulary size).<br>\n",
    "\n",
    "z — A topic from a set of k topics. A topic is a distribution of words. For example it might be, Animal = (0.3 Cats, 0.4 Dogs, 0 AI, 0.2 Loyal, 0.1 Evil).<br>\n",
    "\n",
    "α — Distribution related parameter that governs what the distribution of topics is for all the documents in the corpus looks like.<br>\n",
    "\n",
    "η — Distribution related parameter that governs what the distribution of words in each topic looks like.<br>\n",
    "\n",
    "θ — Random matrix where θ(i,j) represents the probability of the i th document to containing the j th topic.<br>\n",
    "\n",
    "β — A random matrix where β(i,j) represents the probability of i th topic containing the j th word.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "</p>The plate notation above represents the algorithm in a graphical format. α is the parameter for the Dirichlet distribution prior that influences the topic-document distribution described by θ. η is the parameter for the Dirichlet distribution prior that influences the word-topic distribution described by β.  While shown as a constant value below, alpha and eta are actually 1-d vectors with the length determined by the set of topics (k) that we specify.</p><br><br>\n",
    "\n",
    "</p>The largest plate surrounds all the variables related to a single document in the set of documents (M) that comprise the corpus of interest.  The plate indicates that the variables contained within are repeated M times, once for each document, which also represents a for loop in the pseudocode for the algorithm.</p><br><br>\n",
    "\n",
    "</p>The smaller plate within the largest plate surround all the variables related to a single word within a single document.  The plate indicates that the variables contained within are repeated N times, once for each word for the N words that comprise each of the M documents.  This smaller plate represents a nested for loop within the outer for loop represented by the largest plate.</p><br><br>\n",
    "\n",
    "</p>Within the smaller plate, the variable \"z\" represents a single topic chosen from the topic distribution which represents the distribution of words that belong to that topic.  The variable \"w\" represents the the actual word itself.</p><br><br>\n",
    "    \n",
    "</p>\"w\" is shaded because it is an observed variable belonging to the observed layer.  All other variables are unshaded as they belong to the latent (hidden) layer that cannot be directly observed.</p><br><br>\n",
    "\n",
    "</p>The directed edges between each circle representing each variable indicate dependencies between the variables.  The variable at the head of the edges depends on the variable at the tail of the edges.</p><br><br>\n",
    "\n",
    "</p>The topmost plate surrounds the β word-topic distribution and indicates a for loop where we determine the word-topic distribution for each topic in the set of topics (k).  This is similar to the largest plate surrounding the θ topic-document distribution where there is a for loop that determines the topic-document distribution for each document in the set of documents (M). </p><br><br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Formula for Calculating \"w\" in LDA algorithm):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "The formula below summarizes the plate notation for the LDA algorithm as a probabilistic statement.  It encapsulates the idea that at each step of the algorithm all the latent (hidden) variables are updated and the revised values are then used to calculate the probability of the word (w) being assigned a given topic.<br>  \n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mathematical_model](images/lda_equation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Given a set of M documents each containing N words with each word generated by a topic from a set of k topics, find the joint posterior probability of the theta (θ) distribution, beta distribution (β), and topic (z) given the data (D) using the prior parameters alpha (α) and eta (η). (Ganegedara & Ganegedara \"Intuitive Guide to Latent Dirichlet Allocation\" 2018)\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Joint posterior probability: \n",
    "\n",
    "The revised or updated probability of an event occurring given new information.<br>\n",
    "Calculated by updating the prior probability using Bayes' Theorem.<br>\n",
    "In other words, conditional probability - the probability of event A occurring given that event B has occurred.<br>\n",
    "\n",
    "Prior probability:\n",
    "\n",
    "The probability of an event occurring before new information is given.<br>\n",
    "Calculated using Bayes' Theorem.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dirichlet Distribution Visualization (example):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Dirichlet distributions are a family of continuous multivariate probability distributions parameterized by a vector (in our case α and η) of positive real numbers. (\"Dirichlet distribution\" 2019)  They are often used as priors in Bayesian statistics and here they are used as priors for the theta (document-topic) and beta (topic-word) distributions used in the LDA algorithm.  Large values of α and η push the distribution to the center whereas small values of α and η push the distribution to the edges.  Optimally, we desire values that result in a distribution similar to the one displayed in the top middle.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dirichlet](images/dirichlet_distribution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "</p>The graphs above visualize Dirichlet Distributions using 3 topics (k = 3).  The values for α (alpha) and η (eta) influence the shape of the graphs.  By shape, we mean the shape of the probability density function that determines the θ and β distributions.  In this example, the graph is 3-d because we have k = 3 topics.  As k increases, the graphs would become k-dimensional Dirichlet Distribution graphs.</p><br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode for the LDA algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "Assign topic (z) to each word (w) in each document (d) (randomly or based on some probabilistic distribution)\n",
    "\n",
    "while(NOT exhausted time constraints)\n",
    "\n",
    "    for each document (d)\n",
    "        for each word (w)\n",
    "            for each topic(z)\n",
    "\n",
    "                Compute Probability(topic (z) | document (d))\n",
    "                Compute Probability(word (w) | topic (z))\n",
    "\n",
    "            Assign a new topic (z') to word (W) in the document (d) (based on the selection using computed probabilities).\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "</p>The algorithm for Latent Dirichlet Allocation iteratively assigns a topic to each word in each document based on the computed conditional probabilities of a topic belonging to a document and a word belonging to a topic.  This is repeated until the allocated computation time is exhausted.</p><br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simplified Latent Dirichlet Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics for our example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Topics (k=2) |\n",
    "|--------------|\n",
    "| Topic 1      |\n",
    "| Topic 2      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "It should be noted that the topics in an LDA model are actually just indexed (integer) values from 0-Z and not actually described by any sort of noun, verb, etc.  We later assign \"food\" and \"animals\" as the descriptors for the two topics as we see that the top N words for each indexed topic are strongly associated with those descriptors.  The # of topics and # of top words for each topic are determined by hyperparameter settings set by the user.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial topic assignment for each word in each document:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|    Documents (M = 5, N = 3)    |   Word 1   |  Word 2  |  Word 3  |   |\n",
    "|:------------------------------:|:----------:|:--------:|:--------:|:-:|\n",
    "| Doc 1 Word Topic Assignment--> |      1     |     2    |     1    |   |\n",
    "|           Document 1           |     eat    | broccoli |  banana  |   |\n",
    "| Doc 2 Word Topic Assignment--> |      2     |     1    |     2    |   |\n",
    "|           Document 2           |   banana   |  spinach |   lunch  |   |\n",
    "| Doc 3 Word Topic Assignment--> |      1     |     2    |     1    |   |\n",
    "|           Document 3           | chinchilla |  kitten  |   cute   |   |\n",
    "| Doc 4 Word Topic Assignment--> |      2     |     1    |     2    |   |\n",
    "|           Document 4           |   sister   |  kitten  |   today  |   |\n",
    "| Doc 5 Word Topic Assignment--> |      1     |     2    |     1    |   |\n",
    "|           Document 5           |   hamster  |    eat   | broccoli |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "The above is step 1 in the LDA algorithm pseudocode.  For the purposes of this example, we simply randomly assign a topic to each word for each document rather than use a probabilistic distribution.<br>\n",
    "\n",
    "M = 5 indicates that we have five documents in total.<br>\n",
    "N =3 indicates that we have 3 words per document.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The list of unique words in our vocabulary (V):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Words (V = 11) |\n",
    "|----------------|\n",
    "| eat            |\n",
    "| broccoli       |\n",
    "| banana         |\n",
    "| spinach        |\n",
    "| lunch          |\n",
    "| chinchilla     |\n",
    "| kitten         |\n",
    "| cute           |\n",
    "| sister         |\n",
    "| today          |\n",
    "| hamster        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "The above is all the unique words in our vocabulary across all documents.  These are the words for which we will assign topics to based on our set of topics (k).<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the β (Beta) Distribution:\n",
    "<br>\n",
    "β — A distribution of words, one for each topic.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-kiyi{font-weight:bold;border-color:inherit;text-align:left}\n",
    ".tg .tg-u0o7{font-weight:bold;text-decoration:underline;border-color:inherit;text-align:left;vertical-align:top}\n",
    ".tg .tg-xldj{border-color:inherit;text-align:left}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    ".tg .tg-fymr{font-weight:bold;border-color:inherit;text-align:left;vertical-align:top}\n",
    ".tg .tg-xwhs{font-weight:bold;text-decoration:underline;border-color:inherit;text-align:left}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-xldj\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-u0o7\">Words</td>\n",
    "    <td class=\"tg-fymr\">eat</td>\n",
    "    <td class=\"tg-fymr\">broccoli</td>\n",
    "    <td class=\"tg-fymr\">banana</td>\n",
    "    <td class=\"tg-fymr\">spinach</td>\n",
    "    <td class=\"tg-fymr\">lunch</td>\n",
    "    <td class=\"tg-fymr\">chinchilla</td>\n",
    "    <td class=\"tg-fymr\">kitten</td>\n",
    "    <td class=\"tg-fymr\">cute</td>\n",
    "    <td class=\"tg-fymr\">sister</td>\n",
    "    <td class=\"tg-fymr\">today</td>\n",
    "    <td class=\"tg-fymr\">hamster</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xwhs\">Topics</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-kiyi\">1</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-fymr\">2</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "To compute the Beta distribution, we look at our initial topic assignment for each word in each document.<br>\n",
    "\n",
    "We count the # of times each word is associated with a particular topic across all documents.<br>\n",
    "\n",
    "For example, we see here that the word \"eat\" appears two times in total.  The first time \"eat\" appears, it is associated with topic 1.  The second time \"eat\" appears, it is associated with topic 2.<br>\n",
    "\n",
    "Therefore, we put a 1 in the cell corresponding to Topic 1 and the Word \"eat\" and we also put a 1 in the cell corresponding to Topic 2 and the Word \"eat\".<br>\n",
    "\n",
    "We do this for each word (w) in our vocabulary (V) across all documents (d) based on our initial topic assignment for each word in each document.<br>\n",
    "\n",
    "Note: \"placeholder\" simply means that we are not inputting an actual value for the sake of simplicity in this example.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the θ (Theta) Distribution:\n",
    "<br>\n",
    "θ — A distribution of topics, one for each document.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-kiyi{font-weight:bold;border-color:inherit;text-align:left}\n",
    ".tg .tg-xldj{border-color:inherit;text-align:left}\n",
    ".tg .tg-xwhs{font-weight:bold;text-decoration:underline;border-color:inherit;text-align:left}\n",
    ".tg .tg-fymr{font-weight:bold;border-color:inherit;text-align:left;vertical-align:top}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-xldj\"></th>\n",
    "    <th class=\"tg-xwhs\">Documents</th>\n",
    "    <th class=\"tg-kiyi\">1</th>\n",
    "    <th class=\"tg-fymr\">2</th>\n",
    "    <th class=\"tg-fymr\">3</th>\n",
    "    <th class=\"tg-fymr\">4</th>\n",
    "    <th class=\"tg-fymr\">5</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xwhs\">Topics</td>\n",
    "    <td class=\"tg-xldj\"></td>\n",
    "    <td class=\"tg-xldj\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-kiyi\">1</td>\n",
    "    <td class=\"tg-xldj\"></td>\n",
    "    <td class=\"tg-xldj\">2</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-fymr\">2</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">2</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "To compute the Theta distribution, we look at our initial topic assignment for each word in each document.<br>\n",
    "\n",
    "We count the # of times each document is associated with each topic in our set of topics.<br>\n",
    "\n",
    "Since we have three words per document, we see that Document 1 is associated with Topic 1 two times since two words are associated with Topic 1.  We also see that Document 1 is associated with Topic 2 one time since one word is associated with Topic 2.<br>\n",
    "\n",
    "Therefore, we put a 2 in the cell corresponding to Topic 1 and Document 1 and we also put a 1 in the cell corresponding to Topic 2 and Document 1.<br>\n",
    "\n",
    "We do this for each topic (z) for each document (d) based on our initial topic assignment for each word in each document.<br>\n",
    "\n",
    "Note: \"placeholder\" simply means that we are not inputting an actual value for the sake of simplicity in this example.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the initial topic assignment for each word in each document:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-s268{text-align:left}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-s268\"></th>\n",
    "    <th class=\"tg-s268\"></th>\n",
    "    <th class=\"tg-s268\"></th>\n",
    "    <th class=\"tg-0lax\"></th>\n",
    "    <th class=\"tg-0lax\"></th>\n",
    "    <th class=\"tg-0lax\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\">Document 1</td>\n",
    "    <td class=\"tg-0lax\">Document 2</td>\n",
    "    <td class=\"tg-0lax\">Document 3</td>\n",
    "    <td class=\"tg-0lax\">Document 4</td>\n",
    "    <td class=\"tg-0lax\">Document 5</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Broccoli-Topic 1</td>\n",
    "    <td class=\"tg-0lax\">1 X 2 = 2</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Broccoli-Topic 2</td>\n",
    "    <td class=\"tg-0lax\">1 X 1 = 1</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "In order to update our initial topic assignments for each word in each document, we look at the Beta and Theta distributions we calculated previously.<br>\n",
    "\n",
    "Notice that \"broccoli\" is associated with Topic 1 one time and Topic 2 one time in the Beta distribution while Document 1 is associated with Topic 1 two times and Topic 2 one time in the Theta distribution.<br>\n",
    "\n",
    "Now, to calculate the new topic (z) assignment for the word (w) \"broccoli\", we do some simple arithmetic operations.<br>\n",
    "\n",
    "We multiply the value in the cell associated with Topic 1 and \"broccoli\" in the Beta distribution with the value in the cell associated with Topic 1 and Document 1 in the Theta distribution.  This gives us 1 X 2 = 2.<br>\n",
    "\n",
    "We then multiply the value in the cell associated with Topic 2 and \"broccoli\" in the Beta distribution with the value in the cell associated with Topic 2 and Document 1 in the Theta distribution.  This gives us 1 X 1 = 1.<br>\n",
    "\n",
    "##### Important Note:  This process is repeated for each word in each document BEFORE moving on to the next document.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-88nc{font-weight:bold;border-color:inherit;text-align:center}\n",
    ".tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-uys7{border-color:inherit;text-align:center}\n",
    ".tg .tg-y2k2{font-weight:bold;text-decoration:underline;border-color:inherit;text-align:center}\n",
    ".tg .tg-7btt{font-weight:bold;border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-9353{font-weight:bold;text-decoration:underline;border-color:inherit;text-align:center;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-uys7\"></th>\n",
    "    <th class=\"tg-y2k2\">Documents (M = 5)</th>\n",
    "    <th class=\"tg-88nc\">Document 1</th>\n",
    "    <th class=\"tg-7btt\">Document 2</th>\n",
    "    <th class=\"tg-7btt\">Document 3</th>\n",
    "    <th class=\"tg-7btt\">Document 4</th>\n",
    "    <th class=\"tg-7btt\">Document 5</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-9353\">Words (in Vocabulary)</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">Eat-Topic 1</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">Eat-Topic 2</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">Broccoli-Topic 1</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\">1 X 2 = 2 --&gt; 2 / (2 + 1) = 2/3</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">Broccoli-Topic 2</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\">1 X 1 = 1 --&gt; 1 / (2 + 1) = 1/3</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">...</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">...</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">Hamster-Topic 1</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">Hamster-Topic 2</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Next, we sum the resulting values for the above multiplications to obtain (1 X 2) + (1 X 1) = 3.<br>\n",
    "\n",
    "Then, we divide the resulting values for each of the above multiplications by the value of the sum, 3.<br>\n",
    "\n",
    "Therefore, we have obtained probability values by which we use to select a new topic to assign to the word \"broccoli\".<br>\n",
    "\n",
    "In this case, they are a 2/3 = 0.6666667 chance that we assign \"broccoli\" to Topic 1 in Document 1 and a 1/3 = 0.33333333 chance that we assign \"broccoli\" to Topic 2 in Document 1.<br>\n",
    "\n",
    "Notice that we are assigning a new topic to the word \"broccoli\" in Document 1 according to PROBABILITIES that are calculated using the arithmetic operations above.<br>\n",
    "\n",
    "We are NOT simply arbitrarily assigning a new topic (z) to the word (w) \"broccoli\".  Everything is based on the Beta and Theta distributions and the conditional probabilities in the LDA pseudocode described above.<br>\n",
    "\n",
    "We repeat this for each word (w) in our vocabulary (V) for each document (d) in our set of documents (M).<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated topic assignment for \"broccoli\" in Document 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-c3ow\">Documents (M = 5, N = 3)</th>\n",
    "    <th class=\"tg-c3ow\">Word 1</th>\n",
    "    <th class=\"tg-c3ow\">Word 2</th>\n",
    "    <th class=\"tg-c3ow\">Word 3</th>\n",
    "    <th class=\"tg-c3ow\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Doc 1 Word Topic Assignment--&gt;</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\">2 --&gt; 1</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Document 1</td>\n",
    "    <td class=\"tg-c3ow\">eat</td>\n",
    "    <td class=\"tg-c3ow\">broccoli</td>\n",
    "    <td class=\"tg-c3ow\">banana</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Doc 2 Word Topic Assignment--&gt;</td>\n",
    "    <td class=\"tg-c3ow\">2</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\">2</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Document 2</td>\n",
    "    <td class=\"tg-c3ow\">banana</td>\n",
    "    <td class=\"tg-c3ow\">spinach</td>\n",
    "    <td class=\"tg-c3ow\">lunch</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Doc 3 Word Topic Assignment--&gt;</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\">2</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Document 3</td>\n",
    "    <td class=\"tg-c3ow\">chinchilla</td>\n",
    "    <td class=\"tg-c3ow\">kitten</td>\n",
    "    <td class=\"tg-c3ow\">cute</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Doc 4 Word Topic Assignment--&gt;</td>\n",
    "    <td class=\"tg-c3ow\">2</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\">2</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Document 4</td>\n",
    "    <td class=\"tg-c3ow\">sister</td>\n",
    "    <td class=\"tg-c3ow\">kitten</td>\n",
    "    <td class=\"tg-c3ow\">today</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Doc 5 Word Topic Assignment--&gt;</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\">2</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Document 5</td>\n",
    "    <td class=\"tg-c3ow\">hamster</td>\n",
    "    <td class=\"tg-c3ow\">eat</td>\n",
    "    <td class=\"tg-c3ow\">broccoli</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "Look to the table above for the new topic (z) assigned to the word (w) \"broccoli\" ASSUMING that in using the probabilities we just calculated we decide on reassigning \"broccoli\" to Topic 1 in Document 1.<br>\n",
    "\n",
    "It is important to know that we could also have assigned \"broccoli\" to Topic 2 instead.  However, based on the calculated probabilities for each topic (z) in our set of topics (k) it is far more likely that a randomized selection will select Topic 1 rather than Topic 2 (since Topic 1 = 2/3 chance and Topic 2 = 1/3 chance).<br>\n",
    "\n",
    "In an actual implementation of the LDA model, we would do this reassignment for each word (w) in each document (d) based on the probabilities calculated for each word (w) using the Beta and Theta distributions.<br>\n",
    "\n",
    "However, we are not done yet with just the first iteration of the LDA algorithm.  We still need to update the values for the Beta and Theta distributions for the next iteration of the LDA algorithm.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Updated β (Beta) Distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-xldj{border-color:inherit;text-align:left}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-xldj\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">Words</td>\n",
    "    <td class=\"tg-0pky\">eat</td>\n",
    "    <td class=\"tg-0pky\">broccoli</td>\n",
    "    <td class=\"tg-0pky\">banana</td>\n",
    "    <td class=\"tg-0pky\">spinach</td>\n",
    "    <td class=\"tg-0pky\">lunch</td>\n",
    "    <td class=\"tg-0pky\">chinchilla</td>\n",
    "    <td class=\"tg-0pky\">kitten</td>\n",
    "    <td class=\"tg-0pky\">cute</td>\n",
    "    <td class=\"tg-0pky\">sister</td>\n",
    "    <td class=\"tg-0pky\">today</td>\n",
    "    <td class=\"tg-0pky\">hamster</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">Topics</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">1</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1 --&gt; 2</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">2</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1 --&gt; 0</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Note that the cell associated with Topic 1 and \"broccoli\" has changed from 1 --> 2 and that the cell associated with Topic 2 and \"broccoli\" has changed from 1 --> 0.<br>\n",
    "\n",
    "Refer to the updated topic assignment for \"broccoli\" in Document 1 in the table in the previous section.<br>\n",
    "\n",
    "In that table, notice that the word (w) \"broccoli\" is now only associated with Topic 1 across all documents (d) and that the word (w) \"broccoli\" occurs twice across all documents (d).<br>\n",
    "\n",
    "Therefore, we update the cell associated with Topic 1 and \"broccoli\" in the Beta distribution to 2 and we also update the cell associated with Topic 2 and \"broccoli\" in the Beta distribution to 0.<br>\n",
    "\n",
    "We would do this for all words (w) in our vocabulary (V) for all topics (z) in our set of topics (k).<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Updated θ (Theta) Distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-s268{text-align:left}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-s268\"></th>\n",
    "    <th class=\"tg-s268\">Documents</th>\n",
    "    <th class=\"tg-s268\">1</th>\n",
    "    <th class=\"tg-0lax\">2</th>\n",
    "    <th class=\"tg-0lax\">3</th>\n",
    "    <th class=\"tg-0lax\">4</th>\n",
    "    <th class=\"tg-0lax\">5</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s268\">Topics</td>\n",
    "    <td class=\"tg-s268\"></td>\n",
    "    <td class=\"tg-s268\"></td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s268\">1</td>\n",
    "    <td class=\"tg-s268\"></td>\n",
    "    <td class=\"tg-s268\">2 --&gt; 3</td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">2</td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\">1 --&gt; 0</td>\n",
    "    <td class=\"tg-0lax\">2</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Note that the cell associated with Topic 1 and Document 1 has changed from 2 --> 3 and that the cell associated with Topic 2 and Document 1 has changed from 1 --> 0.<br>\n",
    "\n",
    "Refer to the updated topic assignment for \"broccoli\" in Document 1 in the table in the previous section.<br>\n",
    "\n",
    "In that table, notice that Document 1 contains 3 words (N) that are now all associated with Topic 1.  So, there are no words in Document 1 that are associated with Topic 2.<br>\n",
    "\n",
    "Therefore, we update the cell associated with Topic 1 and Document 1 in the Theta distribution to 3 and we also update the cell associated with Topic 2 and Document 1 in the Theta distribution to 0.<br>\n",
    "\n",
    "We would do this for all documents (d) for all topics (z) in our set of topics (k).<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion of the FIRST iteration of the LDA algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "We would now start at Step 2 and rinse + repeat until we have exhausted our allocated computational time.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Dirichlet_distribution<br>\n",
    "\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Plate_notation<br>\n",
    "\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation<br>\n",
    "\n",
    "\n",
    "- https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158<br>\n",
    "    - Utilized two diagrams, formula, and explanation of associated notation on LDA's.<br>\n",
    "\n",
    "\n",
    "- https://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/<br>\n",
    "    - Utilized blog's example as the basis for the explanation of the LDA algorithm pseudocode.<br>\n",
    "\n",
    "\n",
    "- https://www.coursera.org/learn/ml-clustering-and-retrieval<br>\n",
    "    - Information on collapsed Gibbs sampling and variational inference in relation to LDA's.<br>\n",
    "\n",
    "\n",
    "- https://www.investopedia.com/terms/p/posterior-probability.asp<br>\n",
    "    - Explanation of statistical terminology including posterior and prior probability.<br>\n",
    "\n",
    "\n",
    "- https://cs.calvin.edu/courses/cs/x95/videos/2018-2019/<br>\n",
    "    - Used Derek Fisher's explanation of why LDA does not work well on Tweets (with Scikit-Learn standard implementation).<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
