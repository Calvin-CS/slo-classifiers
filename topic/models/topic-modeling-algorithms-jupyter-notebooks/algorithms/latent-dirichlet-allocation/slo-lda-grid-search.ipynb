{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation Grid Search on SLO Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "**Note: This exhaustive grid search applies specifically to the Scikit-Learn LatentDirichletAllocation() class and CountVectorizer() class.  It also utilizes the Pipeline() class to setup the previous two classes.**<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "We use Scikit-Learn's Pipeline Class to construct a pipeline consisting of the CountVectorizer and LatentDirichletAllocation classes.<br>\n",
    "\n",
    "The \"parameters\" dictionary determines all the possible combinations of hyperparameters we will test in order to find the optimal hyperparameters for the model.<br>\n",
    "\n",
    "The grid search is performed by fitting on the Twitter data we wish to use for topic extraction.<br>\n",
    "\n",
    "The optimal hyperparameters are displayed via \"log.info\" messages so the log verbosity level must be set appropriately to view them.<br>\n",
    "\n",
    "We recommend executing this only on a supercomputer as otherwise, it will take an extremely long time to finish due to the number of possible combinations of hyperparameters as defined in the dictionary above.<br>\n",
    "\n",
    "If running this code snippet on a non-workstation PC, you may want to change \"n_jobs=-1\" to \"n_jobs=0\" for the GridSearchCV() class to prevent Python from utilizing all CPU cores and bogging down your system to unusability for the duration of the search.  This will require you to edit the code base in the \"slo_lda_topic_extraction_utility_functions.py\" file given in the first URL link in the \"Notes\" section below.<br>\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# What parameters do we search for?\n",
    "lda_search_parameters = {\n",
    "    'vect__strip_accents': [None],\n",
    "    'vect__lowercase': [True],\n",
    "    'vect__stop_words': ['english'],\n",
    "    # 'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "    'vect__analyzer': ['word'],\n",
    "    'vect__min_df': [2],\n",
    "    'vect__max_df': [0.95],\n",
    "    'vect__max_features': [1000],\n",
    "    'clf__n_components': [5, 10, 20],\n",
    "    'clf__doc_topic_prior': [None],\n",
    "    'clf__topic_word_prior': [None],\n",
    "    'clf__learning_method': ['online'],\n",
    "    'clf__learning_decay': [0.5, 0.7, 0.9],\n",
    "    'clf__learning_offset': [5, 10, 15],\n",
    "    'clf__max_iter': [1, 5, 10],\n",
    "    'clf__batch_size': [64, 128, 256],\n",
    "    'clf__evaluate_every': [0],\n",
    "    # 'clf__total_samples': [1e4, 1e6, 1e8],\n",
    "    # 'clf__perp_tol': [1e-1, 1e-2, 1e-3],\n",
    "    'clf__mean_change_tol': [1e-1, 1e-3, 1e-5],\n",
    "    'clf__max_doc_update_iter': [50, 100, 150],\n",
    "    'clf__n_jobs': [-1],\n",
    "    'clf__verbose': [0],\n",
    "    'clf__random_state': [None],\n",
    "}\n",
    "\n",
    "# lda_util.latent_dirichlet_allocation_grid_search(slo_feature_set, lda_search_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter list for GridSearchCV():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "strip_accents: strip accents off of individual characters.<br>\n",
    "\n",
    "lowercase: downcase all characters.<br>\n",
    "\n",
    "stop_words: remove all the specified stopwords.<br>\n",
    "\n",
    "ngram_range: lower/upper boundary for word/character n-grams.<br>\n",
    "\n",
    "analyzer: determines whether features consist of words or characters.<br>\n",
    "\n",
    "min_df: ignore words/characters that have a document frequency lower than this threshold.<br>\n",
    "max_df: ignore words/characters that have a document frequency higher than this threshold.<br>\n",
    "\n",
    "max_features: the maximum number of words/characters for building the vocabulary.<br>\n",
    "\n",
    "n_components: determines the # of topics to extract.<br>\n",
    "\n",
    "doc_topic_prior: alpha (α) value prior parameter.<br>\n",
    "topic_word_prior: eta (η) value prior parameter.<br>\n",
    "\n",
    "\n",
    "learning_method: method to update n_components (topics).<br>\n",
    "learning_decay: controls the learning rate of \"online\" learning method.<br>\n",
    "learning_offset: downweights early iterations of the \"online\" learning method.<br>\n",
    "\n",
    "max_iter: maximum number of iterations to run the algorithm.<br>\n",
    "batch_size: number of documents to use in the \"Expectation-maximization\" (EM) algorithm for the  \"online\" learning method.<br>\n",
    "\n",
    "evaluate_every: used in the \"fit\" method of the model to evaluate perplexity - how well a probability distribution or model predicts a sample..<br>\n",
    "\n",
    "total_samples: total number of documents used in the \"partial_fit\" method of the model.\n",
    "\n",
    "perp_tol: perplexity tolerance level for \"batch\" learning method that is used only when \"evaluate_every\" > 0.\n",
    "\n",
    "mean_change_tol: stopping tolerance level for theta distribution in the EM-algorithm \"Expectation\" (E) step - gives latent topic probabilities.<br>\n",
    "\n",
    "max_doc_update_iter: maximum number of iterations for updating theta distribution in the \"Expectation\" (E) step of the EM-algorithm.<br>\n",
    "\n",
    "n_jobs: maximum number of threads to use in the \"Expectation\" (E) step of the EM-algorithm.<br>\n",
    "\n",
    "verbose: sets the verbosity level of the grid search.<br>\n",
    "\n",
    "random_state: use a random number generator.  when set to \"none\", utilizes np.random.<br>\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive grid search for Scikit-Learn LDA using subset of Twitter dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Here, we implement an exhaustive grid search using a smaller subset of the entire Twitter dataset.  This is done to cut down on the computational time required to finish the search.  We have a large dataset of over 650k+ Tweets so utilizing the full dataset drastically increases the search time.<br>\n",
    "\n",
    "The first parameter for the \"dataframe_subset\" function dictates the full dataset you wish to subset while the second parameter defines the number of rows (examples) desired for the subset of the full dataset.<br>\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_subset = lda_util.dataframe_subset(tweet_dataset_processed, 10000)\n",
    "lda_util.latent_dirichlet_allocation_grid_search(data_subset, lda_search_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "**TODO: Get this running on the Borg supercomputer using Singularity Container.**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Refer to Scikit-Learn Documentation for further information on each class used:<br>\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html<br>\n",
    "\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html<br>\n",
    "\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Refer to URL link for the codebase to the utility functions used above for the grid searches:<br>\n",
    "\n",
    "https://github.com/J-Jinn/Summer-Research-2019/blob/master/topic_extraction_utility_functions.py<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
