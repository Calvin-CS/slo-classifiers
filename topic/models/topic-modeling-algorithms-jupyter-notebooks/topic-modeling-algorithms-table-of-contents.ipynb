{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topic Modeling Algorithms\n",
    "\n",
    "### Keith VanderLinden and Joseph Jinn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook provides a table of contents with a summarized overview of each baseline topic modeling algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is a generative probabilistic model that assigns a set of topics to a corpus of documents using Dirichlet distributions as priors.  The model performs well on documents with formal grammatical style and long text lengths but generally performs poorly on short texts with inconsistent grammatical style.  This is the baseline algorithm for topic extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further details, please refer to the following Jupyter Notebook files:\n",
    "\n",
    "[LDA Introduction](algorithms/latent-dirichlet-allocation/slo-lda-introduction.ipynb#bookmark)\n",
    "\n",
    "[LDA Implementation](algorithms/latent-dirichlet-allocation/slo-lda-implementation.ipynb#bookmark)\n",
    "\n",
    "[LDA Grid Search](algorithms/latent-dirichlet-allocation/slo-lda-grid-search.ipynb#bookmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources Referenced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to resources section in \"LDA introduction\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Dirichlet Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDP is a generative probabilistic model that is similar to LDA except that the number of assigned topics is not a hyperparameter.  The set of topics themselves is a random (latent) variable that is generated via Dirichlet processes and there is no upper bound on the number of generated topics.  The \"hierarchical\" part of the name refers to a global set of topics shared among the entire corpus of documents from which the local set of topics assigned to each document is drawn from.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further details, please refer to the following Jupyter Notebook files:\n",
    "\n",
    "[HDP Introduction](algorithms/hierarchical-dirichlet-process/slo-hdp-introduction.ipynb#bookmark)\n",
    "\n",
    "[HDP Implementation](algorithms/hierarchical-dirichlet-process/slo-hdp-implementation.ipynb#bookmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources Referenced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://datascience.stackexchange.com/questions/128/latent-dirichlet-allocation-vs-hierarchical-dirichlet-process\n",
    "    - explains HDP, HLDA, and LDA in a more layman friendly way.\n",
    "\n",
    "\n",
    "- https://stats.stackexchange.com/questions/135736/hierarchical-dirichlet-processes-in-topic-modeling\n",
    "    - a more statistical explanation of HDP in topic modeling.\n",
    "\n",
    "\n",
    "- https://www.quora.com/What-is-an-intuitive-explanation-of-Dirichlet-process-clustering-How-do-Polyas-Urn-or-Stick-Breaking-exemplify-the-Dirichlet-process-How-does-Gibbs-sampling-based-clustering-for-a-Dirichlet-mixture-model-utilize-the-Dirichlet-process\n",
    "    - explanation of Dirichlet processes.\n",
    "\n",
    "\n",
    "- https://people.eecs.berkeley.edu/~jordan/papers/hdp.pdf\n",
    "    - original paper on HDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HLDA is an extension of the LDA algorithm.  LDA is implemented on the original corpus of documents resulting in the usual document-topic and word-topic assignments.  Then, \"synthetic\" documents are created from the word-topic assignments for each document-topic assignment and are grouped into \"synthetic\" corpuses by topic.  LDA is implemented recursively on the \"synthetic\" corpuses until \"synthetic\" documents and corpuses are no longer able to be generated.  For each loop of the recursion, a hierarchy of topic distributions are generated, resulting in a tree-like structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further details, please refer to the following Jupyter Notebook files:\n",
    "\n",
    "[HLDA Introduction](algorithms/hierarchical-latent-dirichlet-allocation/slo-hlda-introduction.ipynb#bookmark)\n",
    "\n",
    "[HLDA Implementation](algorithms/hierarchical-latent-dirichlet-allocation/slo-hlda-implementation.ipynb#bookmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources Referenced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.aclweb.org/anthology/W14-3111\n",
    "    - has a nice overview of the HLDA algorithm with image.\n",
    "    \n",
    "    \n",
    "- https://papers.nips.cc/paper/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.pdf\n",
    "    - original paper by Blei.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author-Topic Model (a.k.a. LDA-u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author-topic model is an extension of the LDA algorithm.  It combines both the Author model and the LDA topic model into the Author-Topic (LDA) model.  Whereas normal LDA generates a document-topic and word-topic distribution, the author-topic model generates a author-topic and word-topic distribution (there is no longer a document-topic distribution in use).  The result is a probabilistic model that assigns topics to authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further details, please refer to the following Jupyter Notebook files:\n",
    "\n",
    "[Author-Topic Introduction](algorithms/author-topic/slo-author-topic-introduction.ipynb#bookmark)\n",
    "\n",
    "[Author-Topic Implementation](algorithms/author-topic/slo-author-topic-implementation.ipynb#bookmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources Referenced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://mimno.infosci.cornell.edu/info6150/readings/398.pdf\n",
    "    - original paper on author-topic model.\n",
    "    \n",
    "    \n",
    "- https://www.slideshare.net/FREEZ7/author-topic-model?from_action=save\n",
    "    - nice slides explaining the author-topic model.\n",
    "\n",
    "- https://stackoverflow.com/questions/47434426/pandas-groupby-unique-multiple-columns\n",
    "    - using Pandas .groupby() function. \n",
    "\n",
    "- https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/\n",
    "    - using Pandas .groupby() function blog.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biterm Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biterms are unordered word-pair combinations within a text.  The biterm model resolves the data sparsity issue with short texts by modeling biterms across the entire corpus of documents instead of individual words within a document.  Each biterm is associated with a single topic whereas in LDA each word can be associated with multiple topics.  The model infers the topic of each document using the topics its biterms are associated with.  Hypothetically, topic extraction is easier with this model as inferring the topic of a biterm is easier with the added context provided using word co-occurrences over individual words with no context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further details, please refer to the following Jupyter Notebook files:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Biterm Introduction](algorithms/biterm/slo-biterm-introduction.ipynb#bookmark)\n",
    "\n",
    "[Biterm Implementation](algorithms/biterm/slo-biterm-implementation.ipynb#bookmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources Referenced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.researchgate.net/publication/262244963_A_biterm_topic_model_for_short_texts\n",
    "    - original paper on Biterm model.\n",
    "\n",
    "\n",
    "- https://sutheeblog.wordpress.com/2017/03/20/a-biterm-topic-model-for-short-texts/\n",
    "    - blog explaining the biterm topic model in a more palatable way.\n",
    "\n",
    "\n",
    "- https://www.cs.toronto.edu/~jstolee/projects/topic.pdf\n",
    "    - contains a short section explaining the algorithm; includes plate notation diagram.\n",
    "    \n",
    "\n",
    "- https://stackoverflow.com/questions/29786985/whats-the-disadvantage-of-lda-for-short-texts\n",
    "    - contains a response by the author of the biterm model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
