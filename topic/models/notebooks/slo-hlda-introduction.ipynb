{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hierarchical Latent Dirichlet Allocation (HLDA) Topic Modeling Algorithm Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook file provides a very simple high-level overview of the HLDA topic modeling algorithm.  We briefly discuss the plate notation diagram, pseudocode, and statistical formula for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plate Notation for the HLDA Algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hlda](../images/hlda_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right half of the plate diagram depicts the base Latent Dirichlet Allocation (LDA) algorithm.  The left half of the plate diagram depicts the \"hierarchical\" part that makes it HLDA utilizing the nested Chinese Restaurant Process (nCRP).  Latent (hidden) variables are un-shaded whereas observed (visible) variables are shaded.\n",
    "\n",
    "$\\Upsilon$ - parameter controlling how often a document chooses a new path.\n",
    "\n",
    "$T$ - collection of infinite # of L-level paths drawn from a nCRP. (tree-like structure)\n",
    "\n",
    "$c+{d}$ - path of document 'd'.\n",
    "\n",
    "$\\alpha$ - Dirichlet distribution prior on the document-topic distribution.\n",
    "\n",
    "$\\eta$ - Dirichlet distribution prior on the word-topic distribution.\n",
    "\n",
    "$\\theta$ - author-topic distribution.\n",
    "\n",
    "$\\beta$ - word-topic distribution.\n",
    "\n",
    "$z$ - the topic assigned to the word.\n",
    "\n",
    "$w$ - the selected word.\n",
    "\n",
    "$N$ - the entire vocabulary of words in a document \"d\".\n",
    "\n",
    "$M$ - the entire collection of documents in the corpus.\n",
    "\n",
    "$\\infty$ - infinity # of restaurants (topics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese Restaurant Process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The below is essentially copy/paste from one of my references besides some Latex, Markdown, and minor adjustments.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chinese Restaurant Process (CRP) is a distribuition on partitions of integers. Imagine there are M customers in a Chinese restaurant with infinite tables. The first customer sits at the first table. Customers have the following choices:\n",
    "\n",
    "* Sit in the table that some one alse is already there\n",
    "* Sit in a new table\n",
    "\n",
    "These two choices have probabilities that depend on the previosu customers at the tables. \n",
    "\n",
    "Specifically, for the $m$th customer, the probability to sit in a table is:\n",
    "\n",
    "- p(occupied table i| previous customers) = $\\frac{m_i}{\\gamma+m-1}$\n",
    "- p(next unoccupied table| previous customers) = $\\frac{\\gamma}{\\gamma+m-1}$,\n",
    "\n",
    "where $m_i$ represnets the number of previous customers at the table $i$; $\\gamma$ is a parameter.\n",
    "\n",
    "If we have M customers, the CRP will give us a partion of M customers, which has the same structure as a Dirichlet process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CRP](../images/chinese_restaurant_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Chinese Restaurant Process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The below is essentially copy/paste from one of my references besides some Latex, Markdown, and minor adjustments.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CRP establishes a one-to-one relationship between tables and mixture components. A hierarchical version of CRP was also developed to model one-to-many. The process like this is called nested Chinese Restaurant Restaurant Process (nCPR).The nCRP is very similar with CRP except for its hieracrchical structure.\n",
    "\n",
    "We can see an example in the following plot To help understand the nCRP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nCRP](../images/nested_chinese_restaurant_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose a traveller came to a new city and wanter to try the restaurants there. There is a root restaurant, which is the first stop for new travellers. He came to the root restaurant and chose a table based on the Chinese restaurant process we described before. And each table has a card that references to the next restaurant. The traveller followed the card's instruction and went to the restaurant on the card. Then he chose a table in the second restaurant followed the CRP. In a conclusion, each traveller has a path that contains a batch of restaurants and each restaurant represents a level of the topic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Formulas and Explanation of the Gibbs Sampling Process for the HLDA Algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The below is essentially copy/paste from one of my references besides some Latex, Markdown, and minor adjustments.  I do not pretend to have sufficient statistical background to understand all of this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximate inference by Gibbs sampling\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Introduction to Gibbs sampling:\n",
    "\n",
    "Gibbs sampling is commonly used for statistical inference to determine the best value of a parameter. The idea in Gibbs sampling is to generate posterior samples by sweeping through each variable (or block of variables) to sample from its conditional distribution with the remaining variables fixed to their current values. For instance, the standard step for Gibbs sampling over a space of variables a, b, c is:\n",
    "\n",
    "* Draw a conditioned on b, c.\n",
    "* Draw b conditioned on a, c.\n",
    "* Draw c conditioned on a, b This process continues until “convergence”, which means that the sample values have the same distribution as if they were sampled from the true posterior joint distribution.\n",
    "\n",
    "##### Gibbs sampling for the hLDA model:\n",
    "\n",
    "The variables that are needed to be sampled are:\n",
    "\n",
    "1. $w_{m,n}$: the $n$th word in the $m$th document (Important note: these are the only observed variables in the model)\n",
    "\n",
    "2. $c_{m,l}$: the restaurant (node), the $l$th topic in the $m$th document\n",
    "\n",
    "3. $z_{m,n}$: the assignment of the $n$th word in the $m$th document to one of the $L$ topics\n",
    "\n",
    "There are also some variables needed in the model, but they are not needed to be sampled.\n",
    "\n",
    "After illustrating the variables in the model, we also need to know the order and the methods of the sampling. We can apply the sampling methods into two steps:\n",
    "\n",
    "1. sample the $z_{m,n}$ variale by using LDA+CRP\n",
    "2. sample the $c_{m,l}$ based on the first step (given the LDA hidden variables).\n",
    "\n",
    "\n",
    "#### To be more specific:\n",
    "\n",
    "\n",
    "##### Sample $z_{m,n}$:\n",
    "\n",
    "The $z_{m,n}$ is sampled under LDA model based on the method in paper (Dirichlet integrals):\n",
    "\n",
    "\\begin{align*} p(z_{i}=j\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf z}{-i},{\\bf w})\\propto\\frac{n{-i, j}^{(w_{i})}+\\beta}{n_{-i, j}^{(\\cdot)}+W\\beta}\\frac{n^{(d_{i})}+\\alpha}{n_{-i,\\cdot}^{(d_{i})}+T\\alpha} \\end{align*}\n",
    "\n",
    "where:\n",
    "\n",
    "$z_{i}$ is the assignments of words to topics;\n",
    "\n",
    "$n_{-i,j}^{(w_{i})}$ is number of words assigned to topic $j$ that are the same as $w_i$;\n",
    "\n",
    "$n_{-i,j}^{(\\cdot)}$ is total number of words assigned to topic $j$;\n",
    "\n",
    "$n_{-i,j}^{(d_{i})}$ shows number of words from document $d_i$ assigned to topic $j$, $n_{-i,\\cdot}^{(d_{i})}$ represents total number of words in document $d_i$;\n",
    "\n",
    "$W$ shows number of words have been assigned.\n",
    "\n",
    "$\\alpha,\\beta$: free parameters that determine how heavily these empirical distributins are smoothed.\n",
    "\n",
    "\n",
    "##### Sample $c_m$ from the nCRP:\n",
    "\n",
    "The conditional distibution for $c_m$:\n",
    "\n",
    "$p(w_m|c,w_{-m},z)$: the likelihood of the data given a particular choice of $c_m$\n",
    "\n",
    "$p(c_m|c_{-m})$: the prior on $c_m$ implied by the nested CRP\n",
    "\n",
    "$$p(c_m | w, c_{-m}, z) \\propto p(w_m | c, w_{-m}, z) p(c_m | c_{-m})$$\n",
    "\n",
    "\n",
    "The calculation of the $p(w_m | c, w_{-m},z)$ value based on the likelihood function:\n",
    "\n",
    "\n",
    "$$p(w_m | c, w_{-m},z) = \\prod_{l=1}^{L} (\n",
    "\\frac{\n",
    "   \\Gamma (n_{c_{m,l,-m}}^{(\\cdot)}+W\\eta)}{\\prod_{\\omega} \n",
    "   \\Gamma (n_{c_{m,l,-m}}^{(\\omega)}+\\eta)}\n",
    "\\frac{\\prod_{\\omega} \\Gamma(n_{c_{m,l,-m}}^{(\\omega)}+n_{c_{m,l,m}}^{(\\cdot)}+\\eta)}{\\Gamma(n_{c_{m,l,-m}}^{(\\cdot)}+ n_{c_{m,l,m}}^{(\\cdot)} W\\eta)})$$\n",
    "\n",
    "where,\n",
    "\n",
    "$n^{(w)}_{c_m,I,-m}$: the number of word $w$ that have been assigned to the topic indexed y $c_{m,l}$ not including those in the current document\n",
    "\n",
    "$W$: the total vocabulary size\n",
    "\n",
    "$\\eta$: free parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hlda equation](../images/hlda_equation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional distribution for $c_{m}$, the 'L' topics associated with document 'm'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hlda equation](../images/hlda_equation_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability function given a specific choice of $c_{m}$ and $p(c_{m} | c_{-m})$ as the prior on $c_{m}$ implied by nCRP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode for the HLDA algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a graphical version of the pseudocode for the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hlda pseudocode](../images/hlda_pseudocode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the text version of the pseudocode which is equivalent to the graphical version above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hlda pseudocode](../images/hlda_pseudocode_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This was obtained from the README.md for a GitHub Repository containing another implementation of the HLDA algorithm in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a hierarchial topic model with L-levels, we can imagine it as a L-level tree and each node presents a topic.\n",
    "\n",
    "Generation of a document:\n",
    "\n",
    "* Choose a path from the root to a leaf\n",
    "* Choose the topic proportions $\\theta$ from a L-dimension Dirichlet\n",
    "* Generated the words in the document for m a mixture of the topics along the path from the root to leaf, with mixing proportions $\\theta$\n",
    "\n",
    "<br>\n",
    "\n",
    "![hlda explanation](../images/hlda_explanation.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "For the $m$th document in the corpus:\n",
    "\n",
    "* Let $c_1$ be the the root restaurant\n",
    "* For each level $l \\in {2,. . ., L}$: 1. Draw a table from the restaurant $c_{l-1}$. 2. Set $c_l$ to be the restaurant referred to by that table\n",
    "* Draw an $L$-dimensional topic proportion vector $\\theta$ for $Dir (\\alpha)$\n",
    "* For each word $n\\in {1, . . ., N}$: 1. Draw $z \\in {1,. . . ,L}$ from $mult (\\theta)$; 2. Draw $w_n$ from the topic assoicated with the restaurant $c_z$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simplified Hierarchical Latent Dirichlet Allocation Topic Modeling Algorithm Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder.\n",
    "\n",
    "**TODO - implement simple hand-worked example of one iteration through the algorithm (provided we can find an example)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion of the FIRST iteration of the HLDA algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rinse and repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Referenced:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://en.wikipedia.org/wiki/Greek_alphabet\n",
    "    - for Greek alphabet name reference.\n",
    "    \n",
    "\n",
    "- https://en.wikipedia.org/wiki/Dirichlet_integral\n",
    "    - apparently the statistics involved utilize these.\n",
    "\n",
    "\n",
    "- https://www.aclweb.org/anthology/W14-3111\n",
    "    - has a nice overview of the HLDA algorithm with image.\n",
    "    \n",
    "    \n",
    "- https://github.com/sakuramomo1005/STA663_Final_Project\n",
    "    - GitHub page that helps interpret the plate diagram; explains HLDA Gibbs sampling; explain nCRP; etc.\n",
    "    - This reference is has proved very useful.\n",
    "    \n",
    "\n",
    "- https://www.semanticscholar.org/paper/Guided-HTM%3A-Hierarchical-Topic-Model-with-Dirichlet-Shin-Moon/9954048379a85148f0ae4698b84c51f0ef897e2e/figure/4\n",
    "    - image helped explain what $\\Upsilon$ represented.\n",
    "    \n",
    "    \n",
    "- https://cs.brown.edu/courses/csci2950-p/fall2011/lectures/2011-10-25_bryantWang.pdf\n",
    "    - another source for pseudocode; provides explanation of the statistical components; presentation slides.\n",
    "    \n",
    "    \n",
    "- https://www.researchgate.net/publication/261201482_HLDA_based_text_clustering\n",
    "    - utilizing Figure 1 as representative of the pseudocode.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
