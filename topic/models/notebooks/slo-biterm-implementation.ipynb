{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Biterm Topic Model Implementation on SLO Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the requisite libraries, custom utility functions, and set the parameters for our various imported libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import logging as log\n",
    "import warnings\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "from biterm.cbtm import oBTM\n",
    "from biterm.utility import vec_to_biterms, topic_summuary  # helper functions\n",
    "\n",
    "# Import custom utility functions.\n",
    "# import topic_extraction_utility_functions as lda_util\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Pandas options.\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "pd.options.display.max_colwidth = 1000\n",
    "# Pandas float precision display.\n",
    "pd.set_option('precision', 12)\n",
    "# Seaborn setting.\n",
    "sns.set()\n",
    "# Don't output these types of warnings to terminal.\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "# Matplotlib log settings.\n",
    "mylog = log.getLogger(\"matplotlib\")\n",
    "mylog.setLevel(log.INFO)\n",
    "\n",
    "\"\"\"\n",
    "Turn debug log statements for various sections of code on/off.\n",
    "(adjust log level as necessary)\n",
    "\"\"\"\n",
    "log.basicConfig(level=log.INFO)\n",
    "log.disable(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process and Post-process Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize our Tweets using our dataset preprocessing and postprocessing function.  Please refer to \"topic_extraction_utility_functions.py\" for the full code-base and comments on what is done to the Tweet text and user description text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on our topic modeling dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-test-subset-100-examples.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-lda-ready-tweet-text-with-hashtags-excluded-created-7-17-19.csv\",\n",
    "    \"text_derived\")\n",
    "\n",
    "# Test on our topic modeling dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-test-subset-100-examples.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-lda-ready-user-description-text-with-hashtags-excluded-created-7-17-19.csv\",\n",
    "    \"user_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and prepare the preprocessed dataset for use in Biterm topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the general format of insertion into a Pandas dataframe, isolating the column of interest, and generating a dictionary of words and corpus of documents.  Please refer to the code comments for details on the specific steps for the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the dataset (relative path).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"twitter-dataset-7-10-19-lda-ready-tweet-text-with-hashtags-excluded-created-7-17-19.csv\", sep=\",\")\n",
    "\n",
    "# Import the dataset (absolute path).\n",
    "tweet_dataset_processed = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "                \"twitter-dataset-7-10-19-lda-ready-tweet-text-with-hashtags-excluded-created-7-17-19.csv\", sep=\",\")\n",
    "\n",
    "# # Import the dataset (test/debug).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"twitter-dataset-7-10-19-lda-ready-tweet-text-test.csv\", sep=\",\")\n",
    "\n",
    "# # Import the dataset (test/debug).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "#                 \"twitter-dataset-7-10-19-lda-ready-tweet-text-test.csv\", sep=\",\")\n",
    "\n",
    "# Reindex and shuffle the data randomly.\n",
    "tweet_dataset_processed = tweet_dataset_processed.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed.index))\n",
    "\n",
    "# Generate a Pandas dataframe.\n",
    "tweet_text_dataframe = pd.DataFrame(tweet_dataset_processed)\n",
    "\n",
    "# # Print shape and column names.\n",
    "# log.info(f\"\\nThe shape of the Tweet text dataframe:\")\n",
    "# log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "# log.info(f\"\\nThe columns of the Tweet text dataframe:\")\n",
    "# log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(\"\\nThe shape of the Tweet text dataframe:\")\n",
    "log.info(tweet_text_dataframe.shape)\n",
    "log.info(\"\\nThe columns of the Tweet text dataframe:\")\n",
    "log.info(tweet_text_dataframe.columns)\n",
    "\n",
    "# Drop any NaN or empty Tweet rows in dataframe (or else CountVectorizer will blow up).\n",
    "tweet_text_dataframe = tweet_text_dataframe.dropna()\n",
    "\n",
    "# # Print shape and column names.\n",
    "# log.info(f\"\\nThe shape of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "# log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "# log.info(f\"\\nThe columns of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "# log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(\"\\nThe shape of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(tweet_text_dataframe.shape)\n",
    "log.info(\"\\nThe columns of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(tweet_text_dataframe.columns)\n",
    "\n",
    "# Reindex everything.\n",
    "tweet_text_dataframe.index = pd.RangeIndex(len(tweet_text_dataframe.index))\n",
    "\n",
    "# Assign column names.\n",
    "tweet_text_dataframe_column_names = ['text_derived', 'text_derived_preprocessed', 'text_derived_postprocessed']\n",
    "\n",
    "# Rename column in dataframe.\n",
    "tweet_text_dataframe.columns = tweet_text_dataframe_column_names\n",
    "\n",
    "# Create input feature.\n",
    "selected_features = tweet_text_dataframe[['text_derived_postprocessed']]\n",
    "processed_features = selected_features.copy()\n",
    "\n",
    "# # Check what we are using as inputs.\n",
    "# log.info(f\"\\nA sample Tweet in our input feature:\")\n",
    "# log.info(f\"{processed_features['text_derived_postprocessed'][0]}\\n\")\n",
    "\n",
    "# Check what we are using as inputs.\n",
    "log.info(\"\\nA sample Tweet in our input feature:\")\n",
    "log.info(processed_features['text_derived_postprocessed'][0])\n",
    "\n",
    "# Create feature set.\n",
    "slo_feature_series = processed_features['text_derived_postprocessed']\n",
    "slo_feature_series = pd.Series(slo_feature_series)\n",
    "slo_feature_list = slo_feature_series.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function contains the code specific to each topic modeling library we utilize.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def biterm_topic_model_topic_extraction():\n",
    "    \"\"\"\n",
    "    Function performs topic extraction on Tweets using the Gensim HDP model.\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model.\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(slo_feature_series)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    # log.info(f\"\\n.fit_transform - Learn the vocabulary dictionary and return term-document matrix.\")\n",
    "    # log.info(f\"{tf}\\n\")\n",
    "    # log.info(f\"\\n.get_feature_names - Array mapping from feature integer indices to feature name\")\n",
    "    # log.info(f\"{tf_feature_names}\\n\")\n",
    "\n",
    "    log.info(\"\\n.fit_transform - Learn the vocabulary dictionary and return term-document matrix.\")\n",
    "    log.info(tf)\n",
    "    log.info(\"\\n.get_feature_names - Array mapping from feature integer indices to feature name\")\n",
    "    log.info(tf_feature_names)\n",
    "\n",
    "    # Convert corpus of documents (vectorized text) to numpy array.\n",
    "    tf_array = tf.toarray()\n",
    "\n",
    "    # Convert dictionary of words (vocabulary) to numpy array.\n",
    "    tf_feature_names = np.array(tf_vectorizer.get_feature_names())\n",
    "\n",
    "    # get biterms\n",
    "    biterms = vec_to_biterms(tf_array)\n",
    "\n",
    "    # create btm\n",
    "    btm = oBTM(num_topics=20, V=tf_feature_names)\n",
    "\n",
    "    print(\"\\n\\n Train Online BTM ..\")\n",
    "    for i in range(0, len(biterms), 100):  # prozess chunk of 200 texts\n",
    "        biterms_chunk = biterms[i:i + 100]\n",
    "        btm.fit(biterms_chunk, iterations=50)\n",
    "    topics = btm.transform(biterms)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # print(\"\\n\\n Visualize Topics ..\")\n",
    "    # vis = pyLDAvis.prepare(btm.phi_wz.T, topics, np.count_nonzero(tf_array, axis=1), tf_feature_names, np.sum(tf_array, axis=0))\n",
    "    # pyLDAvis.save_html(vis, './vis/online_btm.html')\n",
    "\n",
    "    print(\"\\n\\n Topic coherence ..\")\n",
    "    topic_summuary(btm.phi_wz.T, tf_array, tf_feature_names, 10)\n",
    "\n",
    "    print(\"\\n\\n Texts & Topics ..\")\n",
    "    for i in range(len(slo_feature_series)):\n",
    "        print(\"{} (topic: {})\".format(slo_feature_series[i], topics[i].argmax()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we call the topic modeling function and train it on our Twitter dataset.  We record the time it takes to process the entire dataset and extract topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main function.  Execute the program.\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    my_start_time = time.time()\n",
    "    ################################################\n",
    "    \"\"\"\n",
    "    Perform the topic extraction.\n",
    "    \"\"\"\n",
    "    biterm_topic_model_topic_extraction()\n",
    "\n",
    "    ################################################\n",
    "    my_end_time = time.time()\n",
    "\n",
    "    time_elapsed_in_seconds = (my_end_time - my_start_time)\n",
    "    time_elapsed_in_minutes = (my_end_time - my_start_time) / 60.0\n",
    "    time_elapsed_in_hours = (my_end_time - my_start_time) / 60.0 / 60.0\n",
    "    # print(f\"Time taken to process dataset: {time_elapsed_in_seconds} seconds, \"\n",
    "    #       f\"{time_elapsed_in_minutes} minutes, {time_elapsed_in_hours} hours.\")\n",
    "    print(\"\\n\\nTime taken to process dataset: \" + str(time_elapsed_in_seconds) + \" seconds, \" +\n",
    "          str(time_elapsed_in_minutes) + \" minutes, \" + str(time_elapsed_in_hours) + \" hours.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Extraction Results on Twitter Dataset Tweet Text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution run 1."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "(run 1)\n",
    " Topic coherence ..\n",
    "Topic 0 | Coherence=-176.49 | Top words= adani coal bhp qld santos job australian labor rio gas\n",
    "Topic 1 | Coherence=-146.07 | Top words= adani coal need climate australia job queensland new labor build\n",
    "Topic 2 | Coherence=-124.21 | Top words= adani labor qld coal stop want fund support project need\n",
    "Topic 3 | Coherence=-135.82 | Top words= adani coal loan point abbot government new slo_cashn turnbull project\n",
    "Topic 4 | Coherence=-145.06 | Top words= adani australian job want oppose project loan new people govt\n",
    "Topic 5 | Coherence=-126.36 | Top words= adani reef coal great barrier australia want need australian project\n",
    "Topic 6 | Coherence=-129.98 | Top words= adani coal stop project australia carmichael labor need fund want\n",
    "Topic 7 | Coherence=-150.89 | Top words= adani coal carmichael court title native federal queensland fund new\n",
    "Topic 8 | Coherence=-160.12 | Top words= adani santos water coal stop people land whitehaven want farmer\n",
    "Topic 9 | Coherence=-165.47 | Top words= santos bhp rio new day tinto nsw water gas project\n",
    "Topic 10 | Coherence=-124.98 | Top words= santos gas coal nsw project pilliga seam narrabri csg farmer\n",
    "Topic 11 | Coherence=-145.88 | Top words= coal water qld reef climate project money stop loan public\n",
    "Topic 12 | Coherence=-142.86 | Top words= adani coal loan fund want rail australia qld line veto\n",
    "Topic 13 | Coherence=-139.23 | Top words= bhp rio tinto tax australia australian ore iron billiton fortescue\n",
    "Topic 14 | Coherence=-122.33 | Top words= water adani santos basin great artesian nsw risk support australia\n",
    "Topic 15 | Coherence=-147.24 | Top words= bhp adani tax pay coal company billiton australia year cut\n",
    "Topic 16 | Coherence=-150.63 | Top words= adani coal job 000 reef create 10 australian queensland kill\n",
    "Topic 17 | Coherence=-128.56 | Top words= adani joyce barnaby india money coal taxpayer think rail spend\n",
    "Topic 18 | Coherence=-176.93 | Top words= adani australia santos tax energy pay year woodside people action\n",
    "Topic 19 | Coherence=-119.81 | Top words= gas field land barnaby narrabri propose nsw inland near joyce\n",
    "\n",
    "\n",
    " Texts & Topics ..\n",
    "australia > bhp billiton things watch ninemsn    (topic: 13)\n",
    "nsw govt fast track santos csg project day confirmation contaminate aquifer    (topic: 9)\n",
    "map qld drought       adani get approval unlimited water 26 million litre day    (topic: 14)\n",
    "santos sign subaru australia year agreement    (topic: 18)\n",
    "queensland premier mayor work convince adani ahead       (topic: 14)\n",
    "pm turnbull request publicly fund subsidy bin    (topic: 12)\n",
    "1 month image coal concentration 10 find adani spill coal lade water wetland (topic: 3)\n",
    "carbon capture storage exist call leave fuck grind (topic: 2)\n",
    "terminal job huge contamination    build new coal increase cyclone frequency    (topic: 1)\n",
    "\n",
    "\n",
    "Time taken to process dataset: 40567.238966464996 seconds, 676.1206494410833 minutes, 11.268677490684722 hours.\n",
    "\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution run 2."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "(run 2)\n",
    " Topic coherence ..\n",
    "Topic 0 | Coherence=-153.96 | Top words= adani coal reef australia tax bhp great energy pay barrier\n",
    "Topic 1 | Coherence=-175.85 | Top words= santos adani great australia whitehaven basin climate action day want\n",
    "Topic 2 | Coherence=-141.50 | Top words= santos adani coal gas barnaby joyce project rail fund government\n",
    "Topic 3 | Coherence=-149.54 | Top words= adani coal fund rail project basin galilee money bank public\n",
    "Topic 4 | Coherence=-144.03 | Top words= adani coal carmichael fund need bank want india stop power\n",
    "Topic 5 | Coherence=-150.87 | Top words= adani coal loan time new canavan minister support project matt\n",
    "Topic 6 | Coherence=-137.41 | Top words= adani coal carmichael job queensland project climate australia qld need\n",
    "Topic 7 | Coherence=-175.41 | Top words= bhp job adani santos 000 year 10 coal iron billiton\n",
    "Topic 8 | Coherence=-131.63 | Top words= bhp rio tinto billiton tax ore pay iron australia fortescue\n",
    "Topic 9 | Coherence=-125.09 | Top words= adani loan labor coal stop want election project shorten support\n",
    "Topic 10 | Coherence=-155.47 | Top words= adani coal job labor lnp queensland qld want whitehaven fund\n",
    "Topic 11 | Coherence=-191.38 | Top words= coal rio tinto adani australia energy year india santos woodside\n",
    "Topic 12 | Coherence=-152.15 | Top words= adani coal job qld water reef support great 000 unlimited\n",
    "Topic 13 | Coherence=-115.98 | Top words= santos gas nsw water project pilliga narrabri seam csg farmer\n",
    "Topic 14 | Coherence=-148.41 | Top words= adani coal turnbull government native title govt qld big carmichael\n",
    "Topic 15 | Coherence=-154.30 | Top words= adani climate coal labor change loan billion australian new dollar\n",
    "Topic 16 | Coherence=-159.14 | Top words= adani people labor stop federal wo santos group government govt\n",
    "Topic 17 | Coherence=-139.28 | Top words= adani coal point abbot loan carmichael australian reef great government\n",
    "Topic 18 | Coherence=-149.31 | Top words= coal adani santos people water stop land gas farmer nsw\n",
    "Topic 19 | Coherence=-220.89 | Top words= time queensland work plan key continue poll block profit morning\n",
    "\n",
    "\n",
    " Texts & Topics ..\n",
    "usa dark brazilian santos bean 5 pound bag coffee bean direct    (topic: 1)\n",
    "years corners probably piece dodgy backroom goings politician adani    (topic: 10)\n",
    "essential view tonight 4 corners slo_timepm digging adani     (topic: 1)\n",
    "question crime branch mundra gujarat dance word adani corners digging adani    (topic: 12)\n",
    "bhp confident strength chinese economy     (topic: 8)\n",
    "       (topic: 0)\n",
    "jonathan moylan receive good behaviour bond whitehaven hoax       (topic: 1)\n",
    "major shift language labor adani formal opposition certain (topic: 9)\n",
    "fight adani step way    (topic: 9)\n",
    "\n",
    "\n",
    "Time taken to process dataset: 39396.074105501175 seconds, 656.6012350916862 minutes, 10.94335391819477 hours.\n",
    "\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results display each topoic along with their topic coherence metric values as well as the top 10 words associated with each topic.  A sample of the Tweets in the dataset with their assigned topics is also given.  The library for biterm takes the longest to process using default hyperparameters (ignoring the issues with the hlda library)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://pypi.org/project/biterm/\n",
    "    - Biterm Python library we utilize. (Linux OS only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
