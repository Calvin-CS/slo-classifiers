{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Biterm Topic Model Implementation on SLO Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize a 3rd-party Biterm Model library.  This library will only install and run on the Linux OS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the requisite libraries, custom utility functions, and set the parameters for our various imported libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import logging as log\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from biterm.cbtm import oBTM\n",
    "from biterm.utility import vec_to_biterms, topic_summuary  # helper functions\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Pandas options.\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "pd.options.display.max_colwidth = 1000\n",
    "# Pandas float precision display.\n",
    "pd.set_option('precision', 12)\n",
    "# Seaborn setting.\n",
    "sns.set()\n",
    "# Don't output these types of warnings to terminal.\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "# Matplotlib log settings.\n",
    "mylog = log.getLogger(\"matplotlib\")\n",
    "mylog.setLevel(log.INFO)\n",
    "\n",
    "\"\"\"\n",
    "Turn debug log statements for various sections of code on/off.\n",
    "(adjust log level as necessary)\n",
    "\"\"\"\n",
    "log.basicConfig(level=log.INFO)\n",
    "log.disable(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process and Post-process Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess our Twitter dataset as follows:<br>\n",
    "\n",
    "1) Downcase all text.<br>\n",
    "2) Check that there is text, otherwise convert to empty string.<br>\n",
    "3) Convert html chars. to unicode chars.<br>\n",
    "4) Remove \"RT\" tags.<br>\n",
    "5) Remove concatenated URL's.<br>\n",
    "6) Handle whitespaces by converting all/multiple whitespace characters to a single whitespace.<br>\n",
    "7) Remove URL's and replace with \"slo_url\".<br>\n",
    "8) Remove Tweet mentions and replace with \"slo_mention\".<br>\n",
    "9) Remove Tweet stock symbols and replace with \"slo_stock\".<br>\n",
    "10) Remove Tweet hashtags and replace with \"slo_hash\".<br>\n",
    "11) Remove Tweet cashtags and replace with \"slo_cash\".<br>\n",
    "12) Remove Tweet year and replace with \"slo_year\".<br>\n",
    "13) Remove Tweet time and replace with \"slo_time\".<br>\n",
    "14) Remove character elongations.<br>\n",
    "\n",
    "We post-process our Twitter dataset as follows:<br>\n",
    "\n",
    "1) Remove the following irrelevant words specified in the List below:<br>\n",
    "\n",
    "    delete_list = [\"word_n\", \"auspol\", \"ausbiz\", \"tinto\", \"adelaide\", \"csg\", \"nswpol\",\n",
    "                   \"nsw\", \"lng\", \"don\", \"rio\", \"pilliga\", \"australia\", \"asx\", \"just\", \"today\", \"great\", \"says\", \"like\",\n",
    "                   \"big\", \"better\", \"rite\", \"would\", \"SCREEN_NAME\", \"mining\", \"former\", \"qldpod\", \"qldpol\", \"qld\", \"wr\",\n",
    "                   \"melbourne\", \"andrew\", \"fuck\", \"spadani\", \"greg\", \"th\", \"australians\", \"http\", \"https\", \"rt\",\n",
    "                   \"co\", \"amp\", \"carmichael\", \"abbot\", \"bill shorten\",\n",
    "                   \"slo_url\", \"slo_mention\", \"slo_hash\", \"slo_year\", \"slo_time\", \"slo_cash\", \"slo_stock\",\n",
    "                   \"adani\", \"bhp\", \"cuesta\", \"fotescue\", \"riotinto\", \"newmontmining\", \"santos\", \"oilsearch\",\n",
    "                   \"woodside\", \"ilukaresources\", \"whitehavencoal\",\n",
    "                   \"stopadani\", \"goadani\", \"bhpbilliton\", \"billiton\", \"cuestacoal\", \"cuests coal\", \"cqc\",\n",
    "                   \"fortescuenews\", \"fortescue metals\", \"rio tinto\", \"newmont\", \"newmont mining\", \"santosltd\",\n",
    "                   \"oilsearchltd\", \"oil search\", \"woodsideenergy\", \"woodside petroleum\", \"woodside energy\",\n",
    "                   \"iluka\", \"iluka resources\", \"whitehaven\", \"whitehaven coal\"]\n",
    "\n",
    "2) Remove all punctuation from the Tweet text.<br>\n",
    "3) Remove all English stop words from the Tweet text.<br>\n",
    "4) Lemmatize the words in the Tweet text.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize using our Twitter dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-with-irrelevant-tweets-excluded.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-topic-extraction-ready-tweet-text-with-hashtags-excluded-created-7-29-19.csv\",\n",
    "    \"text_derived\")\n",
    "\n",
    "# Tokenize using our Twitter dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-with-irrelevant-tweets-excluded.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-topic-extraction-ready-user-description-text-with-hashtags-excluded-created-7-29-19.csv\",\n",
    "    \"user_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter in our function call specifies the file path to the dataset to be preprocessed.  The second parameter specifies the location to save the CSV file to.  The 3rd parameter specifies the name of the column in the dataset that contains the original Tweet text.<br>\n",
    "\n",
    "\n",
    "Tweet preprocessing is done via a custom library imported as \"lda_util\" using \"topic_extraction_utility_functions.py\".<br>\n",
    "\n",
    "Refer to URL link for the codebase to the utility functions used above for data preprocessing and below for LDA topic extraction:<br>\n",
    "\n",
    "https://github.com/Calvin-CS/slo-classifiers/blob/master/topic/models/topic_extraction_utility_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and prepare the preprocessed dataset for use in Biterm topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the general format of insertion into a Pandas dataframe, isolating the column of interest, and generating a dictionary of words and a corpus of documents.  Please refer to the code comments for details on the specific steps for the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset (absolute path).\n",
    "tweet_dataset_processed = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "                \"twitter-dataset-7-10-19-topic-extraction-ready-tweet-text-with-hashtags-excluded\"\n",
    "                \"-created-7-29-19-tokenized.csv\", sep=\",\")\n",
    "\n",
    "# # Import the dataset (test/debug).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "#                 \"twitter-dataset-7-10-19-topic-extraction-ready-tweet-text-with-hashtags-excluded\"\n",
    "#                 \"-created-7-30-19-test.csv\", sep=\",\")\n",
    "\n",
    "# Reindex and shuffle the data randomly.\n",
    "tweet_dataset_processed = tweet_dataset_processed.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed.index))\n",
    "\n",
    "# Generate a Pandas dataframe.\n",
    "tweet_text_dataframe = pd.DataFrame(tweet_dataset_processed)\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(f\"\\nThe shape of the Tweet text dataframe:\")\n",
    "log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "log.info(f\"\\nThe columns of the Tweet text dataframe:\")\n",
    "log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Drop any NaN or empty Tweet rows in dataframe (or else CountVectorizer will blow up).\n",
    "tweet_text_dataframe = tweet_text_dataframe.dropna()\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(f\"\\nThe shape of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "log.info(f\"\\nThe columns of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Reindex everything.\n",
    "tweet_text_dataframe.index = pd.RangeIndex(len(tweet_text_dataframe.index))\n",
    "\n",
    "# Assign column names.\n",
    "tweet_text_dataframe_column_names = ['text_derived', 'text_derived_preprocessed', 'text_derived_postprocessed']\n",
    "\n",
    "# Rename column in dataframe.\n",
    "tweet_text_dataframe.columns = tweet_text_dataframe_column_names\n",
    "\n",
    "# Create input feature.\n",
    "selected_features = tweet_text_dataframe[['text_derived_postprocessed']]\n",
    "processed_features = selected_features.copy()\n",
    "\n",
    "# Check what we are using as inputs.\n",
    "log.info(f\"\\nA sample Tweet in our input feature:\")\n",
    "log.info(f\"{processed_features['text_derived_postprocessed'][0]}\\n\")\n",
    "\n",
    "# Create feature set.\n",
    "slo_feature_series = processed_features['text_derived_postprocessed']\n",
    "slo_feature_series = pd.Series(slo_feature_series)\n",
    "slo_feature_list = slo_feature_series.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function contains the code specific to each topic modeling library we utilize.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biterm_topic_model_topic_extraction():\n",
    "    \"\"\"\n",
    "    Function performs topic extraction on Tweets using the Gensim HDP model.\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model.\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(slo_feature_series)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    log.info(f\"\\n.fit_transform - Learn the vocabulary dictionary and return term-document matrix.\")\n",
    "    log.info(f\"{tf}\\n\")\n",
    "    log.info(f\"\\n.get_feature_names - Array mapping from feature integer indices to feature name\")\n",
    "    log.info(f\"{tf_feature_names}\\n\")\n",
    "\n",
    "    # Convert corpus of documents (vectorized text) to numpy array.\n",
    "    tf_array = tf.toarray()\n",
    "\n",
    "    # Convert dictionary of words (vocabulary) to numpy array.\n",
    "    tf_feature_names = np.array(tf_vectorizer.get_feature_names())\n",
    "\n",
    "    # get biterms\n",
    "    biterms = vec_to_biterms(tf_array)\n",
    "\n",
    "    # create btm\n",
    "    btm = oBTM(num_topics=20, V=tf_feature_names)\n",
    "\n",
    "    print(\"\\n\\n Train Online BTM ..\")\n",
    "    for i in range(0, len(biterms), 100):  # prozess chunk of 200 texts\n",
    "        biterms_chunk = biterms[i:i + 100]\n",
    "        btm.fit(biterms_chunk, iterations=50)\n",
    "    topics = btm.transform(biterms)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # print(\"\\n\\n Visualize Topics ..\")\n",
    "    # vis = pyLDAvis.prepare(btm.phi_wz.T, topics, np.count_nonzero(tf_array, axis=1), tf_feature_names, np.sum(tf_array, axis=0))\n",
    "    # pyLDAvis.save_html(vis, './vis/online_btm.html')\n",
    "\n",
    "    print(\"\\n\\n Topic coherence ..\")\n",
    "    topic_summuary(btm.phi_wz.T, tf_array, tf_feature_names, 10)\n",
    "\n",
    "    print(\"\\n\\n Texts & Topics ..\")\n",
    "    for i in range(1, 10):\n",
    "        print(\"{} (topic: {})\".format(slo_feature_series[i], topics[i].argmax()))\n",
    "\n",
    "    # print(\"\\n\\n Texts & Topics ..\")\n",
    "    # for i in range(len(slo_feature_series)):\n",
    "    #     print(\"{} (topic: {})\".format(slo_feature_series[i], topics[i].argmax()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we call the topic modeling function and train it on our Twitter dataset.  We record the time it takes to process the entire dataset and extract topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main function.  Execute the program.\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    my_start_time = time.time()\n",
    "    ################################################\n",
    "    \"\"\"\n",
    "    Perform the topic extraction.\n",
    "    \"\"\"\n",
    "    biterm_topic_model_topic_extraction()\n",
    "\n",
    "    ################################################\n",
    "    my_end_time = time.time()\n",
    "\n",
    "    time_elapsed_in_seconds = (my_end_time - my_start_time)\n",
    "    time_elapsed_in_minutes = (my_end_time - my_start_time) / 60.0\n",
    "    time_elapsed_in_hours = (my_end_time - my_start_time) / 60.0 / 60.0\n",
    "    print(f\"Time taken to process dataset: {time_elapsed_in_seconds} seconds, \"\n",
    "          f\"{time_elapsed_in_minutes} minutes, {time_elapsed_in_hours} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Extraction Results on Twitter Dataset Tweet Text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution run 1."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "(run 1)\n",
    "\n",
    " Topic coherence ..\n",
    "Topic 0 | Coherence=-146.13 | Top words= coal stop labor want climate support project tax new shorten\n",
    "Topic 1 | Coherence=-145.24 | Top words= support stop green labor people climate change queensland government oppose\n",
    "Topic 2 | Coherence=-134.33 | Top words= rail line basin water galilee land support project farmer gas\n",
    "Topic 3 | Coherence=-183.76 | Top words= coal iron ore fortescue year price climate people need stop\n",
    "Topic 4 | Coherence=-147.70 | Top words= coal power energy price new india renewable solar year plant\n",
    "Topic 5 | Coherence=-155.60 | Top words= coal job reef project barrier gas 000 create time stop\n",
    "Topic 6 | Coherence=-156.48 | Top words= gas coal farmer project want people water govt pipeline tell\n",
    "Topic 7 | Coherence=-157.07 | Top words= coal water queensland loan point fund new time year line\n",
    "Topic 8 | Coherence=-166.53 | Top words= coal gas group people seam want plan water climate north\n",
    "Topic 9 | Coherence=-155.40 | Top words= coal india power company stop need year indian plan report\n",
    "Topic 10 | Coherence=-196.49 | Top words= tax pay title native company loan energy slo_cash government island\n",
    "Topic 11 | Coherence=-152.78 | Top words= job coal court people 000 labor 10 time reef queensland\n",
    "Topic 12 | Coherence=-132.85 | Top words= barnaby joyce gas coal project taxpayer money rail new india\n",
    "Topic 13 | Coherence=-161.65 | Top words= job fund want project bank people work tell good public\n",
    "Topic 14 | Coherence=-163.39 | Top words= coal new time need energy job project fund council thank\n",
    "Topic 15 | Coherence=-134.58 | Top words= coal fund loan reef taxpayer want money slo_cashn stop govt\n",
    "Topic 16 | Coherence=-149.31 | Top words= tax pay cut job govt australian billion turnbull dollar point\n",
    "Topic 17 | Coherence=-138.75 | Top words= water support reef risk basin climate world right want artesian\n",
    "Topic 18 | Coherence=-151.31 | Top words= coal labor loan want job billion dollar voter lnp turnbull\n",
    "Topic 19 | Coherence=-128.80 | Top words= cut turnbull billion park push marine week litre groundwater big\n",
    "\n",
    "\n",
    " Texts & Topics ..\n",
    "president national party lobby company represent    (topic: 14)\n",
    "holy shit tho fortescue ascend literally famous price know (topic: 0)\n",
    "good news muswellbrook hospital 's ed redevelopment donation    (topic: 13)\n",
    "important film life depend    (topic: 0)\n",
    "beef fork fork o pork    hahahahha omg í ½í¸í ½í¸í ½í¸slo_hash (topic: 16)\n",
    "yes yes yes action protect collective future (topic: 0)\n",
    "science teach professor laurie santosâ     > > yale happy course online free | think    (topic: 0)\n",
    "exxon mobile chevron shell holding make billion aus resource pay zero tax think profitable 2 business despite corporate tax rate gleefully avoid duly reward aus economy automate job (topic: 16)\n",
    "epa investigate discharge    (topic: 0)\n",
    "\n",
    "\n",
    "Time taken to process dataset: 32306.65420603752 seconds, 538.4442367672921 minutes, 8.974070612788202 hours.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution run 2."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "(run 2)\n",
    "\n",
    " Topic coherence ..\n",
    "Topic 0 | Coherence=-151.18 | Top words= coal support job climate want world right reef risk company\n",
    "Topic 1 | Coherence=-168.36 | Top words= water billion coal basin tax turnbull new fortescue government loan\n",
    "Topic 2 | Coherence=-143.13 | Top words= tax company rail gas money stop australian coal line government\n",
    "Topic 3 | Coherence=-138.24 | Top words= job 000 coal reef create project 10 want turnbull renewable\n",
    "Topic 4 | Coherence=-141.34 | Top words= coal native title land court queensland stop government owner traditional\n",
    "Topic 5 | Coherence=-138.89 | Top words= coal labor stop loan fund want project election rail shorten\n",
    "Topic 6 | Coherence=-156.23 | Top words= coal green labor support people stop farmer vote oppose protest\n",
    "Topic 7 | Coherence=-158.98 | Top words= coal new gas project india plan price government australian report\n",
    "Topic 8 | Coherence=-182.12 | Top words= coal new australian loan gas power report good help minister\n",
    "Topic 9 | Coherence=-152.07 | Top words= tax pay coal ½í energy year company ½í² origin ¼í\n",
    "Topic 10 | Coherence=-160.66 | Top words= coal new point court reef project job australian labor approval\n",
    "Topic 11 | Coherence=-195.12 | Top words= gas project water coal narrabri time seam farmer ore iron\n",
    "Topic 12 | Coherence=-168.23 | Top words= job pay india money power council bank taxpayer build tax\n",
    "Topic 13 | Coherence=-136.61 | Top words= coal reef climate barrier future energy people clean fund need\n",
    "Topic 14 | Coherence=-151.80 | Top words= coal want time money fund loan canavan bank matt project\n",
    "Topic 15 | Coherence=-164.49 | Top words= tax want pay joyce project fund barnaby people minister new\n",
    "Topic 16 | Coherence=-132.85 | Top words= barnaby joyce india think money taxpayer rail spend come govt\n",
    "Topic 17 | Coherence=-153.26 | Top words= coal loan water slo_cashn point government rail break port queensland\n",
    "Topic 18 | Coherence=-138.77 | Top words= coal stop reef want fund climate need money labor public\n",
    "Topic 19 | Coherence=-182.66 | Top words= fortescue group metal site year court prevent owner respect traditional\n",
    "\n",
    "\n",
    " Texts & Topics ..\n",
    "footage release rubbish claim maules creek quiet library    (topic: 10)\n",
    "mgt finally sell asset    wo far pay usslo_cash debt    $ sto    (topic: 16)\n",
    "josh frydenburg dual citizenship hungary think (topic: 0)\n",
    "permanent worker sack hail creek coal ​the cfmeu demand explanation 's dec    (topic: 10)\n",
    ".slo_mention company risk help tell aecom company risk away       (topic: 1)\n",
    "'s climate bomb senselessness 's coal     (topic: 13)\n",
    "reminder twice people work reef area tourism coal entire state queensland risk real job imaginary job       (topic: 0)\n",
    "reef killer    (topic: 0)\n",
    "canavan little desperate ahead crazy desperate make wonder promise í ¾í´    (topic: 14)\n",
    "\n",
    "\n",
    "Time taken to process dataset: 31171.22075152397 seconds, 519.5203458587329 minutes, 8.65867243097888 hours.\n",
    "\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results display each topic along with their topic coherence metric values as well as the top 10 words associated with each topic.  A sample of the Tweets in the dataset with their assigned topics is also given.  The library for biterm takes the longest to process using default hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://pypi.org/project/biterm/\n",
    "    - Biterm Python library we utilize. (Linux OS only)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
