{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Biterm Topic Model Implementation on SLO Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import logging as log\n",
    "import warnings\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "from biterm.cbtm import oBTM\n",
    "from biterm.utility import vec_to_biterms, topic_summuary  # helper functions\n",
    "\n",
    "# Import custom utility functions.\n",
    "# import topic_extraction_utility_functions as lda_util\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Pandas options.\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "pd.options.display.max_colwidth = 1000\n",
    "# Pandas float precision display.\n",
    "pd.set_option('precision', 12)\n",
    "# Seaborn setting.\n",
    "sns.set()\n",
    "# Don't output these types of warnings to terminal.\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "# Matplotlib log settings.\n",
    "mylog = log.getLogger(\"matplotlib\")\n",
    "mylog.setLevel(log.INFO)\n",
    "\n",
    "\"\"\"\n",
    "Turn debug log statements for various sections of code on/off.\n",
    "(adjust log level as necessary)\n",
    "\"\"\"\n",
    "log.basicConfig(level=log.INFO)\n",
    "log.disable(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process and Post-process Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on our topic modeling dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-test-subset-100-examples.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-lda-ready-tweet-text-with-hashtags-excluded-created-7-17-19.csv\",\n",
    "    \"text_derived\")\n",
    "\n",
    "# Test on our topic modeling dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-test-subset-100-examples.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-lda-ready-user-description-text-with-hashtags-excluded-created-7-17-19.csv\",\n",
    "    \"user_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and prepare the preprocessed dataset for use in Biterm topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the dataset (relative path).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"twitter-dataset-7-10-19-lda-ready-tweet-text-with-hashtags-excluded-created-7-17-19.csv\", sep=\",\")\n",
    "\n",
    "# Import the dataset (absolute path).\n",
    "tweet_dataset_processed = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "                \"twitter-dataset-7-10-19-lda-ready-tweet-text-with-hashtags-excluded-created-7-17-19.csv\", sep=\",\")\n",
    "\n",
    "# # Import the dataset (test/debug).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"twitter-dataset-7-10-19-lda-ready-tweet-text-test.csv\", sep=\",\")\n",
    "\n",
    "# # Import the dataset (test/debug).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "#                 \"twitter-dataset-7-10-19-lda-ready-tweet-text-test.csv\", sep=\",\")\n",
    "\n",
    "# Reindex and shuffle the data randomly.\n",
    "tweet_dataset_processed = tweet_dataset_processed.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed.index))\n",
    "\n",
    "# Generate a Pandas dataframe.\n",
    "tweet_text_dataframe = pd.DataFrame(tweet_dataset_processed)\n",
    "\n",
    "# # Print shape and column names.\n",
    "# log.info(f\"\\nThe shape of the Tweet text dataframe:\")\n",
    "# log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "# log.info(f\"\\nThe columns of the Tweet text dataframe:\")\n",
    "# log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(\"\\nThe shape of the Tweet text dataframe:\")\n",
    "log.info(tweet_text_dataframe.shape)\n",
    "log.info(\"\\nThe columns of the Tweet text dataframe:\")\n",
    "log.info(tweet_text_dataframe.columns)\n",
    "\n",
    "# Drop any NaN or empty Tweet rows in dataframe (or else CountVectorizer will blow up).\n",
    "tweet_text_dataframe = tweet_text_dataframe.dropna()\n",
    "\n",
    "# # Print shape and column names.\n",
    "# log.info(f\"\\nThe shape of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "# log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "# log.info(f\"\\nThe columns of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "# log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(\"\\nThe shape of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(tweet_text_dataframe.shape)\n",
    "log.info(\"\\nThe columns of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(tweet_text_dataframe.columns)\n",
    "\n",
    "# Reindex everything.\n",
    "tweet_text_dataframe.index = pd.RangeIndex(len(tweet_text_dataframe.index))\n",
    "\n",
    "# Assign column names.\n",
    "tweet_text_dataframe_column_names = ['text_derived', 'text_derived_preprocessed', 'text_derived_postprocessed']\n",
    "\n",
    "# Rename column in dataframe.\n",
    "tweet_text_dataframe.columns = tweet_text_dataframe_column_names\n",
    "\n",
    "# Create input feature.\n",
    "selected_features = tweet_text_dataframe[['text_derived_postprocessed']]\n",
    "processed_features = selected_features.copy()\n",
    "\n",
    "# # Check what we are using as inputs.\n",
    "# log.info(f\"\\nA sample Tweet in our input feature:\")\n",
    "# log.info(f\"{processed_features['text_derived_postprocessed'][0]}\\n\")\n",
    "\n",
    "# Check what we are using as inputs.\n",
    "log.info(\"\\nA sample Tweet in our input feature:\")\n",
    "log.info(processed_features['text_derived_postprocessed'][0])\n",
    "\n",
    "# Create feature set.\n",
    "slo_feature_series = processed_features['text_derived_postprocessed']\n",
    "slo_feature_series = pd.Series(slo_feature_series)\n",
    "slo_feature_list = slo_feature_series.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def biterm_topic_model_topic_extraction():\n",
    "    \"\"\"\n",
    "    Function performs topic extraction on Tweets using the Gensim HDP model.\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model.\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(slo_feature_series)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    # log.info(f\"\\n.fit_transform - Learn the vocabulary dictionary and return term-document matrix.\")\n",
    "    # log.info(f\"{tf}\\n\")\n",
    "    # log.info(f\"\\n.get_feature_names - Array mapping from feature integer indices to feature name\")\n",
    "    # log.info(f\"{tf_feature_names}\\n\")\n",
    "\n",
    "    log.info(\"\\n.fit_transform - Learn the vocabulary dictionary and return term-document matrix.\")\n",
    "    log.info(tf)\n",
    "    log.info(\"\\n.get_feature_names - Array mapping from feature integer indices to feature name\")\n",
    "    log.info(tf_feature_names)\n",
    "\n",
    "    # Convert corpus of documents (vectorized text) to numpy array.\n",
    "    tf_array = tf.toarray()\n",
    "\n",
    "    # Convert dictionary of words (vocabulary) to numpy array.\n",
    "    tf_feature_names = np.array(tf_vectorizer.get_feature_names())\n",
    "\n",
    "    # get biterms\n",
    "    biterms = vec_to_biterms(tf_array)\n",
    "\n",
    "    # create btm\n",
    "    btm = oBTM(num_topics=20, V=tf_feature_names)\n",
    "\n",
    "    print(\"\\n\\n Train Online BTM ..\")\n",
    "    for i in range(0, len(biterms), 100):  # prozess chunk of 200 texts\n",
    "        biterms_chunk = biterms[i:i + 100]\n",
    "        btm.fit(biterms_chunk, iterations=50)\n",
    "    topics = btm.transform(biterms)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # print(\"\\n\\n Visualize Topics ..\")\n",
    "    # vis = pyLDAvis.prepare(btm.phi_wz.T, topics, np.count_nonzero(tf_array, axis=1), tf_feature_names, np.sum(tf_array, axis=0))\n",
    "    # pyLDAvis.save_html(vis, './vis/online_btm.html')\n",
    "\n",
    "    print(\"\\n\\n Topic coherence ..\")\n",
    "    topic_summuary(btm.phi_wz.T, tf_array, tf_feature_names, 10)\n",
    "\n",
    "    print(\"\\n\\n Texts & Topics ..\")\n",
    "    for i in range(len(slo_feature_series)):\n",
    "        print(\"{} (topic: {})\".format(slo_feature_series[i], topics[i].argmax()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main function.  Execute the program.\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    my_start_time = time.time()\n",
    "    ################################################\n",
    "    \"\"\"\n",
    "    Perform the topic extraction.\n",
    "    \"\"\"\n",
    "    biterm_topic_model_topic_extraction()\n",
    "\n",
    "    ################################################\n",
    "    my_end_time = time.time()\n",
    "\n",
    "    time_elapsed_in_seconds = (my_end_time - my_start_time)\n",
    "    time_elapsed_in_minutes = (my_end_time - my_start_time) / 60.0\n",
    "    time_elapsed_in_hours = (my_end_time - my_start_time) / 60.0 / 60.0\n",
    "    # print(f\"Time taken to process dataset: {time_elapsed_in_seconds} seconds, \"\n",
    "    #       f\"{time_elapsed_in_minutes} minutes, {time_elapsed_in_hours} hours.\")\n",
    "    print(\"\\n\\nTime taken to process dataset: \" + str(time_elapsed_in_seconds) + \" seconds, \" +\n",
    "          str(time_elapsed_in_minutes) + \" minutes, \" + str(time_elapsed_in_hours) + \" hours.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Extraction Results on Twitter Dataset Tweet Text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was stupid and didn't limit the print statement for displaying Tweets and their topic assignments, therefore it printed all 650k+ Tweets and I couldn't scroll back far enough to see the topic coherence data and actual topics.  Waiting on a 2nd and 3rd execution run simulatenously (limited to displaying first 10 Tweets and their associated topics this time around)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Biterm Output: (subset of all output)\n",
    "\n",
    "(not the topics - just example of Tweets with assigned topics )\n",
    " world record objection 276 submission prepare submit invasive gas field    (topic: 13)\n",
    "accenture join bhp set 50/50 gender target    (topic: 12)\n",
    "colin barnett wrong iron ore bhp boss    (topic: 6)\n",
    "approval adani queensland coalmine face legal challenge    hint (topic: 11)\n",
    "declare buy 5 large parcel land 15 minute drive plan route        (topic: 18)\n",
    "having whale time fortescue bay pic ig    (topic: 6)\n",
    "question greens want govern australia greens voter support lnp continual attack australian defend attack labor lnp (topic: 17)\n",
    "courage tear adani 's license people australia new legal advice show adani wo n’t able damn thing    (topic: 11)\n",
    "'s critical adani need controversial government loan    (topic: 0)\n",
    "money matters 10 thing need know open bell spy spx qqq dia jpm aapl    (topic: 0)\n",
    "right alp reconsider support adani    24 hour convince reject    email labor cabinet tell > >    (topic: 11)\n",
    "annastacia palaszczuk billion dollar handout billionaire adani sign petition    (topic: 15)\n",
    "dr nahan payroll tax hike tax job large employer coles woolies rio tinto etc (topic: 12)\n",
    "matt canavan delusional think seq (topic: 0)\n",
    "great silence single weatherill minister available talk listener bhp job hit (topic: 0)\n",
    "australia 's climate bomb senselessness adani carmichael coal    (topic: 4)\n",
    "listen local farmer tourism operator want protect land water coral    (topic: 17)\n",
    "\n",
    "\n",
    "Time taken to process dataset: 39906.75524163246 seconds, 665.112587360541 minutes, 11.08520978934235 hours.\n",
    "\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "(run 1)\n",
    "Placeholder.\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "(run 2)\n",
    "Placeholder.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
