{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Biterm Topic Modeling Algorithm Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook file provides a very simple high-level overview of the Biterm topic modeling algorithm.  We briefly discuss the plate notation diagram, pseudocode, and statistical formula for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biterm Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This description (text) obtained from (https://github.com/bnosac/BTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Biterm Topic Model (BTM) is a word co-occurrence based topic model that learns topics by modeling word-word co-occurrences patterns (e.g., biterms)\n",
    "\n",
    "* A biterm consists of two words co-occurring in the same context, for example, in the same short text window.\n",
    "* BTM models the biterm occurrences in a corpus (unlike LDA models which model the word occurrences in a document).\n",
    "* It's a generative model. In the generation procedure, a biterm is generated by drawing two words independently from a same topic $z$. In other words, the distribution of a biterm $b=(wi,wj)$ is defined as: $P(b) = \\sum_{k}{P(wi|z)*P(wj|z)*P(z)}$ where $k$ is the number of topics you want to extract.\n",
    "* Estimation of the topic model is done with the Gibbs sampling algorithm. Where estimates are provided for $P(w|k)=phi$ and $P(z)=theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![biterm](../images/biterm_visualization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plate Notation for the Biterm Algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![biterm](../images/biterm_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the notation for Figure 1.c:\n",
    "\n",
    "$\\theta$ - topic distribution for the entire corpus.\n",
    "\n",
    "$\\phi$ - topic-specific word distribution.\n",
    "\n",
    "$B$ - the entire set of biterms for the corpus.\n",
    "\n",
    "$K$ - the entire set of topics for the corpus.\n",
    "\n",
    "$z$ - a single topic.\n",
    "\n",
    "$w_{i}$ - a single word.\n",
    "\n",
    "$w_{j}$ - a single word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Formula for Calculating the Biterm algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each topic z\n",
    "    - (a) draw a topic-speciﬁc word distribution φz ∼ Dir(β)\n",
    "    \n",
    "    \n",
    "2. Draw a topic distribution θ ∼ Dir(α) for the whole collection\n",
    "\n",
    "\n",
    "3. For each biterm b in the biterm set B\n",
    "    - (a)\tdraw a topic assignment z ∼ Multi(θ)\n",
    "    - (b)\tdraw two words: wi,wj ∼ Mulit(φz )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![biterm formula](../images/biterm_equation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta$ - topic distribution for the entire corpus.\n",
    "\n",
    "$\\phi$ - topic-specific word distribution.\n",
    "\n",
    "$b = (w_{i}, w_{j})$ - a single biterm (word co-occurence pair).\n",
    "\n",
    "$B$ - the entire set of biterms for the corpus.\n",
    "\n",
    "$z$ - a single topic.\n",
    "\n",
    "$w_{i}$ - a single word.\n",
    "\n",
    "$w_{j}$ - a single word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode for the Gibbs Sampler for the Biterm algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![biterm pseudocode](../images/biterm_pseudocode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizes collapsed Gibbs sampling for approximate (not exact) inferencing to compute $\\phi$ and $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![biterm formula 2](../images/biterm_equation_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the notation present in the pseudocode and equation 5 + 6 above:\n",
    "\n",
    "$\\alpha$ - a Dirichlet distribution prior.\n",
    "\n",
    "$\\beta$ - a Dirichlet distribution prior.\n",
    "\n",
    "$\\theta$ - topic distribution for the entire corpus.\n",
    "\n",
    "$\\phi$ - topic-specific word distribution.\n",
    "\n",
    "$b = (w_{i}, w_{j})$ - a single biterm (word co-occurence pair).\n",
    "\n",
    "$B$ - the entire set of biterms for the corpus.\n",
    "\n",
    "$z$ - a single topic.\n",
    "\n",
    "$w_{i}$ - a single word.\n",
    "\n",
    "$w_{j}$ - a single word.\n",
    "\n",
    "$z_{b}$ - the topic assignment for biterm $b$?\n",
    "\n",
    "$z_{-b}$ - the topic assignments for all biterms except $b$.\n",
    "\n",
    "$n_{z}$ - the # of times biterm $b$ is assigned to topic $z$.\n",
    "\n",
    "$n_{w|z}$ - the # of times word $w$ is assigned to topic $z$.\n",
    "\n",
    "$|B|$ - the total # of biterms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simplified Biterm Topic Modeling Algorithm Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder.\n",
    "\n",
    "**TODO - implement simple hand-worked example of one iteration through the algorithm (provided we can find an example)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion of the FIRST iteration of the Biterm algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rinse and repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Referenced:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://en.wikipedia.org/wiki/Greek_alphabet\n",
    "    - for Greek alphabet name reference.\n",
    "    \n",
    "\n",
    "- https://sutheeblog.wordpress.com/2017/03/20/a-biterm-topic-model-for-short-texts/\n",
    "    - blog explaining the biterm topic model in a more palatable way.\n",
    "\n",
    "\n",
    "- https://www.cs.toronto.edu/~jstolee/projects/topic.pdf\n",
    "    - contains a short section explaining the algorithm; includes plate notation diagram.\n",
    "    \n",
    "\n",
    "- https://stackoverflow.com/questions/29786985/whats-the-disadvantage-of-lda-for-short-texts\n",
    "    - contains a response by the author of the biterm model.\n",
    " \n",
    " \n",
    "- https://github.com/bnosac/BTM\n",
    "    - contains a nice brief description of what a Biterm is and Biterm Model is.\n",
    "    \n",
    "\n",
    "- https://www.slideserve.com/baeddan-williams/a-biterm-topic-model-for-short-texts\n",
    "    - presentation slides containing a nice visual representation of the BTM.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
