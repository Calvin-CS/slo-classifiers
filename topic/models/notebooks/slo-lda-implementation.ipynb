{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation Topic Model Implementation on SLO Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "</p>Our implementation utilizes the Scikit-Learn LatentDirichletAllocation class and the Python \"lda\" library.  We utilize Scikit-Learn's GridSearchCV class to perform an exhaustive grid search for the optimal hyperparameters to fit our Twitter dataset.  We preprocess our raw Twitter dataset before running multiple iterations of the LDA algorithm with the following specified number of topics: 3, 6, 12, and 20.  We limit each topic to the top 10 words that describe that topic.</p><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Adjust log verbosity levels as necessary.<br>\n",
    "\n",
    "Set to \"DEBUG\" to view all debug output.<br>\n",
    "Set to \"INFO\" to view useful information on dataframe shape, etc.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import logging as log\n",
    "import warnings\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import custom utility functions.\n",
    "import topic_extraction_utility_functions as lda_util\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Pandas options.\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "pd.options.display.max_colwidth = 1000\n",
    "# Pandas float precision display.\n",
    "pd.set_option('precision', 12)\n",
    "# Don't output these types of warnings to terminal.\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "# Matplotlib log settings.\n",
    "mylog = log.getLogger(\"matplotlib\")\n",
    "mylog.setLevel(log.INFO)\n",
    "\n",
    "\"\"\"\n",
    "Turn debug log statements for various sections of code on/off.\n",
    "(adjust log level as necessary)\n",
    "\"\"\"\n",
    "log.basicConfig(level=log.INFO)\n",
    "log.disable(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process and Post-process Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "We preprocess our Twitter dataset as follows:<br>\n",
    "\n",
    "1) Downcase all text.<br>\n",
    "2) Check that there is text, otherwise convert to empty string.<br>\n",
    "3) Convert html chars. to unicode chars.<br>\n",
    "4) Remove \"RT\" tags.<br>\n",
    "5) Remove concatenated URL's.<br>\n",
    "6) Handle whitespaces by converting all/multiple whitespace characters to a single whitespace.<br>\n",
    "7) Remove URL's and replace with \"slo_url\".<br>\n",
    "8) Remove Tweet mentions and replace with \"slo_mention\".<br>\n",
    "9) Remove Tweet stock symbols and replace with \"slo_stock\".<br>\n",
    "10) Remove Tweet hashtags and replace with \"slo_hash\".<br>\n",
    "11) Remove Tweet cashtags and replace with \"slo_cash\".<br>\n",
    "12) Remove Tweet year and replace with \"slo_year\".<br>\n",
    "13) Remove Tweet time and replace with \"slo_time\".<br>\n",
    "14) Remove character elongations.<br>\n",
    "\n",
    "We postprocess our Twitter dataset as follows:<br>\n",
    "\n",
    "1) Remove the following irrelevant words specified in the List below:<br>\n",
    "\n",
    "    delete_list = [\"word_n\", \"auspol\", \"ausbiz\", \"tinto\", \"adelaide\", \"csg\", \"nswpol\",\n",
    "                   \"nsw\", \"lng\", \"don\", \"rio\", \"pilliga\", \"australia\", \"asx\", \"just\", \"today\", \"great\", \"says\", \"like\",\n",
    "                   \"big\", \"better\", \"rite\", \"would\", \"SCREEN_NAME\", \"mining\", \"former\", \"qldpod\", \"qldpol\", \"qld\", \"wr\",\n",
    "                   \"melbourne\", \"andrew\", \"fuck\", \"spadani\", \"greg\", \"th\", \"australians\", \"http\", \"https\", \"rt\",\n",
    "                   \"co\", \"amp\", \"carmichael\", \"abbot\", \"bill shorten\",\n",
    "                   \"slo_url\", \"slo_mention\", \"slo_hash\", \"slo_year\", \"slo_time\", \"slo_cash\", \"slo_stock\",\n",
    "                   \"adani\", \"bhp\", \"cuesta\", \"fotescue\", \"riotinto\", \"newmontmining\", \"santos\", \"oilsearch\",\n",
    "                   \"woodside\", \"ilukaresources\", \"whitehavencoal\",\n",
    "                   \"stopadani\", \"goadani\", \"bhpbilliton\", \"billiton\", \"cuestacoal\", \"cuests coal\", \"cqc\",\n",
    "                   \"fortescuenews\", \"fortescue metals\", \"rio tinto\", \"newmont\", \"newmont mining\", \"santosltd\",\n",
    "                   \"oilsearchltd\", \"oil search\", \"woodsideenergy\", \"woodside petroleum\", \"woodside energy\",\n",
    "                   \"iluka\", \"iluka resources\", \"whitehaven\", \"whitehaven coal\"]\n",
    "\n",
    "2) Remove all punctuation from the Tweet text.<br>\n",
    "3) Remove all English stop words from the Tweet text.<br>\n",
    "4) Lemmatize the words in the Tweet text.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize using our Twitter dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-with-irrelevant-tweets-excluded.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-topic-extraction-ready-tweet-text-with-hashtags-excluded-created-7-29-19.csv\",\n",
    "    \"text_derived\")\n",
    "\n",
    "# Tokenize using our Twitter dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-with-irrelevant-tweets-excluded.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-topic-extraction-ready-user-description-text-with-hashtags-excluded-created-7-29-19.csv\",\n",
    "    \"user_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The first parameter in our function call specifies the file path to the dataset to be preprocessed.  The second parameter specifies the location to save the CSV file to.  The 3rd parameter specifies the name of the column in the dataset that contains the original Tweet text.<br>\n",
    "\n",
    "\n",
    "Tweet preprocessing is done via a custom library imported as \"lda_util\" using \"topic_extraction_utility_functions.py\".<br>\n",
    "\n",
    "Refer to URL link for the codebase to the utility functions used above for data preprocessing and below for LDA topic extraction:<br>\n",
    "\n",
    "https://github.com/Calvin-CS/slo-classifiers/blob/master/topic/models/topic_extraction_utility_functions.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and prepare the preprocessed dataset for use in LDA topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "Refer to the code comments for the specific steps performed.<br>\n",
    "Note that we have to use absolute file paths in Jupyter notebook as opposed to relative file paths in PyCharm.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "\n",
      "INFO:root:The shape of our preprocessed SLO dataframe with NaN (empty) rows dropped:\n",
      "INFO:root:(653094, 1)\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:The columns of our preprocessed SLO dataframe with NaN (empty) rows dropped:\n",
      "INFO:root:<bound method NDFrame.head of                                                   tweet_t\n",
      "134130  every australian politician should be watching...\n",
      "109525  exciting to see the results of continued commu...\n",
      "175975  breaking will proceed on but it doesnt have th...\n",
      "221807                 already wtf cant do anything right\n",
      "476425  breaking 4 people occupy a coal train at willo...\n",
      "...                                                   ...\n",
      "434193  others have left holes why we never really got...\n",
      "231520  so confirms they are exploring selling and or ...\n",
      "14479   power refusing to c oo perate with credit rati...\n",
      "38365   suppos mine if it stack up environmentally doe...\n",
      "656954  queensland tourism award winner rejects adanis...\n",
      "\n",
      "[653094 rows x 1 columns]>\n",
      "INFO:root:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset (absolute path).\n",
    "tweet_dataset_processed = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "                \"twitter-dataset-7-10-19-topic-extraction-ready-tweet-text-with-hashtags-excluded\"\n",
    "                \"-created-7-29-19-tokenized.csv\", sep=\",\")\n",
    "\n",
    "# # Import the dataset (test/debug).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "#                 \"twitter-dataset-7-10-19-topic-extraction-ready-tweet-text-with-hashtags-excluded\"\n",
    "#                 \"-created-7-30-19-test.csv\", sep=\",\")\n",
    "\n",
    "# Reindex and shuffle the data randomly.\n",
    "tweet_dataset_processed = tweet_dataset_processed.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed.index))\n",
    "\n",
    "# Generate a Pandas dataframe.\n",
    "tweet_text_dataframe = pd.DataFrame(tweet_dataset_processed)\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(f\"\\nThe shape of the Tweet text dataframe:\")\n",
    "log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "log.info(f\"\\nThe columns of the Tweet text dataframe:\")\n",
    "log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Drop any NaN or empty Tweet rows in dataframe (or else CountVectorizer will blow up).\n",
    "tweet_text_dataframe = tweet_text_dataframe.dropna()\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(f\"\\nThe shape of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "log.info(f\"\\nThe columns of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Reindex everything.\n",
    "tweet_text_dataframe.index = pd.RangeIndex(len(tweet_text_dataframe.index))\n",
    "\n",
    "# Assign column names.\n",
    "tweet_text_dataframe_column_names = ['text_derived', 'text_derived_preprocessed', 'text_derived_postprocessed']\n",
    "\n",
    "# Rename column in dataframe.\n",
    "tweet_text_dataframe.columns = tweet_text_dataframe_column_names\n",
    "\n",
    "# Create input feature.\n",
    "selected_features = tweet_text_dataframe[['text_derived_postprocessed']]\n",
    "processed_features = selected_features.copy()\n",
    "\n",
    "# Check what we are using as inputs.\n",
    "log.info(f\"\\nA sample Tweet in our input feature:\")\n",
    "log.info(f\"{processed_features['text_derived_postprocessed'][0]}\\n\")\n",
    "\n",
    "# Create feature set.\n",
    "slo_feature_series = processed_features['text_derived_postprocessed']\n",
    "slo_feature_series = pd.Series(slo_feature_series)\n",
    "slo_feature_list = slo_feature_series.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The above log.INFO messages depict the shape and contents of the preprocessed dataframe after dropping any rows that are just \"NaN\", indicating the Tweet was full of irrelevant words and is now empty due to the removal of those irrelevant words.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the topic extraction (uses the stance detection tokenized dataset):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We use the Scikit-Learn CountVectorizer class to vectorize our categorical Tweet data.  We set the max_features parameter to 1000 to indicate a maximum vocabulary of 1k words based on the 1000 words with the highest term frequencies.  We set the stop_words parameter to \"English\" to indicate we would like to remove English stop words based on a built-in library of stop words.  We set the min_df and max_df parameters to indicate the words with the threshold term frequencies at which we ignore those words and do not include them in our vocabulary.<br>\n",
    "\n",
    "We use the Scikit-Learn LatentDirichletAllocation class with the below hyperparameters to train and fit our Tweet data.  The parameter n_topics controls the # of topics we would like to extract for topic modeling.  The parameter max_iter controls the # of iterations to perform LDA before we cease.  The parameter learning_method controls the method by which we update the words in our topics.  <br>\n",
    "\n",
    "We use a utility function to display Topics 1-20 and the top 10 words associated with each Topic.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:The time taken to perform the operation is: \n",
      "INFO:root:439.08266735076904\n",
      "INFO:root:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "money slocashn coal work minister ceo taxpayers banks join use\n",
      "Topic 1:\n",
      "cou public lnp community away shares townsville end workers best\n",
      "Topic 2:\n",
      "people time need group thanks come thing cut latest times\n",
      "Topic 3:\n",
      "water reef land farmers barrier help state free adanis plan\n",
      "Topic 4:\n",
      "going narrabri barnaby does fight joyce business pm massive australias\n",
      "Topic 5:\n",
      "stop new coal government wont breaking approval plans premier carbon\n",
      "Topic 6:\n",
      "climate change greens adani vote biggest council companies week coal\n",
      "Topic 7:\n",
      "oil tell political coal hey bad clear taxpayer noh message\n",
      "Topic 8:\n",
      "queensland funding world risk coal groundwater looks access policy paying\n",
      "Topic 9:\n",
      "govt coal india shoen coalmine deal wants years local build\n",
      "Topic 10:\n",
      "adanis project coal turnbull point basin right did protect pollution\n",
      "Topic 11:\n",
      "labor alp way repo coal canavan look really lnp corruption\n",
      "Topic 12:\n",
      "jobs say news create board planet high finance abc thousands\n",
      "Topic 13:\n",
      "rail environmental environment disaster licence gov sign infrastructure don health\n",
      "Topic 14:\n",
      "coal good company power future ahead day mines bank global\n",
      "Topic 15:\n",
      "loan foescue federal make fund line election indian know naif\n",
      "Topic 16:\n",
      "coal gas suppo dont project said po seam thats country\n",
      "Topic 17:\n",
      "iron ore doesnt industry needs coal giant national politicians dam\n",
      "Topic 18:\n",
      "pay energy galilee think beach clean native decision title voters\n",
      "Topic 19:\n",
      "tax want australian action year billion paid giving green stand\n"
     ]
    }
   ],
   "source": [
    "def latent_dirichlet_allocation_topic_extraction():\n",
    "    \"\"\"\n",
    "    Function performs topic extraction on Tweets using Scikit-Learn LDA model.\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model.\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(slo_feature_series)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    # Run LDA.\n",
    "    lda = LatentDirichletAllocation(n_components=20, max_iter=5, learning_method='online', learning_offset=50.,\n",
    "                                    random_state=0).fit(tf)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Display the top words for each topic.\n",
    "    lda_util.display_topics(lda, tf_feature_names, 10)\n",
    "\n",
    "    import pyLDAvis\n",
    "    from pyLDAvis import sklearn\n",
    "    # pyLDAvis.enable_notebook()\n",
    "    visualization = sklearn.prepare(lda_model=lda, vectorizer=tf_vectorizer, dtm=tf)\n",
    "    pyLDAvis.save_html(visualization, 'lda_visualization-no-company-words.html')\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Perform the topic extraction.\n",
    "\"\"\"\n",
    "latent_dirichlet_allocation_topic_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We cannot seem to find any strong correlation between the 10 words in each Topic such that we could assign an English descriptor to each topic, such as \"economic\", \"environmental\", \"social\", etc.\n",
    "\n",
    "Of interesting note is that it appears to take longer to perform LDA topic extraction specifying fewer topics over more topics.  We surmise this is because we have a large dataset of 650k+ Tweets which translates to 650k+ different documents in our corpus.  Therefore, it would take the algorithm less time if it could simply assign 650k+ documents to 650k+ different topics rather than having to assign 650k+ documents to a mere 3 topics or in general a much smaller number of topics in comparison to the number of documents.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Topic Extraction using the \"lda\" library and collapsed Gibbs Sampling ((uses the stance detection tokenized dataset):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The code below uses the \"lda\" Python library package that performs LDA topic extraction using collapsed Gibbs Sampling.<br>\n",
    "This is different from the Scikit-Learn implementation that uses online variational inference.<br>\n",
    "Otherwise, the dataset is the same and we are still using Scikit-Learn's CountVectorizer class to vectorize our data.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 653094\n",
      "INFO:lda:vocab_size: 1000\n",
      "INFO:lda:n_words: 3267212\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -33566606\n",
      "INFO:lda:<10> log likelihood: -27631270\n",
      "INFO:lda:<20> log likelihood: -24168941\n",
      "INFO:lda:<30> log likelihood: -23191677\n",
      "INFO:lda:<40> log likelihood: -22881500\n",
      "INFO:lda:<50> log likelihood: -22754208\n",
      "INFO:lda:<60> log likelihood: -22681384\n",
      "INFO:lda:<70> log likelihood: -22639373\n",
      "INFO:lda:<80> log likelihood: -22612153\n",
      "INFO:lda:<90> log likelihood: -22593660\n",
      "INFO:lda:<99> log likelihood: -22575655\n",
      "INFO:root:The time taken to perform the operation is: \n",
      "INFO:root:90.21567153930664\n",
      "INFO:root:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: coal energy future clean fossil climate carbon time global need\n",
      "Topic 1: water free billion owners coal traditional unlimited farmers giving dollars\n",
      "Topic 2: cou coal native title stop adanis approval federal land turnbull\n",
      "Topic 3: labor greens stop lnp alp vote shoen election suppo want\n",
      "Topic 4: coal thanks latest times australian adanis green bank repo govt\n",
      "Topic 5: coal fund money banks project funding govt adanis wont taxpayers\n",
      "Topic 6: people action stop protest join day protesters campaign time message\n",
      "Topic 7: gas project coal narrabri seam forest water farmers barnaby pipeline\n",
      "Topic 8: beach dam watch day tour video brazil story iluka disaster\n",
      "Topic 9: water basin aesian environmental risk coal right world health suppo\n",
      "Topic 10: foescue shares group metals debt profit year loss fmg news\n",
      "Topic 11: loan canavan minister taxpayer slocashn matt board joyce barnaby money\n",
      "Topic 12: loan rail line adanis coal galilee naif veto basin government\n",
      "Topic 13: coal new power india point mines solar adanis po stranded\n",
      "Topic 14: tax pay paid australian corporate company energy donations companies chevron\n",
      "Topic 15: coal good company corruption deal away govt record news track\n",
      "Topic 16: iron ore oil ceo new production boss price search tintos\n",
      "Topic 17: reef climate coal barrier change save stop protect coral kill\n",
      "Topic 18: want dont doesnt need coal suppo make money townsville adanis\n",
      "Topic 19: jobs create thousands coal 10000 tourism job thats pm turnbull\n"
     ]
    }
   ],
   "source": [
    "def latent_dirichlet_allocation_collapsed_gibbs_sampling():\n",
    "    \"\"\"\n",
    "    Functions performs LDA topic extraction using collapsed Gibbs Sampling.\n",
    "\n",
    "    https://pypi.org/project/lda/\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    import lda\n",
    "\n",
    "    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model.\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(slo_feature_series)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    # Train and fit the LDA model.\n",
    "    model = lda.LDA(n_topics=12, n_iter=1000, random_state=1)\n",
    "    model.fit(tf)  # model.fit_transform(X) is also available\n",
    "    topic_word = model.topic_word_  # model.components_ also works\n",
    "    n_top_words = 10\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Display the topics and the top words associated with.\n",
    "    for i, topic_dist in enumerate(topic_word):\n",
    "        topic_words = np.array(tf_feature_names)[np.argsort(topic_dist)][:-(n_top_words + 1):-1]\n",
    "        print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "        \n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "Perform the topic extraction using collapsed Gibbs Sampling.\n",
    "\"\"\"\n",
    "latent_dirichlet_allocation_collapsed_gibbs_sampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The results seem to be as incoherent as the Scikit-Learn implementation of LDA topic extraction using online variational inference.<br>\n",
    "\n",
    "It's difficult to see any correlation between the 10 top words for each topic.<br>\n",
    "\n",
    "Here, we are using n_iter=100 (iterations) as the fitting to our Twitter data is a lot faster than the Scikit-Learn implementation where max_iter=5 already takes 450 seconds.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Updated Topic Extraction Results on Twitter Topic Modeling Dataset Tweet Text (not the stance detection tokenized dataset):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First execution of LDA on our tokenized Twitter dataset."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "(run 1 - Scikit-Learn online variational inference)\n",
    "\n",
    "Topic 0:\n",
    "loan fortescue slo_cashn oppose stand watch subsidy free meet talk\n",
    "Topic 1:\n",
    "tax pay public right destroy 10 slo_cash office loss opposition\n",
    "Topic 2:\n",
    "government australian look good know come way join coal ask\n",
    "Topic 3:\n",
    "future cut renewable wrong council policy question write lead set\n",
    "Topic 4:\n",
    "taxpayer price coal ½í² coalmine dollar local mega save massive\n",
    "Topic 5:\n",
    "reef coal green world barrier kill port leave end coral\n",
    "Topic 6:\n",
    "day wo industry huge run story miner mean solar worker\n",
    "Topic 7:\n",
    "½í turnbull power court coal ¼í pm start sell claim\n",
    "Topic 8:\n",
    "want fund time govt farmer bank ahead thank shorten state\n",
    "Topic 9:\n",
    "climate change coal say minister point beach face global political\n",
    "Topic 10:\n",
    "stop people think china disaster listen dam link dirty damage\n",
    "Topic 11:\n",
    "new need coal rail line election rule barnaby national joyce\n",
    "Topic 12:\n",
    "project action vote protest party coal cost high oil asset\n",
    "Topic 13:\n",
    "support energy win help community thing clean lie decision demand\n",
    "Topic 14:\n",
    "coal india tell fight indian try premier live billionaire hear\n",
    "Topic 15:\n",
    "job plan 000 news risk group coal create galilee ceo\n",
    "Topic 16:\n",
    "company slo_mention break build naif sign protect environment use native\n",
    "Topic 17:\n",
    "gas queensland money billion project narrabri lose iron ore seam\n",
    "Topic 18:\n",
    "labor lnp report federal approval alp land giant clear close\n",
    "Topic 19:\n",
    "water year work share environmental business deal make week buy\n",
    "\n",
    "\n",
    "\n",
    "(run 1 - lda library  collapsed Gibbs sampling)\n",
    "\n",
    "Topic 0: fund coal taxpayer loan bank money billion want govt dollar\n",
    "Topic 1: thank time day action beach protest work people stop join\n",
    "Topic 2: reef coal climate barrier stop future change need world destroy\n",
    "Topic 3: gas water coal farmer project narrabri seam forest fine licence\n",
    "Topic 4: job 000 create coal 10 lie thousand know time claim\n",
    "Topic 5: coal company climate government australian board environmental face council ceo\n",
    "Topic 6: court coal land approval native title people owner stop fight\n",
    "Topic 7: rail loan line coal barnaby joyce fund queensland galilee basin\n",
    "Topic 8: coal power india new energy renewable world clean solar need\n",
    "Topic 9: labor green support vote stop shorten want alp lnp coal\n",
    "Topic 10: tax ½í pay ½í² ¼í slo_cash company cut coal island\n",
    "Topic 11: fortescue iron ore share price oil year fall dam profit\n",
    "\n",
    "\n",
    "Time taken to process dataset: 696.9720530509949 seconds, 11.616200884183248 minutes, 0.19360334806972082 hours.\n",
    "\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second execution of LDA with the same hyperparameters."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "(run 2 - Scikit-Learn LDA online variational inference)\n",
    "\n",
    "Topic 0:\n",
    "work court risk world group port native live title coal\n",
    "Topic 1:\n",
    "want good coalmine claim profit finance clear gov voter poll\n",
    "Topic 2:\n",
    "gas time coal know oppose thank industry narrabri think ask\n",
    "Topic 3:\n",
    "project fund reef climate coal change wo barrier kill watch\n",
    "Topic 4:\n",
    "fortescue protest cost local pm forest fine coal financial hand\n",
    "Topic 5:\n",
    "support fight right join wrong campaign demand planet country happen\n",
    "Topic 6:\n",
    "need farmer deal cut rule coal premier run royalty investment\n",
    "Topic 7:\n",
    "water australian future land question open coal pollution morning concern\n",
    "Topic 8:\n",
    "labor coal rail energy slo_cashn line win shorten election stop\n",
    "Topic 9:\n",
    "stop billion public action destroy dollar naif sign national party\n",
    "Topic 10:\n",
    "plan say business big council use end law year production\n",
    "Topic 11:\n",
    "new loan govt coal bank price giant make support long\n",
    "Topic 12:\n",
    "slo_mention tell help leave start environment talk massive away turn\n",
    "Topic 13:\n",
    "job government turnbull lnp look 000 year way lose create\n",
    "Topic 14:\n",
    "people queensland power let coal alp community thing stand close\n",
    "Topic 15:\n",
    "coal ½í build ½í² ¼í galilee ahead basin approve china\n",
    "Topic 16:\n",
    "company state iron indian ore news high lie face carbon\n",
    "Topic 17:\n",
    "tax pay break approval federal ceo subsidy issue slo_cash miner\n",
    "Topic 18:\n",
    "india money report come taxpayer point day barnaby joyce week\n",
    "Topic 19:\n",
    "green vote minister share sell canavan matt promise buy read\n",
    "\n",
    "\n",
    "\n",
    "(run 2 - lda library collapsed Gibbs sampling)\n",
    "\n",
    "Topic 0: tax ½í pay ½í² ¼í company slo_cash ¾í island corporate\n",
    "Topic 1: gas coal water project narrabri seam point farmer plan forest\n",
    "Topic 2: water court land farmer native coal title basin approval owner\n",
    "Topic 3: coal climate future energy change world need new clean reef\n",
    "Topic 4: loan coal rail fund line government queensland naif galilee labor\n",
    "Topic 5: coal time thank late company india council australian minister board\n",
    "Topic 6: fortescue iron ore barnaby joyce dam metal rail wa disaster\n",
    "Topic 7: coal price share year oil asset power market high low\n",
    "Topic 8: day action beach protest work join people stop win come\n",
    "Topic 9: job reef coal 000 barrier create turnbull 10 cut lie\n",
    "Topic 10: coal fund money bank taxpayer project want slo_cashn loan govt\n",
    "Topic 11: labor green stop vote support shorten lnp alp want election\n",
    "\n",
    "\n",
    "Time taken to process dataset: 696.3975586891174 seconds, 11.606625978151957 minutes, 0.19344376630253263 hours.\n",
    "\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are similar to that of the tokenized stance detection dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does it work poorly on Tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "Based on Derek Fisher's senior project presentation:\n",
    "\n",
    "- LDA typically works best when the documents are lengthy (large word count) and written in a formal proper style.\n",
    "\n",
    "- Tweet text is generally very short in length with a max of around 280 characters.\n",
    "\n",
    "- Tweet text is generally written very informally style-wise, e.g.:\n",
    "\n",
    "    - emojis\n",
    "    - spelling errors\n",
    "    - other grammatical errors\n",
    "\n",
    "- The above makes it difficult for the LDA algorithm to discover any prominent underlying hidden structures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation<br>\n",
    "    - Scikit-Learn introduction to LDAs'.<br>\n",
    "\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation<br>\n",
    "    - Scikit-Learn documentation on the LDA class.<br>\n",
    "\n",
    "\n",
    "- https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730<br>\n",
    "    - Article with example of topic modeling using Scikit-Learn LDA and NMF.<br>\n",
    "\n",
    "\n",
    "- https://pypi.org/project/lda/<br>\n",
    "    - Links to the \"lda\" Python package website.<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
