{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation Topic Model Implementation on SLO Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "</p>Our implementation utilizes the Scikit-Learn LatentDirichletAllocation class and the Python \"lda\" library.  We utilize Scikit-Learn's GridSearchCV class to perform an exhaustive grid search for the optimal hyperparameters to fit our Twitter dataset.  We preprocess our raw Twitter dataset before running multiple iterations of the LDA algorithm with the following specified number of topics: 3, 6, 12, and 20.  We limit each topic to the top 10 words that describe that topic.</p><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Adjust log verbosity levels as necessary.<br>\n",
    "\n",
    "Set to \"DEBUG\" to view all debug output.<br>\n",
    "Set to \"INFO\" to view useful information on dataframe shape, etc.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import logging as log\n",
    "import warnings\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import custom utility functions.\n",
    "import topic_extraction_utility_functions as lda_util\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Pandas options.\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "pd.options.display.max_colwidth = 1000\n",
    "# Pandas float precision display.\n",
    "pd.set_option('precision', 12)\n",
    "# Don't output these types of warnings to terminal.\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "# Matplotlib log settings.\n",
    "mylog = log.getLogger(\"matplotlib\")\n",
    "mylog.setLevel(log.INFO)\n",
    "\n",
    "\"\"\"\n",
    "Turn debug log statements for various sections of code on/off.\n",
    "(adjust log level as necessary)\n",
    "\"\"\"\n",
    "log.basicConfig(level=log.INFO)\n",
    "log.disable(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process and Post-process Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "We preprocess our Twitter dataset as follows:<br>\n",
    "\n",
    "1) Downcase all text.<br>\n",
    "2) Remove \"RT\" tags.<br>\n",
    "3) Remove URL's and replace with slo_url.<br>\n",
    "4) Remove Tweet mentions and replace with slo_mention.<br>\n",
    "5) Remove Tweet hashtags and replace with slo_hashtag.<br>\n",
    "6) Remove all punctuation in the Tweet.<br>\n",
    "7) Remove all words we find to be irrelevant for topic extraction from the Tweet.<br>\n",
    "8) Save the preprocessed Tweets to a external CSV file for use in LDA topic extraction.<br>\n",
    "\n",
    "**TODO - update to reflect current function**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test on our topic modeling dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-test-subset-100-examples.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-lda-ready-tweet-text-with-hashtags-excluded-created-7-17-19.csv\",\n",
    "    \"text_derived\")\n",
    "\n",
    "# Test on our topic modeling dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-test-subset-100-examples.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-lda-ready-user-description-text-with-hashtags-excluded-created-7-17-19.csv\",\n",
    "    \"user_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The first parameter in our function call specifies the file path to the dataset to be preprocessed.  The second parameter specifies the location to save the CSV file to.  The 3rd parameter specifies the name of the column in the dataset that contains the original Tweet text.<br>\n",
    "\n",
    "\n",
    "Tweet preprocessing is done via a custom library imported as \"lda_util\" using \"slo_lda_topic_extraction_utility_functions.py\".<br>\n",
    "\n",
    "Refer to URL link for the codebase to the utility functions used above for data preprocessing and below for LDA topic extraction:<br>\n",
    "\n",
    "https://github.com/J-Jinn/Summer-Research-2019/blob/master/slo_lda_topic_extraction_utility_functions.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and prepare the preprocessed dataset for use in LDA topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "Refer to the code comments for the specific steps performed.<br>\n",
    "Note that we have to use absolute file paths in Jupyter notebook as opposed to relative file paths in PyCharm.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "\n",
      "INFO:root:The shape of our preprocessed SLO dataframe with NaN (empty) rows dropped:\n",
      "INFO:root:(653094, 1)\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:The columns of our preprocessed SLO dataframe with NaN (empty) rows dropped:\n",
      "INFO:root:<bound method NDFrame.head of                                                   tweet_t\n",
      "134130  every australian politician should be watching...\n",
      "109525  exciting to see the results of continued commu...\n",
      "175975  breaking will proceed on but it doesnt have th...\n",
      "221807                 already wtf cant do anything right\n",
      "476425  breaking 4 people occupy a coal train at willo...\n",
      "...                                                   ...\n",
      "434193  others have left holes why we never really got...\n",
      "231520  so confirms they are exploring selling and or ...\n",
      "14479   power refusing to c oo perate with credit rati...\n",
      "38365   suppos mine if it stack up environmentally doe...\n",
      "656954  queensland tourism award winner rejects adanis...\n",
      "\n",
      "[653094 rows x 1 columns]>\n",
      "INFO:root:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Import the dataset (relative path).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"twitter-dataset-7-10-19-lda-ready-tweet-text-with-hashtags-excluded-created-7-17-19.csv\", sep=\",\")\n",
    "\n",
    "# Import the dataset (absolute path).\n",
    "tweet_dataset_processed = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "                \"twitter-dataset-7-10-19-lda-ready-tweet-text-with-hashtags-excluded-created-7-17-19.csv\", sep=\",\")\n",
    "\n",
    "# # Import the dataset (test/debug).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"twitter-dataset-7-10-19-lda-ready-tweet-text-test.csv\", sep=\",\")\n",
    "\n",
    "# # Import the dataset (test/debug).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "#                 \"twitter-dataset-7-10-19-lda-ready-tweet-text-test.csv\", sep=\",\")\n",
    "\n",
    "# Reindex and shuffle the data randomly.\n",
    "tweet_dataset_processed = tweet_dataset_processed.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed.index))\n",
    "\n",
    "# Generate a Pandas dataframe.\n",
    "tweet_text_dataframe = pd.DataFrame(tweet_dataset_processed)\n",
    "\n",
    "# # Print shape and column names.\n",
    "# log.info(f\"\\nThe shape of the Tweet text dataframe:\")\n",
    "# log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "# log.info(f\"\\nThe columns of the Tweet text dataframe:\")\n",
    "# log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(\"\\nThe shape of the Tweet text dataframe:\")\n",
    "log.info(tweet_text_dataframe.shape)\n",
    "log.info(\"\\nThe columns of the Tweet text dataframe:\")\n",
    "log.info(tweet_text_dataframe.columns)\n",
    "\n",
    "# Drop any NaN or empty Tweet rows in dataframe (or else CountVectorizer will blow up).\n",
    "tweet_text_dataframe = tweet_text_dataframe.dropna()\n",
    "\n",
    "# # Print shape and column names.\n",
    "# log.info(f\"\\nThe shape of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "# log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "# log.info(f\"\\nThe columns of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "# log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(\"\\nThe shape of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(tweet_text_dataframe.shape)\n",
    "log.info(\"\\nThe columns of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(tweet_text_dataframe.columns)\n",
    "\n",
    "# Reindex everything.\n",
    "tweet_text_dataframe.index = pd.RangeIndex(len(tweet_text_dataframe.index))\n",
    "\n",
    "# Assign column names.\n",
    "tweet_text_dataframe_column_names = ['text_derived', 'text_derived_preprocessed', 'text_derived_postprocessed']\n",
    "\n",
    "# Rename column in dataframe.\n",
    "tweet_text_dataframe.columns = tweet_text_dataframe_column_names\n",
    "\n",
    "# Create input feature.\n",
    "selected_features = tweet_text_dataframe[['text_derived_postprocessed']]\n",
    "processed_features = selected_features.copy()\n",
    "\n",
    "# # Check what we are using as inputs.\n",
    "# log.info(f\"\\nA sample Tweet in our input feature:\")\n",
    "# log.info(f\"{processed_features['text_derived_postprocessed'][0]}\\n\")\n",
    "\n",
    "# Check what we are using as inputs.\n",
    "log.info(\"\\nA sample Tweet in our input feature:\")\n",
    "log.info(processed_features['text_derived_postprocessed'][0])\n",
    "\n",
    "# Create feature set.\n",
    "slo_feature_series = processed_features['text_derived_postprocessed']\n",
    "slo_feature_series = pd.Series(slo_feature_series)\n",
    "slo_feature_list = slo_feature_series.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The above log.INFO messages depict the shape and contents of the preprocessed dataframe after dropping any rows that are just \"NaN\", indicating the Tweet was full of irrelevant words and is now empty due to the removal of those irrelevant words.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We use the Scikit-Learn CountVectorizer class to vectorize our categorical Tweet data.  We set the max_features parameter to 1000 to indicate a maximum vocabulary of 1k words based on the 1000 words with the highest term frequencies.  We set the stop_words parameter to \"English\" to indicate we would like to remove English stop words based on a built-in library of stop words.  We set the min_df and max_df parameters to indicate the words with the threshold term frequencies at which we ignore those words and do not include them in our vocabulary.<br>\n",
    "\n",
    "We use the Scikit-Learn LatentDirichletAllocation class with the below hyperparameters to train and fit our Tweet data.  The parameter n_topics controls the # of topics we would like to extract for topic modeling.  The parameter max_iter controls the # of iterations to perform LDA before we cease.  The parameter learning_method controls the method by which we update the words in our topics.  <br>\n",
    "\n",
    "We use a utility function to display Topics 1-20 and the top 10 words associated with each Topic.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:The time taken to perform the operation is: \n",
      "INFO:root:439.08266735076904\n",
      "INFO:root:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "money slocashn coal work minister ceo taxpayers banks join use\n",
      "Topic 1:\n",
      "cou public lnp community away shares townsville end workers best\n",
      "Topic 2:\n",
      "people time need group thanks come thing cut latest times\n",
      "Topic 3:\n",
      "water reef land farmers barrier help state free adanis plan\n",
      "Topic 4:\n",
      "going narrabri barnaby does fight joyce business pm massive australias\n",
      "Topic 5:\n",
      "stop new coal government wont breaking approval plans premier carbon\n",
      "Topic 6:\n",
      "climate change greens adani vote biggest council companies week coal\n",
      "Topic 7:\n",
      "oil tell political coal hey bad clear taxpayer noh message\n",
      "Topic 8:\n",
      "queensland funding world risk coal groundwater looks access policy paying\n",
      "Topic 9:\n",
      "govt coal india shoen coalmine deal wants years local build\n",
      "Topic 10:\n",
      "adanis project coal turnbull point basin right did protect pollution\n",
      "Topic 11:\n",
      "labor alp way repo coal canavan look really lnp corruption\n",
      "Topic 12:\n",
      "jobs say news create board planet high finance abc thousands\n",
      "Topic 13:\n",
      "rail environmental environment disaster licence gov sign infrastructure don health\n",
      "Topic 14:\n",
      "coal good company power future ahead day mines bank global\n",
      "Topic 15:\n",
      "loan foescue federal make fund line election indian know naif\n",
      "Topic 16:\n",
      "coal gas suppo dont project said po seam thats country\n",
      "Topic 17:\n",
      "iron ore doesnt industry needs coal giant national politicians dam\n",
      "Topic 18:\n",
      "pay energy galilee think beach clean native decision title voters\n",
      "Topic 19:\n",
      "tax want australian action year billion paid giving green stand\n"
     ]
    }
   ],
   "source": [
    "def latent_dirichlet_allocation_topic_extraction():\n",
    "    \"\"\"\n",
    "    Function performs topic extraction on Tweets using Scikit-Learn LDA model.\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model.\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(slo_feature_series)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    # Run LDA.\n",
    "    lda = LatentDirichletAllocation(n_components=20, max_iter=5, learning_method='online', learning_offset=50.,\n",
    "                                    random_state=0).fit(tf)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Display the top words for each topic.\n",
    "    lda_util.display_topics(lda, tf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Perform the topic extraction.\n",
    "    \"\"\"\n",
    "    latent_dirichlet_allocation_topic_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We cannot seem to find any strong correlation between the 10 words in each Topic such that we could assign an English descriptor to each topic, such as \"economic\", \"environmental\", \"social\", etc.\n",
    "\n",
    "Of interesting note is that it appears to take longer to perform LDA topic extraction specifying fewer topics over more topics.  We surmise this is because we have a large dataset of 650k+ Tweets which translates to 650k+ different documents in our corpus.  Therefore, it would take the algorithm less time if it could simply assign 650k+ documents to 650k+ different topics rather than having to assign 650k+ documents to a mere 3 topics or in general a much smaller number of topics in comparison to the number of documents.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Topic Extraction using the \"lda\" library and collapsed Gibbs Sampling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The code below uses the \"lda\" Python library package that performs LDA topic extraction using collapsed Gibbs Sampling.<br>\n",
    "This is different from the Scikit-Learn implementation that uses online variational inference.<br>\n",
    "Otherwise, the dataset is the same and we are still using Scikit-Learn's CountVectorizer class to vectorize our data.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 653094\n",
      "INFO:lda:vocab_size: 1000\n",
      "INFO:lda:n_words: 3267212\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -33566606\n",
      "INFO:lda:<10> log likelihood: -27631270\n",
      "INFO:lda:<20> log likelihood: -24168941\n",
      "INFO:lda:<30> log likelihood: -23191677\n",
      "INFO:lda:<40> log likelihood: -22881500\n",
      "INFO:lda:<50> log likelihood: -22754208\n",
      "INFO:lda:<60> log likelihood: -22681384\n",
      "INFO:lda:<70> log likelihood: -22639373\n",
      "INFO:lda:<80> log likelihood: -22612153\n",
      "INFO:lda:<90> log likelihood: -22593660\n",
      "INFO:lda:<99> log likelihood: -22575655\n",
      "INFO:root:The time taken to perform the operation is: \n",
      "INFO:root:90.21567153930664\n",
      "INFO:root:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: coal energy future clean fossil climate carbon time global need\n",
      "Topic 1: water free billion owners coal traditional unlimited farmers giving dollars\n",
      "Topic 2: cou coal native title stop adanis approval federal land turnbull\n",
      "Topic 3: labor greens stop lnp alp vote shoen election suppo want\n",
      "Topic 4: coal thanks latest times australian adanis green bank repo govt\n",
      "Topic 5: coal fund money banks project funding govt adanis wont taxpayers\n",
      "Topic 6: people action stop protest join day protesters campaign time message\n",
      "Topic 7: gas project coal narrabri seam forest water farmers barnaby pipeline\n",
      "Topic 8: beach dam watch day tour video brazil story iluka disaster\n",
      "Topic 9: water basin aesian environmental risk coal right world health suppo\n",
      "Topic 10: foescue shares group metals debt profit year loss fmg news\n",
      "Topic 11: loan canavan minister taxpayer slocashn matt board joyce barnaby money\n",
      "Topic 12: loan rail line adanis coal galilee naif veto basin government\n",
      "Topic 13: coal new power india point mines solar adanis po stranded\n",
      "Topic 14: tax pay paid australian corporate company energy donations companies chevron\n",
      "Topic 15: coal good company corruption deal away govt record news track\n",
      "Topic 16: iron ore oil ceo new production boss price search tintos\n",
      "Topic 17: reef climate coal barrier change save stop protect coral kill\n",
      "Topic 18: want dont doesnt need coal suppo make money townsville adanis\n",
      "Topic 19: jobs create thousands coal 10000 tourism job thats pm turnbull\n"
     ]
    }
   ],
   "source": [
    "def latent_dirichlet_allocation_collapsed_gibbs_sampling():\n",
    "    \"\"\"\n",
    "    Functions performs LDA topic extraction using collapsed Gibbs Sampling.\n",
    "\n",
    "    https://pypi.org/project/lda/\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    import lda\n",
    "\n",
    "    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model.\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(slo_feature_series)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    # Train and fit the LDA model.\n",
    "    model = lda.LDA(n_topics=12, n_iter=1000, random_state=1)\n",
    "    model.fit(tf)  # model.fit_transform(X) is also available\n",
    "    topic_word = model.topic_word_  # model.components_ also works\n",
    "    n_top_words = 10\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Display the topics and the top words associated with.\n",
    "    for i, topic_dist in enumerate(topic_word):\n",
    "        topic_words = np.array(tf_feature_names)[np.argsort(topic_dist)][:-(n_top_words + 1):-1]\n",
    "        print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Perform the topic extraction using collapsed Gibbs Sampling.\n",
    "    \"\"\"\n",
    "    latent_dirichlet_allocation_collapsed_gibbs_sampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The results seem to be as incoherent as the Scikit-Learn implementation of LDA topic extraction using online variational inference.<br>\n",
    "\n",
    "It's difficult to see any correlation between the 10 top words for each topic.<br>\n",
    "\n",
    "Here, we are using n_iter=100 (iterations) as the fitting to our Twitter data is a lot faster than the Scikit-Learn implementation where max_iter=5 already takes 450 seconds.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does it work poorly on Tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "Based on Derek Fisher's senior project presentation:\n",
    "\n",
    "1) LDA typically works best when the documents are lengthy (large word count) and written in a formal proper style.\n",
    "\n",
    "2) Tweet text is generally very short in length with a max of around 280 characters.\n",
    "\n",
    "3) Tweet text is generally written very informally style-wise.\n",
    "\n",
    "    i) emojis.\n",
    "    ii) spelling errors.\n",
    "    iii) other grammatical errors.\n",
    "    iv) etc.\n",
    "\n",
    "4) The above makes it difficult for the LDA algorithm to discover any prominent underlying hidden structures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation<br>\n",
    "    - Scikit-Learn introduction to LDAs'.<br>\n",
    "\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation<br>\n",
    "    - Scikit-Learn documentation on the LDA class.<br>\n",
    "\n",
    "\n",
    "- https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730<br>\n",
    "    - Article with example of topic modeling using Scikit-Learn LDA and NMF.<br>\n",
    "\n",
    "\n",
    "- https://pypi.org/project/lda/<br>\n",
    "    - Links to the \"lda\" Python package website.<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
