{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Author-Topic Model Implementation on SLO Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the requisite libraries, custom utility functions, and set the parameters for our various imported libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import logging as log\n",
    "import warnings\n",
    "import time\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import custom utility functions.\n",
    "import topic_extraction_utility_functions as topic_util\n",
    "import slo_twitter_data_analysis_utility_functions as tweet_util_v2\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Pandas options.\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "pd.options.display.max_colwidth = 1000\n",
    "# Pandas float precision display.\n",
    "pd.set_option('precision', 12)\n",
    "# Seaborn setting.\n",
    "sns.set()\n",
    "# Don't output these types of warnings to terminal.\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "# Matplotlib log settings.\n",
    "mylog = log.getLogger(\"matplotlib\")\n",
    "mylog.setLevel(log.INFO)\n",
    "\n",
    "\"\"\"\n",
    "Turn debug log statements for various sections of code on/off.\n",
    "(adjust log level as necessary)\n",
    "\"\"\"\n",
    "log.basicConfig(level=log.INFO)\n",
    "log.disable(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process and Post-process Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize our Tweets using our dataset preprocessing and postprocessing function.  Please refer to \"topic_extraction_utility_functions.py\" for the full code-base and comments on what is done to the Tweet text and user description text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on our topic modeling dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-test-subset-100-examples.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-lda-ready-tweet-text-with-hashtags-excluded-created-7-17-19.csv\",\n",
    "    \"text_derived\")\n",
    "\n",
    "# Test on our topic modeling dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-test-subset-100-examples.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-lda-ready-user-description-text-with-hashtags-excluded-created-7-17-19.csv\",\n",
    "    \"user_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and prepare the preprocessed dataset for use in Author-Topic topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the general format of insertion into a Pandas dataframe, isolating the column of interest, and generating a dictionary of words and corpus of documents.  For the author-topic model, we also generate a mapping of unique author screen names to the row index values of the Tweets they are associated with.  Please refer to the code comments for details on the specific steps for the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import untokenized CSV dataset.\n",
    "# tweet_dataset_untokenized = tweet_util_v2.import_dataset(\n",
    "#     \"twitter-dataset-7-10-19-with-irrelevant-tweets-excluded.csv\",\n",
    "#     \"csv\", False)\n",
    "\n",
    "# Import untokenized CSV dataset.\n",
    "tweet_dataset_untokenized = tweet_util_v2.import_dataset(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-with-irrelevant-tweets-excluded.csv\",\n",
    "    \"csv\", False)\n",
    "\n",
    "# # Import untokenized CSV dataset. (test/debug)\n",
    "# tweet_dataset_untokenized = tweet_util_v2.import_dataset(\"twitter-dataset-7-10-19-test-subset-100-examples.csv\",\n",
    "#                                                          \"csv\", False)\n",
    "\n",
    "# # Import untokenized CSV dataset. (test/debug)\n",
    "# tweet_dataset_untokenized = tweet_util_v2.import_dataset(\n",
    "#     \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "#     \"twitter-dataset-7-10-19-test-subset-100-examples.csv\",\n",
    "#     \"csv\", False)\n",
    "\n",
    "# Create author-document mappings as a dictionary of key: author, values: tweet ID's\n",
    "author2doc = topic_util.topic_author_model_group_by_dataset_row_index_value(tweet_dataset_untokenized, True)\n",
    "\n",
    "# # Import tokenized CSV dataset.\n",
    "# tweet_dataset_tokenized = tweet_util_v2.import_dataset(\n",
    "#     \"twitter-dataset-7-10-19-lda-ready-tweet-text-with-hashtags-excluded-created-7-17-19.csv\",\n",
    "#     \"csv\", False)\n",
    "\n",
    "# Import tokenized CSV dataset.\n",
    "tweet_dataset_tokenized = tweet_util_v2.import_dataset(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-lda-ready-tweet-text-with-hashtags-excluded-created-7-17-19.csv\",\n",
    "    \"csv\", False)\n",
    "\n",
    "# # Import tokenized CSV dataset. (test/debug)\n",
    "# tweet_dataset_tokenized = tweet_util_v2.import_dataset(\"twitter-dataset-7-10-19-lda-ready-tweet-text-test.csv\",\n",
    "#                                                        \"csv\", False)\n",
    "\n",
    "# # Import tokenized CSV dataset. (test/debug)\n",
    "# tweet_dataset_tokenized = tweet_util_v2.import_dataset(\n",
    "#     \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "#     \"twitter-dataset-7-10-19-lda-ready-tweet-text-test.csv\",\n",
    "#     \"csv\", False)\n",
    "\n",
    "# # Reindex and shuffle the data randomly.\n",
    "# tweet_dataset_tokenized = tweet_dataset_tokenized.reindex(\n",
    "#     pd.np.random.permutation(tweet_dataset_tokenized.index))\n",
    "\n",
    "# Generate a Pandas dataframe.\n",
    "tweet_text_dataframe = pd.DataFrame(tweet_dataset_tokenized)\n",
    "\n",
    "# # Print shape and column names.\n",
    "# log.info(f\"\\nThe shape of the Tweet text dataframe:\")\n",
    "# log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "# log.info(f\"\\nThe columns of the Tweet text dataframe:\")\n",
    "# log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(\"\\nThe shape of the Tweet text dataframe:\")\n",
    "log.info(tweet_text_dataframe.shape)\n",
    "log.info(\"\\nThe columns of the Tweet text dataframe:\")\n",
    "log.info(tweet_text_dataframe.columns)\n",
    "\n",
    "# # Drop any NaN or empty Tweet rows in dataframe (or else CountVectorizer will blow up).\n",
    "# tweet_text_dataframe = tweet_text_dataframe.dropna()\n",
    "\n",
    "# # Print shape and column names.\n",
    "# log.info(f\"\\nThe shape of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "# log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "# log.info(f\"\\nThe columns of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "# log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# # Print shape and column names.\n",
    "# log.info(\"\\nThe shape of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "# log.info(tweet_text_dataframe.shape)\n",
    "# log.info(\"\\nThe columns of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "# log.info(tweet_text_dataframe.columns)\n",
    "#\n",
    "# # Reindex everything.\n",
    "# tweet_text_dataframe.index = pd.RangeIndex(len(tweet_text_dataframe.index))\n",
    "\n",
    "# Assign column names.\n",
    "tweet_text_dataframe_column_names = ['text_derived', 'text_derived_preprocessed', 'text_derived_postprocessed']\n",
    "\n",
    "# Rename column in dataframe.\n",
    "tweet_text_dataframe.columns = tweet_text_dataframe_column_names\n",
    "\n",
    "# Create input feature.\n",
    "selected_features = tweet_text_dataframe[['text_derived_postprocessed']]\n",
    "processed_features = selected_features.copy()\n",
    "\n",
    "# # Check what we are using as inputs.\n",
    "# log.info(f\"\\nA sample Tweet in our input feature:\")\n",
    "# log.info(f\"{processed_features['text_derived_postprocessed'][0]}\\n\")\n",
    "\n",
    "# Check what we are using as inputs.\n",
    "log.info(\"\\nA sample Tweet in our input feature:\")\n",
    "log.info(processed_features['text_derived_postprocessed'][0])\n",
    "\n",
    "# Create feature set.\n",
    "slo_feature_series = processed_features['text_derived_postprocessed']\n",
    "slo_feature_series = pd.Series(slo_feature_series)\n",
    "slo_feature_list = slo_feature_series.tolist()\n",
    "\n",
    "words = []\n",
    "# Create the corpus of documents and dictionary of words (vocabulary)\n",
    "for tweet in slo_feature_list:\n",
    "    tweet_string = str(tweet)\n",
    "    words.append(tweet_string.split())\n",
    "\n",
    "# Convert feature list of sentences to comma-separated dictionary of words for each document in the corpus.\n",
    "# Note: Deprecated - issue with \"float\" values instead of string.\n",
    "# words = [[text for text in tweet.split()] for tweet in slo_feature_list]\n",
    "# log.info(f\"\\nDictionary of individual words:\")\n",
    "# log.info(f\"{words[0]}\\n\")\n",
    "log.info(\"\\ndictionary of individual words for each document in the corpus:\")\n",
    "log.info(words[0])\n",
    "# for word in words:\n",
    "#     log.info(word)\n",
    "\n",
    "# # Create the Gensim dictionary of words.\n",
    "# dictionary = corpora.Dictionary(words)\n",
    "# log.info(f\"\\nGensim dictionary of tokenized words.\")\n",
    "# log.info(f\"{dictionary}\\n\")\n",
    "# log.info(f\"\\nGensim dictionary of tokenized words with index ID's.\")\n",
    "# log.info(f\"{dictionary.token2id}\\n\")\n",
    "\n",
    "# Create the Gensim dictionary of words.\n",
    "dictionary = corpora.Dictionary(words)\n",
    "log.info(\"\\nGensim dictionary of tokenized words.\")\n",
    "log.info(dictionary)\n",
    "log.info(\"\\n\")\n",
    "log.info(\"\\nGensim dictionary of tokenized words with index ID's.\")\n",
    "log.info(dictionary.token2id)\n",
    "log.info(\"\\n\")\n",
    "\n",
    "# # Create the Gensim corpus of document term frequencies.\n",
    "# corpus = [dictionary.doc2bow(word, allow_update=True) for word in words]\n",
    "# log.info(f\"# of documents in corpus: {len(corpus)}\\n\")\n",
    "# log.info(f\"\\nSample of Gensim corpus of document-term frequencies.\")\n",
    "# log.info(f\"{corpus[0:10]}\\n\")\n",
    "\n",
    "# Create the Gensim corpus of document term frequencies.\n",
    "corpus = [dictionary.doc2bow(word, allow_update=True) for word in words]\n",
    "log.info(\"# of documents in corpus: \" + str(len(corpus)) + \"\\n\")\n",
    "log.info(\"\\nSample of Gensim corpus of document-term frequencies.\")\n",
    "log.info(corpus[0:10])\n",
    "log.info(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function contains the code specific to each topic modeling library we utilize.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_topic_model_topic_extraction():\n",
    "    \"\"\"\n",
    "    Function performs topic extraction on Tweets using the Gensim Author-Topic model.\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    from gensim.models import AuthorTopicModel\n",
    "\n",
    "    model = AuthorTopicModel(corpus=corpus, num_topics=10, id2word=dictionary,\n",
    "                             author2doc=author2doc, chunksize=2000, passes=1, eval_every=0,\n",
    "                             iterations=1, random_state=1)\n",
    "    time.sleep(3)\n",
    "    topic_labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "    for topic in model.show_topics(num_topics=10):\n",
    "        # print(f\"'Label: ' + {topic_labels[topic[0]]}\")\n",
    "        print('Label: ' + str(topic_labels[topic[0]]))\n",
    "        wordz = ''\n",
    "        for word, prob in model.show_topic(topic[0]):\n",
    "            wordz += word + ' '\n",
    "        # print(f\"'Words: ' + {wordz}\")\n",
    "        print('Words: ' + wordz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we call the topic modeling function and train it on our Twitter dataset.  We record the time it takes to process the entire dataset and extract topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main function.  Execute the program.\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    my_start_time = time.time()\n",
    "    ################################################\n",
    "    \"\"\"\n",
    "    Perform the topic extraction.\n",
    "    \"\"\"\n",
    "    author_topic_model_topic_extraction()\n",
    "\n",
    "    ################################################\n",
    "    my_end_time = time.time()\n",
    "\n",
    "    time_elapsed_in_seconds = (my_end_time - my_start_time)\n",
    "    time_elapsed_in_minutes = (my_end_time - my_start_time) / 60.0\n",
    "    time_elapsed_in_hours = (my_end_time - my_start_time) / 60.0 / 60.0\n",
    "    # print(f\"Time taken to process dataset: {time_elapsed_in_seconds} seconds, \"\n",
    "    #       f\"{time_elapsed_in_minutes} minutes, {time_elapsed_in_hours} hours.\")\n",
    "    print(\"\\n\\nTime taken to process dataset: \" + str(time_elapsed_in_seconds) + \" seconds, \" +\n",
    "          str(time_elapsed_in_minutes) + \" minutes, \" + str(time_elapsed_in_hours) + \" hours.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Extraction Results on Twitter Dataset Tweet Text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First execution run.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "(run 1)\n",
    "Label: 1\n",
    "Words: project written woodside santos eis north downer group coal .slo_mention \n",
    "Label: 2\n",
    "Words: tax joyce barnaby inland woodside slo_cashil pay property chevron go \n",
    "Label: 3\n",
    "Words: gas coal seam water rd stop farmer people time pipeline \n",
    "Label: 4\n",
    "Words: woodside coal new 's energy | coleman wa project cfs \n",
    "Label: 5\n",
    "Words: santos nsw gas great australia water narrabri pilliga basin pipeline \n",
    "Label: 6\n",
    "Words: beach road win 1 tour day > video morning park \n",
    "Label: 7\n",
    "Words: santos route have party breed be right well v like \n",
    "Label: 8\n",
    "Words: woodside whitehaven lng coal tycoon rio oil price gas sale \n",
    "Label: 9\n",
    "Words: í whitehaven $ ° ½í² ¼í¶ sto james whc ltd \n",
    "Label: 10\n",
    "Words: santos forest coal gas narrabri water petroleum csg creek field \n",
    "\n",
    "\n",
    "Time taken to process dataset: 9772.976831436157 seconds, 162.8829471906026 minutes, 2.7147157865100433 hours.\n",
    "\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Second execution run."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "(run 2)\n",
    "Label: 1\n",
    "Words: project written woodside santos eis north downer group coal .slo_mention \n",
    "Label: 2\n",
    "Words: tax joyce barnaby inland woodside slo_cashil pay property chevron go \n",
    "Label: 3\n",
    "Words: gas coal seam water rd stop farmer people time pipeline \n",
    "Label: 4\n",
    "Words: woodside coal new 's energy | coleman wa project cfs \n",
    "Label: 5\n",
    "Words: santos nsw gas great australia water narrabri pilliga basin pipeline \n",
    "Label: 6\n",
    "Words: beach road win 1 tour day > video morning park \n",
    "Label: 7\n",
    "Words: santos route have party breed be right well v like \n",
    "Label: 8\n",
    "Words: woodside whitehaven lng coal tycoon rio oil price gas sale \n",
    "Label: 9\n",
    "Words: í whitehaven $ ° ½í² ¼í¶ sto james whc ltd \n",
    "Label: 10\n",
    "Words: santos forest coal gas narrabri water petroleum csg creek field \n",
    "\n",
    "\n",
    "Time taken to process dataset: 9236.188854932785 seconds, 153.9364809155464 minutes, 2.565608015259107 hours.\n",
    "\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are non-ASCII characters present, which suggests we may need to modify our tokenization regular expressions and other preprocessing and postprocessing functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://radimrehurek.com/gensim/models/atmodel.html\n",
    "    - Gensim author-topic modeling Class.\n",
    "    \n",
    "    \n",
    "- https://stackoverflow.com/questions/47434426/pandas-groupby-unique-multiple-columns\n",
    "    - using Pandas .groupby() function. \n",
    "\n",
    "\n",
    "- https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/\n",
    "    - using Pandas .groupby() function blog.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
