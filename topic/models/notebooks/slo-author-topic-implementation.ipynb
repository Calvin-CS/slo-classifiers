{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Author-Topic Model Implementation on SLO Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize the Gensim Author-Topic Model.  This model requires mapping authors to document ID's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the requisite libraries, custom utility functions, and set the parameters for our various imported libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import logging as log\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from gensim import corpora\n",
    "\n",
    "# Import custom utility functions.\n",
    "import slo_twitter_data_analysis_utility_functions as tweet_util_v2\n",
    "import topic_extraction_utility_functions as topic_util\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Pandas options.\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "pd.options.display.max_colwidth = 1000\n",
    "# Pandas float precision display.\n",
    "pd.set_option('precision', 12)\n",
    "# Seaborn setting.\n",
    "sns.set()\n",
    "# Don't output these types of warnings to terminal.\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "# Matplotlib log settings.\n",
    "mylog = log.getLogger(\"matplotlib\")\n",
    "mylog.setLevel(log.INFO)\n",
    "\n",
    "\"\"\"\n",
    "Turn debug log statements for various sections of code on/off.\n",
    "(adjust log level as necessary)\n",
    "\"\"\"\n",
    "log.basicConfig(level=log.INFO)\n",
    "log.disable(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process and Post-process Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "We preprocess our Twitter dataset as follows:<br>\n",
    "\n",
    "1) Downcase all text.<br>\n",
    "2) Check that there is text, otherwise convert to empty string.<br>\n",
    "3) Convert html chars. to unicode chars.<br>\n",
    "4) Remove \"RT\" tags.<br>\n",
    "5) Remove concatenated URL's.<br>\n",
    "6) Handle whitespaces by converting all/multiple whitespace characters to a single whitespace.<br>\n",
    "7) Remove URL's and replace with \"slo_url\".<br>\n",
    "8) Remove Tweet mentions and replace with \"slo_mention\".<br>\n",
    "9) Remove Tweet stock symbols and replace with \"slo_stock\".<br>\n",
    "10) Remove Tweet hashtags and replace with \"slo_hash\".<br>\n",
    "11) Remove Tweet cashtags and replace with \"slo_cash\".<br>\n",
    "12) Remove Tweet year and replace with \"slo_year\".<br>\n",
    "13) Remove Tweet time and replace with \"slo_time\".<br>\n",
    "14) Remove character elongations.<br>\n",
    "\n",
    "We postprocess our Twitter dataset as follows:<br>\n",
    "\n",
    "1) Remove the following irrelevant words specified in the List below:<br>\n",
    "\n",
    "    delete_list = [\"word_n\", \"auspol\", \"ausbiz\", \"tinto\", \"adelaide\", \"csg\", \"nswpol\",\n",
    "                   \"nsw\", \"lng\", \"don\", \"rio\", \"pilliga\", \"australia\", \"asx\", \"just\", \"today\", \"great\", \"says\", \"like\",\n",
    "                   \"big\", \"better\", \"rite\", \"would\", \"SCREEN_NAME\", \"mining\", \"former\", \"qldpod\", \"qldpol\", \"qld\", \"wr\",\n",
    "                   \"melbourne\", \"andrew\", \"fuck\", \"spadani\", \"greg\", \"th\", \"australians\", \"http\", \"https\", \"rt\",\n",
    "                   \"co\", \"amp\", \"carmichael\", \"abbot\", \"bill shorten\",\n",
    "                   \"slo_url\", \"slo_mention\", \"slo_hash\", \"slo_year\", \"slo_time\", \"slo_cash\", \"slo_stock\",\n",
    "                   \"adani\", \"bhp\", \"cuesta\", \"fotescue\", \"riotinto\", \"newmontmining\", \"santos\", \"oilsearch\",\n",
    "                   \"woodside\", \"ilukaresources\", \"whitehavencoal\",\n",
    "                   \"stopadani\", \"goadani\", \"bhpbilliton\", \"billiton\", \"cuestacoal\", \"cuests coal\", \"cqc\",\n",
    "                   \"fortescuenews\", \"fortescue metals\", \"rio tinto\", \"newmont\", \"newmont mining\", \"santosltd\",\n",
    "                   \"oilsearchltd\", \"oil search\", \"woodsideenergy\", \"woodside petroleum\", \"woodside energy\",\n",
    "                   \"iluka\", \"iluka resources\", \"whitehaven\", \"whitehaven coal\"]\n",
    "\n",
    "2) Remove all punctuation from the Tweet text.<br>\n",
    "3) Remove all English stop words from the Tweet text.<br>\n",
    "4) Lemmatize the words in the Tweet text.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize using our Twitter dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-with-irrelevant-tweets-excluded.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-topic-extraction-ready-tweet-text-with-hashtags-excluded-created-7-29-19.csv\",\n",
    "    \"text_derived\")\n",
    "\n",
    "# Tokenize using our Twitter dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-with-irrelevant-tweets-excluded.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-topic-extraction-ready-user-description-text-with-hashtags-excluded-created-7-29-19.csv\",\n",
    "    \"user_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The first parameter in our function call specifies the file path to the dataset to be preprocessed.  The second parameter specifies the location to save the CSV file to.  The 3rd parameter specifies the name of the column in the dataset that contains the original Tweet text.<br>\n",
    "\n",
    "\n",
    "Tweet preprocessing is done via a custom library imported as \"lda_util\" using \"topic_extraction_utility_functions.py\".<br>\n",
    "\n",
    "Refer to URL link for the codebase to the utility functions used above for data preprocessing and below for LDA topic extraction:<br>\n",
    "\n",
    "https://github.com/Calvin-CS/slo-classifiers/blob/master/topic/models/topic_extraction_utility_functions.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and prepare the preprocessed dataset for use in Author-Topic topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the general format of insertion into a Pandas dataframe, isolating the column of interest, and generating a dictionary of words and corpus of documents.  For the author-topic model, we also generate a mapping of unique author screen names to the row index values of the Tweets they are associated with.  Please refer to the code comments for details on the specific steps for the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import untokenized CSV dataset.\n",
    "tweet_dataset_untokenized = tweet_util_v2.import_dataset(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-with-irrelevant-tweets-excluded.csv\",\n",
    "    \"csv\", False)\n",
    "\n",
    "# # Import untokenized CSV dataset. (test/debug)\n",
    "# tweet_dataset_untokenized = tweet_util_v2.import_dataset(\n",
    "#     \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "#     \"twitter-dataset-7-10-19-test-subset-100-examples.csv\",\n",
    "#     \"csv\", False)\n",
    "\n",
    "# Create author-document mappings as a dictionary of key: author, values: tweet ID's\n",
    "author2doc = topic_util.topic_author_model_group_by_dataset_row_index_value(tweet_dataset_untokenized, True)\n",
    "\n",
    "# Import the dataset (absolute path).\n",
    "tweet_dataset_tokenized = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "                \"twitter-dataset-7-10-19-topic-extraction-ready-tweet-text-with-hashtags-excluded\"\n",
    "                \"-created-7-29-19-tokenized.csv\", sep=\",\")\n",
    "\n",
    "# # Import the dataset (test/debug).\n",
    "# tweet_dataset_tokenized = \\\n",
    "#     pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "#                 \"twitter-dataset-7-10-19-topic-extraction-ready-tweet-text-with-hashtags-excluded\"\n",
    "#                 \"-created-7-30-19-test.csv\", sep=\",\")\n",
    "\n",
    "# # Reindex and shuffle the data randomly.\n",
    "# tweet_dataset_tokenized = tweet_dataset_tokenized.reindex(\n",
    "#     pd.np.random.permutation(tweet_dataset_tokenized.index))\n",
    "\n",
    "# Generate a Pandas dataframe.\n",
    "tweet_text_dataframe = pd.DataFrame(tweet_dataset_tokenized)\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(f\"\\nThe shape of the Tweet text dataframe:\")\n",
    "log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "log.info(f\"\\nThe columns of the Tweet text dataframe:\")\n",
    "log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# # Drop any NaN or empty Tweet rows in dataframe (or else CountVectorizer will blow up).\n",
    "# tweet_text_dataframe = tweet_text_dataframe.dropna()\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(f\"\\nThe shape of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "log.info(f\"\\nThe columns of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# # Reindex everything.\n",
    "# tweet_text_dataframe.index = pd.RangeIndex(len(tweet_text_dataframe.index))\n",
    "\n",
    "# Assign column names.\n",
    "tweet_text_dataframe_column_names = ['text_derived', 'text_derived_preprocessed', 'text_derived_postprocessed']\n",
    "\n",
    "# Rename column in dataframe.\n",
    "tweet_text_dataframe.columns = tweet_text_dataframe_column_names\n",
    "\n",
    "# Create input feature.\n",
    "selected_features = tweet_text_dataframe[['text_derived_postprocessed']]\n",
    "processed_features = selected_features.copy()\n",
    "\n",
    "# Check what we are using as inputs.\n",
    "log.info(f\"\\nA sample Tweet in our input feature:\")\n",
    "log.info(f\"{processed_features['text_derived_postprocessed'][0]}\\n\")\n",
    "\n",
    "# Create feature set.\n",
    "slo_feature_series = processed_features['text_derived_postprocessed']\n",
    "slo_feature_series = pd.Series(slo_feature_series)\n",
    "slo_feature_list = slo_feature_series.tolist()\n",
    "\n",
    "words = []\n",
    "# Create the corpus of documents and dictionary of words (vocabulary)\n",
    "for tweet in slo_feature_list:\n",
    "    tweet_string = str(tweet)\n",
    "    words.append(tweet_string.split())\n",
    "\n",
    "log.info(f\"\\nSample of Dictionary of individual words:\")\n",
    "log.info(f\"{words[0]}\\n\")\n",
    "\n",
    "# Create the Gensim dictionary of words.\n",
    "dictionary = corpora.Dictionary(words)\n",
    "log.info(f\"\\nGensim dictionary of tokenized words.\")\n",
    "log.info(f\"{dictionary}\\n\")\n",
    "log.info(f\"\\nGensim dictionary of tokenized words with index ID's.\")\n",
    "log.info(f\"{dictionary.token2id}\\n\")\n",
    "\n",
    "# Create the Gensim corpus of document term frequencies.\n",
    "corpus = [dictionary.doc2bow(word, allow_update=True) for word in words]\n",
    "log.info(f\"# of documents in corpus: {len(corpus)}\\n\")\n",
    "log.info(f\"\\nSample of Gensim corpus of document-term frequencies.\")\n",
    "log.info(f\"{corpus[0:10]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function contains the code specific to each topic modeling library we utilize.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_topic_model_topic_extraction():\n",
    "    \"\"\"\n",
    "    Function performs topic extraction on Tweets using the Gensim Author-Topic model.\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    from gensim.models import AuthorTopicModel\n",
    "\n",
    "    model = AuthorTopicModel(corpus=corpus, num_topics=10, id2word=dictionary,\n",
    "                             author2doc=author2doc, chunksize=2000, passes=1, eval_every=0,\n",
    "                             iterations=1, random_state=1)\n",
    "    time.sleep(3)\n",
    "    topic_labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "    for topic in model.show_topics(num_topics=10):\n",
    "        print(f\"'Label: ' + {topic_labels[topic[0]]}\")\n",
    "        wordz = ''\n",
    "        for word, prob in model.show_topic(topic[0]):\n",
    "            wordz += word + ' '\n",
    "        print(f\"'Words: ' + {wordz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we call the topic modeling function and train it on our Twitter dataset.  We record the time it takes to process the entire dataset and extract topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main function.  Execute the program.\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    my_start_time = time.time()\n",
    "    ################################################\n",
    "    \"\"\"\n",
    "    Perform the topic extraction.\n",
    "    \"\"\"\n",
    "    author_topic_model_topic_extraction()\n",
    "\n",
    "    ################################################\n",
    "    my_end_time = time.time()\n",
    "\n",
    "    time_elapsed_in_seconds = (my_end_time - my_start_time)\n",
    "    time_elapsed_in_minutes = (my_end_time - my_start_time) / 60.0\n",
    "    time_elapsed_in_hours = (my_end_time - my_start_time) / 60.0 / 60.0\n",
    "    print(f\"Time taken to process dataset: {time_elapsed_in_seconds} seconds, \"\n",
    "          f\"{time_elapsed_in_minutes} minutes, {time_elapsed_in_hours} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Extraction Results on Twitter Dataset Tweet Text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First execution run.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "(run 1)\n",
    "\n",
    "Label: 1\n",
    "Words: leard go look maules work think well get have property \n",
    "Label: 2\n",
    "Words: project road beach news win rd downer ceo 's peter \n",
    "Label: 3\n",
    "Words: gas coal farmer seam water stop community people want protest \n",
    "Label: 4\n",
    "Words: creek write 's wa coal | project company timor plan \n",
    "Label: 5\n",
    "Words: gas coal forest petroleum tycoon energy price oil sale whc \n",
    "Label: 6\n",
    "Words: tax joyce barnaby slo_cashil pay chevron liberal rail coal origin \n",
    "Label: 7\n",
    "Words: narrabri water gas risk basin coal national artesian > farmer \n",
    "Label: 8\n",
    "Words: í ° ½í² beach field ¼í¶ tour $ win home \n",
    "Label: 9\n",
    "Words: eis land coal water inland go seam want appliance fracking \n",
    "Label: 10\n",
    "Words: $ wpl sto video sire share field day gain result \n",
    "\n",
    "\n",
    "Time taken to process dataset: 9229.738131284714 seconds, 153.82896885474523 minutes, 2.5638161475790873 hours.\n",
    "\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second execution run with the same hyper-parameters gives the same results."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "(run 2)\n",
    "\n",
    "Label: 1\n",
    "Words: leard go look maules work think well get have property \n",
    "Label: 2\n",
    "Words: project road beach news win rd downer ceo 's peter \n",
    "Label: 3\n",
    "Words: gas coal farmer seam water stop community people want protest \n",
    "Label: 4\n",
    "Words: creek write 's wa coal | project company timor plan \n",
    "Label: 5\n",
    "Words: gas coal forest petroleum tycoon energy price oil sale whc \n",
    "Label: 6\n",
    "Words: tax joyce barnaby slo_cashil pay chevron liberal rail coal origin \n",
    "Label: 7\n",
    "Words: narrabri water gas risk basin coal national artesian > farmer \n",
    "Label: 8\n",
    "Words: í ° ½í² beach field ¼í¶ tour $ win home \n",
    "Label: 9\n",
    "Words: eis land coal water inland go seam want appliance fracking \n",
    "Label: 10\n",
    "Words: $ wpl sto video sire share field day gain result \n",
    "\n",
    "\n",
    "Time taken to process dataset: 9202.727442502975 seconds, 153.37879070838292 minutes, 2.5563131784730486 hours.\n",
    "\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are non-ASCII characters present, which suggests we may need to modify our tokenization regular expressions and other preprocessing and postprocessing functions to remove non-English characters that are present in the Tweet text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://radimrehurek.com/gensim/models/atmodel.html\n",
    "    - Gensim author-topic modeling Class.\n",
    "    \n",
    "    \n",
    "- https://stackoverflow.com/questions/47434426/pandas-groupby-unique-multiple-columns\n",
    "    - using Pandas .groupby() function. \n",
    "\n",
    "\n",
    "- https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/\n",
    "    - using Pandas .groupby() function blog.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
