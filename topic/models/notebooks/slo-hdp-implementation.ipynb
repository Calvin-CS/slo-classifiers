{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hierarchical Dirichlet Process Topic Model Implementation on SLO Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize the Gensim Hierarchical Dirichlet Process Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the requisite libraries, custom utility functions, and set the parameters for our various imported libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import logging as log\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Pandas options.\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "pd.options.display.max_colwidth = 1000\n",
    "# Pandas float precision display.\n",
    "pd.set_option('precision', 12)\n",
    "# Seaborn setting.\n",
    "sns.set()\n",
    "# Don't output these types of warnings to terminal.\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "# Matplotlib log settings.\n",
    "mylog = log.getLogger(\"matplotlib\")\n",
    "mylog.setLevel(log.INFO)\n",
    "\n",
    "\"\"\"\n",
    "Turn debug log statements for various sections of code on/off.\n",
    "(adjust log level as necessary)\n",
    "\"\"\"\n",
    "log.basicConfig(level=log.INFO)\n",
    "log.disable(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process and Post-process Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess our Twitter dataset as follows:<br>\n",
    "\n",
    "1) Downcase all text.<br>\n",
    "2) Check that there is text, otherwise convert to empty string.<br>\n",
    "3) Convert html chars. to unicode chars.<br>\n",
    "4) Remove \"RT\" tags.<br>\n",
    "5) Remove concatenated URL's.<br>\n",
    "6) Handle whitespaces by converting all/multiple whitespace characters to a single whitespace.<br>\n",
    "7) Remove URL's and replace with \"slo_url\".<br>\n",
    "8) Remove Tweet mentions and replace with \"slo_mention\".<br>\n",
    "9) Remove Tweet stock symbols and replace with \"slo_stock\".<br>\n",
    "10) Remove Tweet hashtags and replace with \"slo_hash\".<br>\n",
    "11) Remove Tweet cashtags and replace with \"slo_cash\".<br>\n",
    "12) Remove Tweet year and replace with \"slo_year\".<br>\n",
    "13) Remove Tweet time and replace with \"slo_time\".<br>\n",
    "14) Remove character elongations.<br>\n",
    "\n",
    "We post-process our Twitter dataset as follows:<br>\n",
    "\n",
    "1) Remove the following irrelevant words specified in the List below:<br>\n",
    "\n",
    "    delete_list = [\"word_n\", \"auspol\", \"ausbiz\", \"tinto\", \"adelaide\", \"csg\", \"nswpol\",\n",
    "                   \"nsw\", \"lng\", \"don\", \"rio\", \"pilliga\", \"australia\", \"asx\", \"just\", \"today\", \"great\", \"says\", \"like\",\n",
    "                   \"big\", \"better\", \"rite\", \"would\", \"SCREEN_NAME\", \"mining\", \"former\", \"qldpod\", \"qldpol\", \"qld\", \"wr\",\n",
    "                   \"melbourne\", \"andrew\", \"fuck\", \"spadani\", \"greg\", \"th\", \"australians\", \"http\", \"https\", \"rt\",\n",
    "                   \"co\", \"amp\", \"carmichael\", \"abbot\", \"bill shorten\",\n",
    "                   \"slo_url\", \"slo_mention\", \"slo_hash\", \"slo_year\", \"slo_time\", \"slo_cash\", \"slo_stock\",\n",
    "                   \"adani\", \"bhp\", \"cuesta\", \"fotescue\", \"riotinto\", \"newmontmining\", \"santos\", \"oilsearch\",\n",
    "                   \"woodside\", \"ilukaresources\", \"whitehavencoal\",\n",
    "                   \"stopadani\", \"goadani\", \"bhpbilliton\", \"billiton\", \"cuestacoal\", \"cuests coal\", \"cqc\",\n",
    "                   \"fortescuenews\", \"fortescue metals\", \"rio tinto\", \"newmont\", \"newmont mining\", \"santosltd\",\n",
    "                   \"oilsearchltd\", \"oil search\", \"woodsideenergy\", \"woodside petroleum\", \"woodside energy\",\n",
    "                   \"iluka\", \"iluka resources\", \"whitehaven\", \"whitehaven coal\"]\n",
    "\n",
    "2) Remove all punctuation from the Tweet text.<br>\n",
    "3) Remove all English stop words from the Tweet text.<br>\n",
    "4) Lemmatize the words in the Tweet text.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize using our Twitter dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-with-irrelevant-tweets-excluded.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-topic-extraction-ready-tweet-text-with-hashtags-excluded-created-7-29-19.csv\",\n",
    "    \"text_derived\")\n",
    "\n",
    "# Tokenize using our Twitter dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-with-irrelevant-tweets-excluded.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-topic-extraction-ready-user-description-text-with-hashtags-excluded-created-7-29-19.csv\",\n",
    "    \"user_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter in our function call specifies the file path to the dataset to be preprocessed.  The second parameter specifies the location to save the CSV file to.  The 3rd parameter specifies the name of the column in the dataset that contains the original Tweet text.<br>\n",
    "\n",
    "\n",
    "Tweet preprocessing is done via a custom library imported as \"lda_util\" using \"topic_extraction_utility_functions.py\".<br>\n",
    "\n",
    "Refer to URL link for the codebase to the utility functions used above for data preprocessing and below for LDA topic extraction:<br>\n",
    "\n",
    "https://github.com/Calvin-CS/slo-classifiers/blob/master/topic/models/topic_extraction_utility_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and prepare the preprocessed dataset for use in HDP topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the general format of insertion into a Pandas dataframe, isolating the column of interest, and generating a dictionary of words and a corpus of documents.  Please refer to the code comments for details on the specific steps for the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset (absolute path).\n",
    "tweet_dataset_processed = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "                \"twitter-dataset-7-10-19-topic-extraction-ready-tweet-text-with-hashtags-excluded\"\n",
    "                \"-created-7-29-19-tokenized.csv\", sep=\",\")\n",
    "\n",
    "# # Import the dataset (test/debug).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "#                 \"twitter-dataset-7-10-19-topic-extraction-ready-tweet-text-with-hashtags-excluded\"\n",
    "#                 \"-created-7-30-19-test.csv\", sep=\",\")\n",
    "\n",
    "# Reindex and shuffle the data randomly.\n",
    "tweet_dataset_processed = tweet_dataset_processed.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed.index))\n",
    "\n",
    "# Generate a Pandas dataframe.\n",
    "tweet_text_dataframe = pd.DataFrame(tweet_dataset_processed)\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(f\"\\nThe shape of the Tweet text dataframe:\")\n",
    "log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "log.info(f\"\\nThe columns of the Tweet text dataframe:\")\n",
    "log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Drop any NaN or empty Tweet rows in dataframe (or else CountVectorizer will blow up).\n",
    "tweet_text_dataframe = tweet_text_dataframe.dropna()\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(f\"\\nThe shape of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "log.info(f\"\\nThe columns of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Reindex everything.\n",
    "tweet_text_dataframe.index = pd.RangeIndex(len(tweet_text_dataframe.index))\n",
    "\n",
    "# Assign column names.\n",
    "tweet_text_dataframe_column_names = ['text_derived', 'text_derived_preprocessed', 'text_derived_postprocessed']\n",
    "\n",
    "# Rename column in dataframe.\n",
    "tweet_text_dataframe.columns = tweet_text_dataframe_column_names\n",
    "\n",
    "# Create input feature.\n",
    "selected_features = tweet_text_dataframe[['text_derived_postprocessed']]\n",
    "processed_features = selected_features.copy()\n",
    "\n",
    "# Check what we are using as inputs.\n",
    "log.info(f\"\\nA sample Tweet in our input feature:\")\n",
    "log.info(f\"{processed_features['text_derived_postprocessed'][0]}\\n\")\n",
    "\n",
    "# Create feature set.\n",
    "slo_feature_series = processed_features['text_derived_postprocessed']\n",
    "slo_feature_series = pd.Series(slo_feature_series)\n",
    "slo_feature_list = slo_feature_series.tolist()\n",
    "\n",
    "# Convert feature list of sentences to comma-separated dictionary of words.\n",
    "words = [[text for text in tweet.split()] for tweet in slo_feature_list]\n",
    "log.info(f\"\\nDictionary of individual words:\")\n",
    "log.info(f\"{words[0]}\\n\")\n",
    "\n",
    "# Create the Gensim dictionary of words.\n",
    "dictionary = corpora.Dictionary(words)\n",
    "log.info(f\"\\nGensim dictionary of tokenized words.\")\n",
    "log.info(f\"{dictionary}\\n\")\n",
    "log.info(f\"\\nGensim dictionary of tokenized words with index ID's.\")\n",
    "log.info(f\"{dictionary.token2id}\\n\")\n",
    "\n",
    "# Create the Gensim corpus of document term frequencies.\n",
    "corpus = [dictionary.doc2bow(word, allow_update=True) for word in words]\n",
    "log.info(f\"# of documents in corpus: {len(corpus)}\\n\")\n",
    "log.info(f\"\\nSample of Gensim corpus of document-term frequencies.\")\n",
    "log.info(f\"{corpus[0:10]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function contains the code specific to each topic modeling library we utilize.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_dirichlet_process_topic_extraction():\n",
    "    \"\"\"\n",
    "    Function performs topic extraction on Tweets using the Gensim HDP model.\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    from gensim.models import HdpModel\n",
    "\n",
    "    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model.\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(slo_feature_series)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    log.info(\"\\n.fit_transform - Learn the vocabulary dictionary and return term-document matrix.\")\n",
    "    log.info(f\"{tf}\\n\")\n",
    "    log.info(\"\\n.get_feature_names - Array mapping from feature integer indices to feature name\")\n",
    "    log.info(f\"{tf_feature_names}\\n\")\n",
    "\n",
    "    # Train the HDP model.\n",
    "    hdp = HdpModel(corpus, dictionary)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # # For use as wrapper with Scikit-Learn API.\n",
    "    # model = HdpTransformer(id2word=dictionary)\n",
    "    # distribution = model.fit_transform(corpus)\n",
    "\n",
    "    # Display the top words for each topic.\n",
    "    topic_info = hdp.print_topics(num_topics=20, num_words=10)\n",
    "\n",
    "    for topic in topic_info:\n",
    "        print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we call the topic modeling function and train it on our Twitter dataset.  We record the time it takes to process the entire dataset and extract topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main function.  Execute the program.\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    my_start_time = time.time()\n",
    "    ################################################\n",
    "    \"\"\"\n",
    "    Perform the topic extraction.\n",
    "    \"\"\"\n",
    "    hierarchical_dirichlet_process_topic_extraction()\n",
    "\n",
    "    ################################################\n",
    "    my_end_time = time.time()\n",
    "\n",
    "    time_elapsed_in_seconds = (my_end_time - my_start_time)\n",
    "    time_elapsed_in_minutes = (my_end_time - my_start_time) / 60.0\n",
    "    time_elapsed_in_hours = (my_end_time - my_start_time) / 60.0 / 60.0\n",
    "    print(f\"Time taken to process dataset: {time_elapsed_in_seconds} seconds, \"\n",
    "          f\"{time_elapsed_in_minutes} minutes, {time_elapsed_in_hours} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Extraction Results on Twitter Dataset Tweet Text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First execution run.  The output shows the weight of each of the top words associated with each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(run 1)\n",
    "\n",
    "(0, \"0.025*coal + 0.008*job + 0.007*'s + 0.006*project + 0.006*stop + 0.006*want + 0.006*labor + 0.005*water + 0.005*fund + 0.005*loan\")\n",
    "(1, '0.082*í + 0.050*° + 0.043*tax + 0.041*½í² + 0.023*¼í¶\\x93 + 0.016*pay + 0.013*$ + 0.012*coal + 0.010*energy + 0.008*½í')\n",
    "(2, '0.170*$ + 0.009*cba + 0.009*anz + 0.009*wbc + 0.009*nab + 0.007*coal + 0.006*price + 0.005*fmg + 0.005*bxb + 0.005*syd')\n",
    "(3, \"0.016*coal + 0.005*'s + 0.005*job + 0.005*$ + 0.004*stop + 0.004*project + 0.004*want + 0.004*í + 0.003*fund + 0.003*reef\")\n",
    "(4, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*$ + 0.004*project + 0.004*stop + 0.004*want + 0.003*fund + 0.003*gas + 0.003*labor\")\n",
    "(5, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*$ + 0.004*project + 0.004*stop + 0.004*want + 0.003*gas + 0.003*water + 0.003*fund\")\n",
    "(6, \"0.015*coal + 0.005*$ + 0.005*job + 0.005*'s + 0.004*stop + 0.004*project + 0.004*want + 0.003*gas + 0.003*fund + 0.003*support\")\n",
    "(7, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*$ + 0.004*project + 0.004*stop + 0.004*want + 0.003*gas + 0.003*fund + 0.003*labor\")\n",
    "(8, \"0.015*coal + 0.005*job + 0.004*'s + 0.004*$ + 0.004*stop + 0.004*project + 0.003*want + 0.003*gas + 0.003*fund + 0.003*support\")\n",
    "(9, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*$ + 0.004*project + 0.004*stop + 0.004*want + 0.003*water + 0.003*gas + 0.003*fund\")\n",
    "(10, \"0.015*coal + 0.005*'s + 0.005*job + 0.004*$ + 0.004*stop + 0.004*project + 0.004*argyle + 0.004*want + 0.003*gas + 0.003*fund\")\n",
    "(11, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*$ + 0.004*project + 0.004*stop + 0.004*want + 0.003*í + 0.003*fund + 0.003*gas\")\n",
    "(12, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*$ + 0.004*project + 0.004*stop + 0.004*want + 0.003*gas + 0.003*fund + 0.003*labor\")\n",
    "(13, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*$ + 0.004*stop + 0.004*project + 0.004*want + 0.003*gas + 0.003*fund + 0.003*water\")\n",
    "(14, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*$ + 0.004*stop + 0.004*project + 0.004*want + 0.004*fund + 0.003*gas + 0.003*labor\")\n",
    "(15, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*project + 0.004*stop + 0.004*$ + 0.004*want + 0.004*gas + 0.003*water + 0.003*fund\")\n",
    "(16, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*$ + 0.004*stop + 0.004*project + 0.004*want + 0.003*fund + 0.003*support + 0.003*gas\")\n",
    "(17, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*$ + 0.004*stop + 0.004*project + 0.004*want + 0.003*fund + 0.003*gas + 0.003*support\")\n",
    "(18, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*$ + 0.004*stop + 0.004*project + 0.004*want + 0.003*fund + 0.003*support + 0.003*labor\")\n",
    "(19, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*$ + 0.004*stop + 0.004*project + 0.004*want + 0.003*gas + 0.003*fund + 0.003*labor\")\n",
    "\n",
    "\n",
    "Time taken to process dataset: 1036.1106204986572 seconds, 17.268510341644287 minutes, 0.28780850569407146 hours.\n",
    "\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second run.  The output shows the weight of each of the top words associated with each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(run 2)\n",
    "\n",
    "(0, \"0.024*coal + 0.008*job + 0.007*'s + 0.006*want + 0.006*stop + 0.006*labor + 0.006*project + 0.005*fund + 0.005*support + 0.005*reef\")\n",
    "(1, '0.158*$ + 0.009*cba + 0.008*anz + 0.008*wbc + 0.008*nab + 0.007*coal + 0.007*price + 0.005*fmg + 0.005*share + 0.005*bxb')\n",
    "(2, '0.086*í + 0.050*° + 0.043*½í² + 0.032*tax + 0.025*¼í¶\\x93 + 0.015*$ + 0.013*coal + 0.009*energy + 0.007*½í + 0.006*pay')\n",
    "(3, \"0.020*coal + 0.013*gas + 0.008*water + 0.007*project + 0.005*'s + 0.005*seam + 0.004*narrabri + 0.004*new + 0.004*rail + 0.004*company\")\n",
    "(4, \"0.035*tax + 0.024*pay + 0.012*coal + 0.007*'s + 0.006*ato + 0.005*energy + 0.004*job + 0.004*company + 0.004*haven + 0.004*chevron\")\n",
    "(5, '0.015*coal + 0.006*job + 0.006*go + 0.006*money + 0.006*taxpayer + 0.005*india + 0.005*joyce + 0.005*barnaby + 0.005*think + 0.005*give')\n",
    "(6, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*project + 0.004*stop + 0.004*$ + 0.004*want + 0.003*fund + 0.003*gas + 0.003*reef\")\n",
    "(7, \"0.016*coal + 0.005*job + 0.005*'s + 0.004*$ + 0.004*stop + 0.004*project + 0.004*loan + 0.004*fund + 0.004*want + 0.003*support\")\n",
    "(8, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*loan + 0.004*project + 0.004*stop + 0.004*labor + 0.004*$ + 0.004*want + 0.003*reef\")\n",
    "(9, \"0.015*coal + 0.006*job + 0.006*council + 0.005*townsville + 0.005*'s + 0.004*$ + 0.004*project + 0.004*stop + 0.004*fund + 0.004*pay\")\n",
    "(10, \"0.016*coal + 0.005*í + 0.005*'s + 0.005*job + 0.004*$ + 0.004*° + 0.004*stop + 0.004*project + 0.004*want + 0.003*labor\")\n",
    "(11, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*$ + 0.004*stop + 0.004*project + 0.004*want + 0.003*water + 0.003*tax + 0.003*gas\")\n",
    "(12, \"0.016*coal + 0.005*job + 0.005*'s + 0.004*fund + 0.004*$ + 0.004*stop + 0.004*project + 0.004*want + 0.003*water + 0.003*gas\")\n",
    "(13, \"0.015*coal + 0.005*job + 0.004*'s + 0.004*stop + 0.004*$ + 0.004*project + 0.004*want + 0.003*water + 0.003*fund + 0.003*gas\")\n",
    "(14, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*$ + 0.004*stop + 0.004*project + 0.004*want + 0.003*í + 0.003*fund + 0.003*support\")\n",
    "(15, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*$ + 0.004*stop + 0.004*project + 0.004*want + 0.003*gas + 0.003*fund + 0.003*support\")\n",
    "(16, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*$ + 0.004*stop + 0.004*project + 0.004*want + 0.003*fund + 0.003*gas + 0.003*support\")\n",
    "(17, \"0.015*coal + 0.006*í + 0.005*job + 0.004*'s + 0.004*$ + 0.004*stop + 0.004*project + 0.004*want + 0.003*fund + 0.003*gas\")\n",
    "(18, \"0.015*coal + 0.006*job + 0.005*'s + 0.004*$ + 0.004*stop + 0.004*project + 0.004*want + 0.003*fund + 0.003*gas + 0.003*support\")\n",
    "(19, \"0.015*coal + 0.005*job + 0.005*'s + 0.004*$ + 0.004*stop + 0.004*project + 0.004*want + 0.003*fund + 0.003*labor + 0.003*gas\")\n",
    "\n",
    "\n",
    "Time taken to process dataset: 1036.3824276924133 seconds, 17.273040461540223 minutes, 0.2878840076923371 hours.\n",
    "\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are sub-par using just default hyperparameter values.  They are mostly the same words with perhaps slightly different weight values, across all the different topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://radimrehurek.com/gensim/models/hdpmodel.html\n",
    "    - Gensim HDP topic modeling Class.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
