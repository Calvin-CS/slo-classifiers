{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hierarchical Dirichlet Process Topic Model Implementation on SLO Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the requisite libraries, custom utility functions, and set the parameters for our various imported libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import logging as log\n",
    "import warnings\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import custom utility functions.\n",
    "import topic_extraction_utility_functions as lda_util\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Pandas options.\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "pd.options.display.max_colwidth = 1000\n",
    "# Pandas float precision display.\n",
    "pd.set_option('precision', 12)\n",
    "# Seaborn setting.\n",
    "sns.set()\n",
    "# Don't output these types of warnings to terminal.\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "# Matplotlib log settings.\n",
    "mylog = log.getLogger(\"matplotlib\")\n",
    "mylog.setLevel(log.INFO)\n",
    "\n",
    "\"\"\"\n",
    "Turn debug log statements for various sections of code on/off.\n",
    "(adjust log level as necessary)\n",
    "\"\"\"\n",
    "log.basicConfig(level=log.INFO)\n",
    "log.disable(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process and Post-process Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize our Tweets using our dataset preprocessing and postprocessing function.  Please refer to \"topic_extraction_utility_functions.py\" for the full code-base and comments on what is done to the Tweet text and user description text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on our topic modeling dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-test-subset-100-examples.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-lda-ready-tweet-text-with-hashtags-excluded-created-7-17-19.csv\",\n",
    "    \"text_derived\")\n",
    "\n",
    "# Test on our topic modeling dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-test-subset-100-examples.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-lda-ready-user-description-text-with-hashtags-excluded-created-7-17-19.csv\",\n",
    "    \"user_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and prepare the preprocessed dataset for use in HDP topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the general format of insertion into a Pandas dataframe, isolating the column of interest, and generating a dictionary of words and corpus of documents.  Please refer to the code comments for details on the specific steps for the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the dataset (relative path).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"twitter-dataset-7-10-19-lda-ready-tweet-text-with-hashtags-excluded-created-7-17-19.csv\", sep=\",\")\n",
    "\n",
    "# Import the dataset (absolute path).\n",
    "tweet_dataset_processed = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "                \"twitter-dataset-7-10-19-lda-ready-tweet-text-with-hashtags-excluded-created-7-17-19.csv\", sep=\",\")\n",
    "\n",
    "# # Import the dataset (test/debug).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"twitter-dataset-7-10-19-lda-ready-tweet-text-test.csv\", sep=\",\")\n",
    "\n",
    "# # Import the dataset (test/debug).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "#                 \"twitter-dataset-7-10-19-lda-ready-tweet-text-test.csv\", sep=\",\")\n",
    "\n",
    "# Reindex and shuffle the data randomly.\n",
    "tweet_dataset_processed = tweet_dataset_processed.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed.index))\n",
    "\n",
    "# Generate a Pandas dataframe.\n",
    "tweet_text_dataframe = pd.DataFrame(tweet_dataset_processed)\n",
    "\n",
    "# # Print shape and column names.\n",
    "# log.info(f\"\\nThe shape of the Tweet text dataframe:\")\n",
    "# log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "# log.info(f\"\\nThe columns of the Tweet text dataframe:\")\n",
    "# log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(\"\\nThe shape of the Tweet text dataframe:\")\n",
    "log.info(tweet_text_dataframe.shape)\n",
    "log.info(\"\\nThe columns of the Tweet text dataframe:\")\n",
    "log.info(tweet_text_dataframe.columns)\n",
    "\n",
    "# Drop any NaN or empty Tweet rows in dataframe (or else CountVectorizer will blow up).\n",
    "tweet_text_dataframe = tweet_text_dataframe.dropna()\n",
    "\n",
    "# # Print shape and column names.\n",
    "# log.info(f\"\\nThe shape of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "# log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "# log.info(f\"\\nThe columns of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "# log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(\"\\nThe shape of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(tweet_text_dataframe.shape)\n",
    "log.info(\"\\nThe columns of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(tweet_text_dataframe.columns)\n",
    "\n",
    "# Reindex everything.\n",
    "tweet_text_dataframe.index = pd.RangeIndex(len(tweet_text_dataframe.index))\n",
    "\n",
    "# Assign column names.\n",
    "tweet_text_dataframe_column_names = ['text_derived', 'text_derived_preprocessed', 'text_derived_postprocessed']\n",
    "\n",
    "# Rename column in dataframe.\n",
    "tweet_text_dataframe.columns = tweet_text_dataframe_column_names\n",
    "\n",
    "# Create input feature.\n",
    "selected_features = tweet_text_dataframe[['text_derived_postprocessed']]\n",
    "processed_features = selected_features.copy()\n",
    "\n",
    "# # Check what we are using as inputs.\n",
    "# log.info(f\"\\nA sample Tweet in our input feature:\")\n",
    "# log.info(f\"{processed_features['text_derived_postprocessed'][0]}\\n\")\n",
    "\n",
    "# Check what we are using as inputs.\n",
    "log.info(\"\\nA sample Tweet in our input feature:\")\n",
    "log.info(processed_features['text_derived_postprocessed'][0])\n",
    "\n",
    "# Create feature set.\n",
    "slo_feature_series = processed_features['text_derived_postprocessed']\n",
    "slo_feature_series = pd.Series(slo_feature_series)\n",
    "slo_feature_list = slo_feature_series.tolist()\n",
    "\n",
    "# Convert feature list of sentences to comma-separated dictionary of words.\n",
    "words = [[text for text in tweet.split()] for tweet in slo_feature_list]\n",
    "# log.info(f\"\\nDictionary of individual words:\")\n",
    "# log.info(f\"{words[0]}\\n\")\n",
    "log.info(\"\\nDictionary of individual words:\")\n",
    "log.info(words[0])\n",
    "\n",
    "# # Create the Gensim dictionary of words.\n",
    "# dictionary = corpora.Dictionary(words)\n",
    "# log.info(f\"\\nGensim dictionary of tokenized words.\")\n",
    "# log.info(f\"{dictionary}\\n\")\n",
    "# log.info(f\"\\nGensim dictionary of tokenized words with index ID's.\")\n",
    "# log.info(f\"{dictionary.token2id}\\n\")\n",
    "\n",
    "# Create the Gensim dictionary of words.\n",
    "dictionary = corpora.Dictionary(words)\n",
    "log.info(\"\\nGensim dictionary of tokenized words.\")\n",
    "log.info(dictionary)\n",
    "log.info(\"\\n\")\n",
    "log.info(\"\\nGensim dictionary of tokenized words with index ID's.\")\n",
    "log.info(dictionary.token2id)\n",
    "log.info(\"\\n\")\n",
    "\n",
    "# # Create the Gensim corpus of document term frequencies.\n",
    "# corpus = [dictionary.doc2bow(word, allow_update=True) for word in words]\n",
    "# log.info(f\"# of documents in corpus: {len(corpus)}\\n\")\n",
    "# log.info(f\"\\nSample of Gensim corpus of document-term frequencies.\")\n",
    "# log.info(f\"{corpus[0:10]}\\n\")\n",
    "\n",
    "# Create the Gensim corpus of document term frequencies.\n",
    "corpus = [dictionary.doc2bow(word, allow_update=True) for word in words]\n",
    "log.info(\"# of documents in corpus: \" + str(len(corpus)) + \"\\n\")\n",
    "log.info(\"\\nSample of Gensim corpus of document-term frequencies.\")\n",
    "log.info(corpus[0:10])\n",
    "log.info(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function contains the code specific to each topic modeling library we utilize.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_dirichlet_process_topic_extraction():\n",
    "    \"\"\"\n",
    "    Function performs topic extraction on Tweets using the Gensim HDP model.\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    from gensim.test.utils import common_corpus, common_dictionary\n",
    "    from gensim.models import HdpModel\n",
    "    from gensim.sklearn_api import HdpTransformer\n",
    "\n",
    "    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model.\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(slo_feature_series)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    # log.info(\"\\n.fit_transform - Learn the vocabulary dictionary and return term-document matrix.\")\n",
    "    # log.info(f\"{tf}\\n\")\n",
    "    # log.info(\"\\n.get_feature_names - Array mapping from feature integer indices to feature name\")\n",
    "    # log.info(f\"{tf_feature_names}\\n\")\n",
    "\n",
    "    log.info(\"\\n.fit_transform - Learn the vocabulary dictionary and return term-document matrix.\")\n",
    "    log.info(tf)\n",
    "    log.info(\"\\n.get_feature_names - Array mapping from feature integer indices to feature name\")\n",
    "    log.info(tf_feature_names)\n",
    "\n",
    "    # # Sample dictionary and corpus.\n",
    "    # log.info(f\"\\nExample dictionary format for Gensim:\")\n",
    "    # log.info(f\"{common_dictionary}\\n\")\n",
    "    # log.info(f\"\\nExample corpus format for Gensim:\")\n",
    "    # log.info(f\"{common_corpus}\\n\")\n",
    "\n",
    "    # Train the HDP model.\n",
    "    hdp = HdpModel(corpus, dictionary)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # # For use as wrapper with Scikit-Learn API.\n",
    "    # model = HdpTransformer(id2word=dictionary)\n",
    "    # distribution = model.fit_transform(corpus)\n",
    "\n",
    "    # Display the top words for each topic.\n",
    "    topic_info = hdp.print_topics(num_topics=20, num_words=10)\n",
    "\n",
    "    for topic in topic_info:\n",
    "        print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we call the topic modeling function and train it on our Twitter dataset.  We record the time it takes to process the entire dataset and extract topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main function.  Execute the program.\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    my_start_time = time.time()\n",
    "    ################################################\n",
    "    \"\"\"\n",
    "    Perform the topic extraction.\n",
    "    \"\"\"\n",
    "    hierarchical_dirichlet_process_topic_extraction()\n",
    "\n",
    "    ################################################\n",
    "    my_end_time = time.time()\n",
    "\n",
    "    time_elapsed_in_seconds = (my_end_time - my_start_time)\n",
    "    time_elapsed_in_minutes = (my_end_time - my_start_time) / 60.0\n",
    "    time_elapsed_in_hours = (my_end_time - my_start_time) / 60.0 / 60.0\n",
    "    # print(f\"Time taken to process dataset: {time_elapsed_in_seconds} seconds, \"\n",
    "    #       f\"{time_elapsed_in_minutes} minutes, {time_elapsed_in_hours} hours.\")\n",
    "    print(\"\\n\\nTime taken to process dataset: \" + str(time_elapsed_in_seconds) + \" seconds, \" +\n",
    "          str(time_elapsed_in_minutes) + \" minutes, \" + str(time_elapsed_in_hours) + \" hours.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Extraction Results on Twitter Dataset Tweet Text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First run.  Output shows the weight of each of the top words associated with each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(run 1)\n",
    "(0, \"0.044*adani + 0.022*coal + 0.008*santos + 0.007*job + 0.007*'s + 0.006*bhp + 0.006*project + 0.006*stop + 0.005*australia + 0.005*want\")\n",
    "(1, '0.076*í + 0.048*° + 0.042*tax + 0.040*½í² + 0.022*¼í¶\\x93 + 0.021*adani + 0.016*pay + 0.013*$ + 0.011*coal + 0.010*energy')\n",
    "(2, '0.144*$ + 0.027*bhp + 0.013*rio + 0.012*adani + 0.008*cba + 0.008*anz + 0.007*wbc + 0.007*nab + 0.006*coal + 0.006*price')\n",
    "(3, \"0.028*adani + 0.013*coal + 0.009*santos + 0.007*bhp + 0.006*rio + 0.005*australia + 0.005*tinto + 0.004*'s + 0.004*job + 0.004*$\")\n",
    "(4, \"0.029*adani + 0.014*coal + 0.008*santos + 0.007*bhp + 0.004*job + 0.004*'s + 0.004*australia + 0.004*rio + 0.004*$ + 0.004*stop\")\n",
    "(5, \"0.029*adani + 0.014*coal + 0.009*santos + 0.007*bhp + 0.004*australia + 0.004*'s + 0.004*job + 0.004*rio + 0.004*project + 0.004*$\")\n",
    "(6, \"0.026*adani + 0.013*coal + 0.011*bhp + 0.007*santos + 0.007*rio + 0.006*tinto + 0.004*$ + 0.004*australia + 0.004*job + 0.004*'s\")\n",
    "(7, \"0.029*adani + 0.014*coal + 0.008*santos + 0.007*bhp + 0.004*'s + 0.004*job + 0.004*australia + 0.004*stop + 0.004*rio + 0.004*$\")\n",
    "(8, \"0.028*adani + 0.014*coal + 0.008*santos + 0.008*bhp + 0.004*rio + 0.004*'s + 0.004*australia + 0.004*job + 0.004*$ + 0.004*project\")\n",
    "(9, \"0.029*adani + 0.014*coal + 0.008*santos + 0.007*bhp + 0.004*'s + 0.004*job + 0.004*australia + 0.004*rio + 0.004*$ + 0.004*stop\")\n",
    "(10, \"0.029*adani + 0.014*coal + 0.008*santos + 0.008*bhp + 0.004*$ + 0.004*'s + 0.004*rio + 0.004*australia + 0.004*job + 0.004*project\")\n",
    "(11, \"0.029*adani + 0.014*coal + 0.009*santos + 0.007*bhp + 0.004*'s + 0.004*job + 0.004*australia + 0.004*rio + 0.004*stop + 0.004*project\")\n",
    "(12, \"0.029*adani + 0.014*coal + 0.008*santos + 0.007*bhp + 0.004*'s + 0.004*job + 0.004*australia + 0.004*rio + 0.004*$ + 0.004*project\")\n",
    "(13, \"0.029*adani + 0.014*coal + 0.008*santos + 0.008*bhp + 0.004*rio + 0.004*'s + 0.004*australia + 0.004*job + 0.004*$ + 0.004*project\")\n",
    "(14, \"0.029*adani + 0.014*coal + 0.008*santos + 0.007*bhp + 0.004*'s + 0.004*australia + 0.004*job + 0.004*rio + 0.004*$ + 0.004*project\")\n",
    "(15, \"0.028*adani + 0.014*coal + 0.008*santos + 0.008*bhp + 0.007*$ + 0.005*í + 0.004*rio + 0.004*australia + 0.004*job + 0.004*'s\")\n",
    "(16, \"0.029*adani + 0.014*coal + 0.008*santos + 0.007*bhp + 0.004*$ + 0.004*'s + 0.004*job + 0.004*australia + 0.004*rio + 0.004*project\")\n",
    "(17, \"0.029*adani + 0.014*coal + 0.008*santos + 0.007*bhp + 0.004*'s + 0.004*job + 0.004*australia + 0.004*rio + 0.004*$ + 0.004*project\")\n",
    "(18, \"0.029*adani + 0.014*coal + 0.008*santos + 0.007*bhp + 0.004*'s + 0.004*job + 0.004*australia + 0.004*rio + 0.004*$ + 0.004*stop\")\n",
    "(19, \"0.029*adani + 0.014*coal + 0.008*santos + 0.007*bhp + 0.005*'s + 0.004*job + 0.004*australia + 0.004*rio + 0.004*$ + 0.004*project\")\n",
    "\n",
    "\n",
    "Time taken to process dataset: 1077.2826988697052 seconds, 17.95471164782842 minutes, 0.2992451941304737 hours.\n",
    "\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second run.  Output shows the weight of each of the top words associated with each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(run 2)\n",
    "(0, \"0.046*adani + 0.022*coal + 0.008*job + 0.007*bhp + 0.007*'s + 0.006*project + 0.006*stop + 0.006*santos + 0.005*australia + 0.005*want\")\n",
    "(1, '0.079*í + 0.048*° + 0.040*tax + 0.040*½í² + 0.022*¼í¶\\x93 + 0.021*adani + 0.016*pay + 0.013*$ + 0.011*coal + 0.009*energy')\n",
    "(2, '0.030*santos + 0.019*adani + 0.016*gas + 0.013*coal + 0.012*nsw + 0.011*water + 0.008*pilliga + 0.007*great + 0.006*project + 0.006*narrabri')\n",
    "(3, '0.154*$ + 0.024*bhp + 0.014*adani + 0.010*rio + 0.008*cba + 0.008*wbc + 0.008*anz + 0.008*nab + 0.007*coal + 0.005*price')\n",
    "(4, \"0.030*adani + 0.017*coal + 0.008*santos + 0.007*bhp + 0.005*'s + 0.004*australia + 0.004*rio + 0.004*job + 0.004*project + 0.003*want\")\n",
    "(5, \"0.029*adani + 0.016*coal + 0.010*santos + 0.007*bhp + 0.005*water + 0.005*gas + 0.004*'s + 0.004*point + 0.004*australia + 0.004*rio\")\n",
    "(6, \"0.027*adani + 0.013*coal + 0.010*santos + 0.007*bhp + 0.006*'s + 0.004*project + 0.004*australia + 0.004*rio + 0.004*job + 0.004*$\")\n",
    "(7, \"0.029*adani + 0.014*coal + 0.008*santos + 0.007*bhp + 0.004*'s + 0.004*job + 0.004*australia + 0.004*rio + 0.004*$ + 0.004*stop\")\n",
    "(8, \"0.026*adani + 0.013*coal + 0.010*bhp + 0.009*rio + 0.007*santos + 0.007*tinto + 0.005*australia + 0.004*$ + 0.004*'s + 0.004*job\")\n",
    "(9, \"0.028*adani + 0.015*coal + 0.008*santos + 0.007*bhp + 0.004*job + 0.004*'s + 0.004*australia + 0.004*$ + 0.004*rio + 0.004*project\")\n",
    "(10, \"0.029*adani + 0.013*coal + 0.008*santos + 0.007*bhp + 0.005*rio + 0.005*want + 0.004*'s + 0.004*job + 0.004*australia + 0.004*$\")\n",
    "(11, \"0.028*adani + 0.014*coal + 0.009*santos + 0.007*bhp + 0.004*'s + 0.004*australia + 0.004*job + 0.004*$ + 0.004*rio + 0.004*stop\")\n",
    "(12, \"0.029*adani + 0.014*coal + 0.008*santos + 0.007*bhp + 0.004*job + 0.004*$ + 0.004*'s + 0.004*australia + 0.004*rio + 0.004*project\")\n",
    "(13, \"0.030*adani + 0.014*coal + 0.008*santos + 0.007*bhp + 0.004*'s + 0.004*australia + 0.004*job + 0.004*rio + 0.004*$ + 0.004*stop\")\n",
    "(14, \"0.029*adani + 0.014*coal + 0.008*santos + 0.008*bhp + 0.004*job + 0.004*australia + 0.004*'s + 0.004*rio + 0.004*$ + 0.004*stop\")\n",
    "(15, \"0.028*adani + 0.014*coal + 0.008*santos + 0.007*bhp + 0.004*'s + 0.004*australia + 0.004*job + 0.004*rio + 0.004*$ + 0.003*stop\")\n",
    "(16, \"0.029*adani + 0.014*coal + 0.008*santos + 0.008*bhp + 0.004*australia + 0.004*'s + 0.004*job + 0.004*$ + 0.004*rio + 0.004*project\")\n",
    "(17, \"0.029*adani + 0.014*coal + 0.008*santos + 0.007*bhp + 0.004*'s + 0.004*australia + 0.004*job + 0.004*rio + 0.004*$ + 0.004*project\")\n",
    "(18, \"0.029*adani + 0.014*coal + 0.009*santos + 0.007*bhp + 0.004*'s + 0.004*job + 0.004*australia + 0.004*rio + 0.004*$ + 0.004*stop\")\n",
    "(19, \"0.029*adani + 0.014*coal + 0.008*santos + 0.007*bhp + 0.004*'s + 0.004*job + 0.004*australia + 0.004*rio + 0.004*$ + 0.004*project\")\n",
    "\n",
    "\n",
    "Time taken to process dataset: 1071.8396093845367 seconds, 17.863993489742278 minutes, 0.29773322482903797 hours.\n",
    "\n",
    "\n",
    "Process finished with exit code 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are sub-par using just default hyperparameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://radimrehurek.com/gensim/models/hdpmodel.html\n",
    "    - Gensim HDP topic modeling Class.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
