{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Author-Topic Topic Modeling Algorithm Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook file provides a very simple high-level overview of the Author-Topic topic modeling algorithm.  We briefly discuss the plate notation diagram, pseudocode, and statistical formula for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plate Notation for the Author-Topic Algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![author-topic](../images/author_topic_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author-topic model is a combination of the LDA and Author model.  Latent (hidden) variables are un-shaded whereas observed (visible) variables are shaded.\n",
    "\n",
    "$a_{d}$ - group of authors associated with a document \"d\".\n",
    "\n",
    "$D$ - the entire collection of documents in the corpus.\n",
    "\n",
    "$N_{d}$ - the entire vocabulary of words in a document \"d\".\n",
    "\n",
    "$x$ - author associated with the given word chosen from a $a_{d}$\n",
    "\n",
    "$z$ - the topic assigned to the word.\n",
    "\n",
    "$w$ - the selected word.\n",
    "\n",
    "$\\alpha$ - Dirichlet distribution prior.\n",
    "\n",
    "$\\theta$ - author-topic distribution.\n",
    "\n",
    "$A$ - the set of authors in the corpus.\n",
    "\n",
    "$\\beta$ - Dirichlet distribution prior.\n",
    "\n",
    "$\\phi$ - word-topic distribution.\n",
    "\n",
    "$T$ - the set of topics in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Formula for Calculating the Author-Topic algorithm):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![author-topic equation](../images/author_topic_equation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_{i} = j$ - assignment of the 'i'th word in a document to topic j.\n",
    "\n",
    "$x_{i} = k$ - assignment of the 'i'th word in a document to author k.\n",
    "\n",
    "$w_{i} = m$ - observation that the 'ith' word in a document is the 'm'th word in the vocabulary.\n",
    "\n",
    "$z_{-i}$ - all topic assignments excluding the 'i'th word.\n",
    "\n",
    "$x_{-i}$ - all author assignments excluding the 'i'th word.\n",
    "\n",
    "$w_{-i}$ - ?\n",
    "\n",
    "$a_{d}$ - group of authors associated with a document \"d\".\n",
    "\n",
    "$\\propto$ - is proportional to symbol.\n",
    "\n",
    "$\\sum_{}^{}$ - summation symbol.\n",
    "\n",
    "$C^{AT}_{kj}$ - # of times author 'k' is assigned to topic 'j' excluding the current instance.\n",
    "\n",
    "$C^{WT}_{mj}$ - # of times word 'm' is assigned to topic 'j' excluding the current instance.\n",
    "\n",
    "$\\alpha$ - hyperparameter set at 50 / T.\n",
    "\n",
    "$\\beta$ - hyperparameters set at 0.01.\n",
    "\n",
    "**TODO - understand what prime is**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode for the Author-Topic algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This peudocode was obtained from a different article than the one used for the plate diagram and statistical formula.  The author of this article set the # of iterations to 500 but this value can be anything desired.  The $\\alpha$ and $\\beta$ hyperparameter values are also set but can be adjusted as necessary.  Otherwise, the steps describe should be representative of the plate diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![author-topic pseudocode](../images/author_topic_pseudocode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simplified Author-Topic Topic Modeling Algorithm Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder.\n",
    "\n",
    "**TODO - implement simple hand-worked example of one iteration through the algorithm (provided we can find an example)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion of the FIRST iteration of the Author-Topic algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rinse and repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Referenced:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://en.wikipedia.org/wiki/Greek_alphabet\n",
    "    - for Greek alphabet name reference.\n",
    "    \n",
    "    \n",
    "- https://www.slideshare.net/FREEZ7/author-topic-model?from_action=save\n",
    "    - nice slides explaining the author-topic model.\n",
    "\n",
    "\n",
    "- https://www.slideshare.net/clauwa/topic-models-5274169\n",
    "    - another set of slides explaining the author-topic model algorithm.\n",
    "    \n",
    "\n",
    "-  https://www.researchgate.net/publication/316983079_Mining_Individual_Learning_Topics_in_Course_Reviews_Based_on_Author_Topic_Model\n",
    "    - finally found a direct pseudocode represntation for ATM.\n",
    "\n",
    "\n",
    "- https://en.wiktionary.org/wiki/%E2%88%9D\n",
    "    - what the symbol means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
