{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hierarchical Latent Dirichlet Allocation Topic Model Implementation on SLO Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our HLDA topic model utilizes a 3rd party library based on a Gibbs sampler ported over from the Java-based MALLET machine learning suite of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the requisite libraries, custom utility functions, and set the parameters for our various imported libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import logging as log\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Pandas options.\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "pd.options.display.max_colwidth = 1000\n",
    "# Pandas float precision display.\n",
    "pd.set_option('precision', 12)\n",
    "# Seaborn setting.\n",
    "sns.set()\n",
    "# Don't output these types of warnings to terminal.\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "# Matplotlib log settings.\n",
    "mylog = log.getLogger(\"matplotlib\")\n",
    "mylog.setLevel(log.INFO)\n",
    "\n",
    "\"\"\"\n",
    "Turn debug log statements for various sections of code on/off.\n",
    "(adjust log level as necessary)\n",
    "\"\"\"\n",
    "log.basicConfig(level=log.INFO)\n",
    "log.disable(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process and Post-process Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "We preprocess our Twitter dataset as follows:<br>\n",
    "\n",
    "1) Downcase all text.<br>\n",
    "2) Check that there is text, otherwise convert to empty string.<br>\n",
    "3) Convert html chars. to unicode chars.<br>\n",
    "4) Remove \"RT\" tags.<br>\n",
    "5) Remove concatenated URL's.<br>\n",
    "6) Handle whitespaces by converting all/multiple whitespace characters to a single whitespace.<br>\n",
    "7) Remove URL's and replace with \"slo_url\".<br>\n",
    "8) Remove Tweet mentions and replace with \"slo_mention\".<br>\n",
    "9) Remove Tweet stock symbols and replace with \"slo_stock\".<br>\n",
    "10) Remove Tweet hashtags and replace with \"slo_hash\".<br>\n",
    "11) Remove Tweet cashtags and replace with \"slo_cash\".<br>\n",
    "12) Remove Tweet year and replace with \"slo_year\".<br>\n",
    "13) Remove Tweet time and replace with \"slo_time\".<br>\n",
    "14) Remove character elongations.<br>\n",
    "\n",
    "We postprocess our Twitter dataset as follows:<br>\n",
    "\n",
    "1) Remove the following irrelevant words specified in the List below:<br>\n",
    "\n",
    "    delete_list = [\"word_n\", \"auspol\", \"ausbiz\", \"tinto\", \"adelaide\", \"csg\", \"nswpol\",\n",
    "                   \"nsw\", \"lng\", \"don\", \"rio\", \"pilliga\", \"australia\", \"asx\", \"just\", \"today\", \"great\", \"says\", \"like\",\n",
    "                   \"big\", \"better\", \"rite\", \"would\", \"SCREEN_NAME\", \"mining\", \"former\", \"qldpod\", \"qldpol\", \"qld\", \"wr\",\n",
    "                   \"melbourne\", \"andrew\", \"fuck\", \"spadani\", \"greg\", \"th\", \"australians\", \"http\", \"https\", \"rt\",\n",
    "                   \"co\", \"amp\", \"carmichael\", \"abbot\", \"bill shorten\",\n",
    "                   \"slo_url\", \"slo_mention\", \"slo_hash\", \"slo_year\", \"slo_time\", \"slo_cash\", \"slo_stock\",\n",
    "                   \"adani\", \"bhp\", \"cuesta\", \"fotescue\", \"riotinto\", \"newmontmining\", \"santos\", \"oilsearch\",\n",
    "                   \"woodside\", \"ilukaresources\", \"whitehavencoal\",\n",
    "                   \"stopadani\", \"goadani\", \"bhpbilliton\", \"billiton\", \"cuestacoal\", \"cuests coal\", \"cqc\",\n",
    "                   \"fortescuenews\", \"fortescue metals\", \"rio tinto\", \"newmont\", \"newmont mining\", \"santosltd\",\n",
    "                   \"oilsearchltd\", \"oil search\", \"woodsideenergy\", \"woodside petroleum\", \"woodside energy\",\n",
    "                   \"iluka\", \"iluka resources\", \"whitehaven\", \"whitehaven coal\"]\n",
    "\n",
    "2) Remove all punctuation from the Tweet text.<br>\n",
    "3) Remove all English stop words from the Tweet text.<br>\n",
    "4) Lemmatize the words in the Tweet text.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize using our Twitter dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-with-irrelevant-tweets-excluded.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-topic-extraction-ready-tweet-text-with-hashtags-excluded-created-7-29-19.csv\",\n",
    "    \"text_derived\")\n",
    "\n",
    "# Tokenize using our Twitter dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-with-irrelevant-tweets-excluded.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-topic-extraction-ready-user-description-text-with-hashtags-excluded-created-7-29-19.csv\",\n",
    "    \"user_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The first parameter in our function call specifies the file path to the dataset to be preprocessed.  The second parameter specifies the location to save the CSV file to.  The 3rd parameter specifies the name of the column in the dataset that contains the original Tweet text.<br>\n",
    "\n",
    "\n",
    "Tweet preprocessing is done via a custom library imported as \"lda_util\" using \"topic_extraction_utility_functions.py\".<br>\n",
    "\n",
    "Refer to URL link for the codebase to the utility functions used above for data preprocessing and below for LDA topic extraction:<br>\n",
    "\n",
    "https://github.com/Calvin-CS/slo-classifiers/blob/master/topic/models/topic_extraction_utility_functions.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and prepare the preprocessed dataset for use in HLDA topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the general format of insertion into a Pandas dataframe, isolating the column of interest, and generating a dictionary of words and corpus of documents.  Please refer to the code comments for details on the specific steps for the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset (absolute path).\n",
    "tweet_dataset_processed = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "                \"twitter-dataset-7-10-19-topic-extraction-ready-tweet-text-with-hashtags-excluded\"\n",
    "                \"-created-7-29-19-tokenized.csv\", sep=\",\")\n",
    "\n",
    "# # Import the dataset (test/debug).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "#                 \"twitter-dataset-7-10-19-topic-extraction-ready-tweet-text-with-hashtags-excluded\"\n",
    "#                 \"-created-7-30-19-test.csv\", sep=\",\")\n",
    "\n",
    "# Reindex and shuffle the data randomly.\n",
    "tweet_dataset_processed = tweet_dataset_processed.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed.index))\n",
    "\n",
    "# Generate a Pandas dataframe.\n",
    "tweet_text_dataframe = pd.DataFrame(tweet_dataset_processed)\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(f\"\\nThe shape of the Tweet text dataframe:\")\n",
    "log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "log.info(f\"\\nThe columns of the Tweet text dataframe:\")\n",
    "log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Drop any NaN or empty Tweet rows in dataframe (or else CountVectorizer will blow up).\n",
    "tweet_text_dataframe = tweet_text_dataframe.dropna()\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(f\"\\nThe shape of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "log.info(f\"\\nThe columns of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Reindex everything.\n",
    "tweet_text_dataframe.index = pd.RangeIndex(len(tweet_text_dataframe.index))\n",
    "\n",
    "# Assign column names.\n",
    "tweet_text_dataframe_column_names = ['text_derived', 'text_derived_preprocessed', 'text_derived_postprocessed']\n",
    "\n",
    "# Rename column in dataframe.\n",
    "tweet_text_dataframe.columns = tweet_text_dataframe_column_names\n",
    "\n",
    "# Create input feature.\n",
    "selected_features = tweet_text_dataframe[['text_derived_postprocessed']]\n",
    "processed_features = selected_features.copy()\n",
    "\n",
    "# Check what we are using as inputs.\n",
    "log.info(f\"\\nA sample Tweet in our input feature:\")\n",
    "log.info(f\"{processed_features['text_derived_postprocessed'][0]}\\n\")\n",
    "\n",
    "# Create feature set.\n",
    "slo_feature_series = processed_features['text_derived_postprocessed']\n",
    "slo_feature_series = pd.Series(slo_feature_series)\n",
    "slo_feature_list = slo_feature_series.tolist()\n",
    "\n",
    "#############################################################\n",
    "\n",
    "corpus = []\n",
    "dictionary = set()\n",
    "nlp = spacy.load('en')\n",
    "nlp.remove_pipe(\"parser\")\n",
    "nlp.remove_pipe(\"tagger\")\n",
    "nlp.remove_pipe(\"ner\")\n",
    "\n",
    "# Create the corpus of documents and dictionary of words (vocabulary)\n",
    "for tweet in slo_feature_list:\n",
    "    # Tokenize each Tweet (document) and add to List of documents in the corpus.\n",
    "    corpus.append(tweet.split())\n",
    "    # Tokenize each Tweet (document) and add individual words to the dictionary of words (vocabulary).\n",
    "    dictionary.update(tweet.split())\n",
    "\n",
    "# Attach indices to each word to represent their position in the dictionary of words (vocabulary).\n",
    "dictionary = sorted(list(dictionary))\n",
    "vocab_index = {}\n",
    "for i, w in enumerate(dictionary):\n",
    "    vocab_index[w] = i\n",
    "\n",
    "print(f\"\\nThe number of documents: {len(slo_feature_list)}\")\n",
    "print(f\"\\nThe number of words in the dictionary: {len(dictionary)}\")\n",
    "print(f\"Sample of the words in the dictionary:\\n {dictionary[0:100]}\")\n",
    "print(f\"\\nThe number of documents in the corpus: {len(corpus)}\")\n",
    "print(f\"Sample of the documents in the corpus:\\n {corpus}\")\n",
    "\n",
    "# Visualize the dictionary of words.\n",
    "wordcloud = WordCloud(background_color='white').generate(' '.join(slo_feature_list))\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLength of the dictionary, corpus, document 0 in corpus, document 1 in corpus (in that order)\")\n",
    "print(len(dictionary), len(corpus), len(corpus[0]), len(corpus[1]))\n",
    "\n",
    "\"\"\"\n",
    "Modify the corpus of documents to store the index value of each word from the dictionary (vocabulary)\n",
    "rather than the words themselves.\n",
    "\"\"\"\n",
    "new_corpus = []\n",
    "for document in corpus:\n",
    "    new_document = []\n",
    "    for word in document:\n",
    "        word_index = vocab_index[word]\n",
    "        new_document.append(word_index)\n",
    "    new_corpus.append(new_document)\n",
    "\n",
    "print(\"\\nLength of the dictionary and corpus (as word dictionary index values (in that order))\")\n",
    "print(len(dictionary), len(new_corpus))\n",
    "\n",
    "print(\"\\nDocument 0 in the corpus as tokenized words:\")\n",
    "print(corpus[0][0:10])\n",
    "print(\"Document 0 in the corpus as tokenized word index values from the dictionary:\")\n",
    "print(new_corpus[0][0:10])\n",
    "\n",
    "print(\"\\nDocument 1 in the corpus as tokenized words:\")\n",
    "print(corpus[1][0:10])\n",
    "print(\"Document 1 in the corpus as tokenized word index values from the dictionary:\")\n",
    "print(new_corpus[1][0:10])\n",
    "\n",
    "print(\"\\nDocument 2 in the corpus as tokenized words:\")\n",
    "print(corpus[2][0:10])\n",
    "print(\"Document 2 in the corpus as tokenized word index values from the dictionary:\")\n",
    "print(new_corpus[2][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function contains the code specific to each topic modeling library we utilize.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_latent_dirichlet_allocation_topic_extraction():\n",
    "    \"\"\"\n",
    "    Function performs topic extraction on Tweets using the Gensim HDP model.\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    from hlda.sampler import HierarchicalLDA\n",
    "\n",
    "    # Set parameters.\n",
    "    n_samples = 500  # no of iterations for the sampler\n",
    "    alpha = 10.0  # smoothing over level distributions\n",
    "    gamma = 1.0  # CRP smoothing parameter; number of imaginary customers at next, as yet unused table\n",
    "    eta = 0.1  # smoothing over topic-word distributions\n",
    "    num_levels = 3  # the number of levels in the tree\n",
    "    display_topics = 50  # the number of iterations between printing a brief summary of the topics so far\n",
    "    n_words = 5  # the number of most probable words to print for each topic after model estimation\n",
    "    with_weights = False  # whether to print the words with the weights\n",
    "\n",
    "    # Train the model.\n",
    "    hlda = HierarchicalLDA(new_corpus, dictionary, alpha=alpha, gamma=gamma, eta=eta, num_levels=num_levels)\n",
    "    hlda.estimate(n_samples, display_topics=display_topics, n_words=n_words, with_weights=with_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we call the topic modeling function and train it on our Twitter dataset.  We record the time it takes to process the entire dataset and extract topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main function.  Execute the program.\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    my_start_time = time.time()\n",
    "    ################################################\n",
    "    \"\"\"\n",
    "    Perform the topic extraction.\n",
    "    \"\"\"\n",
    "    hierarchical_latent_dirichlet_allocation_topic_extraction()\n",
    "\n",
    "    ################################################\n",
    "    my_end_time = time.time()\n",
    "\n",
    "    time_elapsed_in_seconds = (my_end_time - my_start_time)\n",
    "    time_elapsed_in_minutes = (my_end_time - my_start_time) / 60.0\n",
    "    time_elapsed_in_hours = (my_end_time - my_start_time) / 60.0 / 60.0\n",
    "    print(f\"Time taken to process dataset: {time_elapsed_in_seconds} seconds, \"\n",
    "          f\"{time_elapsed_in_minutes} minutes, {time_elapsed_in_hours} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Extraction Results on Twitter Dataset Tweet Text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution run with recursive depth of 1 takes almost 4 hours to complete and provides just the results below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "(run 1)\n",
    "\n",
    "....................................................................................................100\n",
    "topic=0 level=0 (documents=653900): coal, $, job, 's, stop, project, want, í, tax, fund, \n",
    "\n",
    "....................................................................................................200\n",
    "topic=0 level=0 (documents=653900): coal, $, job, 's, stop, project, want, í, tax, fund, \n",
    "\n",
    "....................................................................................................300\n",
    "topic=0 level=0 (documents=653900): coal, $, job, 's, stop, project, want, í, tax, fund, \n",
    "\n",
    "....................................................................................................400\n",
    "topic=0 level=0 (documents=653900): coal, $, job, 's, stop, project, want, í, tax, fund, \n",
    "\n",
    "....................................................................................................500\n",
    "topic=0 level=0 (documents=653900): coal, $, job, 's, stop, project, want, í, tax, fund, \n",
    "\n",
    "\n",
    "\n",
    "Time taken to process dataset: 61687.211238861084 seconds, 1028.1201873143514 minutes, 17.13533645523919 hours.\n",
    "\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution run with recursive depth of 2 will not complete successfully.  This will require utilizing the Borg supercomputer with its much larger RAM capacity."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "(run 2)\n",
    "(below with recursive tree depth of 2 only)\n",
    "\n",
    "Process finished with exit code 137 (interrupted by signal 9: SIGKILL)\n",
    "\n",
    "- failed to produce any output whatsoever.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution run with recursive depth of 3 will not complete successfully.  This will require utilizing the Borg supercomputer with its much larger RAM capacity."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "(run 3)\n",
    "(below with recursive tree depth of 3 only)\n",
    "\n",
    "Process finished with exit code 137 (interrupted by signal 9: SIGKILL)\n",
    "\n",
    "- failed to produce any output whatsoever.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library for this topic model is extremely memory intensive.  This makes sense as it is a recursive algorithm that creates a branching tree-like hierarchy of topics from the root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://pypi.org/project/hlda/\n",
    "    - The Python library we utilize for HLDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
