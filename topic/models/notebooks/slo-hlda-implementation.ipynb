{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hierarchical Latent Dirichlet Allocation Topic Model Implementation on SLO Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our HLDA topic model utilizes a 3rd party library based on a Gibbs sampler ported over from the Java-based MALLET machine learning suite of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the requisite libraries, custom utility functions, and set the parameters for our various imported libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import logging as log\n",
    "import warnings\n",
    "import time\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import custom utility functions.\n",
    "import topic_extraction_utility_functions as lda_util\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Pandas options.\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "pd.options.display.max_colwidth = 1000\n",
    "# Pandas float precision display.\n",
    "pd.set_option('precision', 12)\n",
    "# Seaborn setting.\n",
    "sns.set()\n",
    "# Don't output these types of warnings to terminal.\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "# Matplotlib log settings.\n",
    "mylog = log.getLogger(\"matplotlib\")\n",
    "mylog.setLevel(log.INFO)\n",
    "\n",
    "\"\"\"\n",
    "Turn debug log statements for various sections of code on/off.\n",
    "(adjust log level as necessary)\n",
    "\"\"\"\n",
    "log.basicConfig(level=log.INFO)\n",
    "log.disable(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process and Post-process Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize our Tweets using our dataset preprocessing and postprocessing function.  Please refer to \"topic_extraction_utility_functions.py\" for the full code-base and comments on what is done to the Tweet text and user description text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on our topic modeling dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-test-subset-100-examples.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-lda-ready-tweet-text-with-hashtags-excluded-created-7-17-19.csv\",\n",
    "    \"text_derived\")\n",
    "\n",
    "# Test on our topic modeling dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-test-subset-100-examples.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-lda-ready-user-description-text-with-hashtags-excluded-created-7-17-19.csv\",\n",
    "    \"user_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and prepare the preprocessed dataset for use in HLDA topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the general format of insertion into a Pandas dataframe, isolating the column of interest, and generating a dictionary of words and corpus of documents.  Please refer to the code comments for details on the specific steps for the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the dataset (relative path).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"twitter-dataset-7-10-19-lda-ready-tweet-text-with-hashtags-excluded-created-7-17-19.csv\", sep=\",\")\n",
    "\n",
    "# Import the dataset (absolute path).\n",
    "tweet_dataset_processed = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "                \"twitter-dataset-7-10-19-lda-ready-tweet-text-with-hashtags-excluded-created-7-17-19.csv\", sep=\",\")\n",
    "\n",
    "# # Import the dataset (test/debug).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"twitter-dataset-7-10-19-lda-ready-tweet-text-test.csv\", sep=\",\")\n",
    "\n",
    "# # Import the dataset (test/debug).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "#                 \"twitter-dataset-7-10-19-lda-ready-tweet-text-test.csv\", sep=\",\")\n",
    "\n",
    "# Reindex and shuffle the data randomly.\n",
    "tweet_dataset_processed = tweet_dataset_processed.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed.index))\n",
    "\n",
    "# Generate a Pandas dataframe.\n",
    "tweet_text_dataframe = pd.DataFrame(tweet_dataset_processed)\n",
    "\n",
    "# # Print shape and column names.\n",
    "# log.info(f\"\\nThe shape of the Tweet text dataframe:\")\n",
    "# log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "# log.info(f\"\\nThe columns of the Tweet text dataframe:\")\n",
    "# log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(\"\\nThe shape of the Tweet text dataframe:\")\n",
    "log.info(tweet_text_dataframe.shape)\n",
    "log.info(\"\\nThe columns of the Tweet text dataframe:\")\n",
    "log.info(tweet_text_dataframe.columns)\n",
    "\n",
    "# Drop any NaN or empty Tweet rows in dataframe (or else CountVectorizer will blow up).\n",
    "tweet_text_dataframe = tweet_text_dataframe.dropna()\n",
    "\n",
    "# # Print shape and column names.\n",
    "# log.info(f\"\\nThe shape of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "# log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "# log.info(f\"\\nThe columns of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "# log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(\"\\nThe shape of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(tweet_text_dataframe.shape)\n",
    "log.info(\"\\nThe columns of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(tweet_text_dataframe.columns)\n",
    "\n",
    "# Reindex everything.\n",
    "tweet_text_dataframe.index = pd.RangeIndex(len(tweet_text_dataframe.index))\n",
    "\n",
    "# Assign column names.\n",
    "tweet_text_dataframe_column_names = ['text_derived', 'text_derived_preprocessed', 'text_derived_postprocessed']\n",
    "\n",
    "# Rename column in dataframe.\n",
    "tweet_text_dataframe.columns = tweet_text_dataframe_column_names\n",
    "\n",
    "# Create input feature.\n",
    "selected_features = tweet_text_dataframe[['text_derived_postprocessed']]\n",
    "processed_features = selected_features.copy()\n",
    "\n",
    "# # Check what we are using as inputs.\n",
    "# log.info(f\"\\nA sample Tweet in our input feature:\")\n",
    "# log.info(f\"{processed_features['text_derived_postprocessed'][0]}\\n\")\n",
    "\n",
    "# Check what we are using as inputs.\n",
    "log.info(\"\\nA sample Tweet in our input feature:\")\n",
    "log.info(processed_features['text_derived_postprocessed'][0])\n",
    "\n",
    "# Create feature set.\n",
    "slo_feature_series = processed_features['text_derived_postprocessed']\n",
    "slo_feature_series = pd.Series(slo_feature_series)\n",
    "slo_feature_list = slo_feature_series.tolist()\n",
    "\n",
    "#############################################################\n",
    "\n",
    "corpus = []\n",
    "dictionary = set()\n",
    "nlp = spacy.load('en')\n",
    "nlp.remove_pipe(\"parser\")\n",
    "nlp.remove_pipe(\"tagger\")\n",
    "nlp.remove_pipe(\"ner\")\n",
    "\n",
    "# Create the corpus of documents and dictionary of words (vocabulary)\n",
    "for tweet in slo_feature_list:\n",
    "    # Tokenize each Tweet (document) and add to List of documents in the corpus.\n",
    "    corpus.append(tweet.split())\n",
    "    # Tokenize each Tweet (document) and add individual words to the dictionary of words (vocabulary).\n",
    "    dictionary.update(tweet.split())\n",
    "\n",
    "# Attach indices to each word to represent their position in the dictionary of words (vocabulary).\n",
    "dictionary = sorted(list(dictionary))\n",
    "vocab_index = {}\n",
    "for i, w in enumerate(dictionary):\n",
    "    vocab_index[w] = i\n",
    "\n",
    "# print(f\"\\nThe number of documents: {len(slo_feature_list)}\")\n",
    "# print(f\"\\nThe number of words in the dictionary: {len(dictionary)}\")\n",
    "# print(f\"Sample of the words in the dictionary:\\n {dictionary[0:100]}\")\n",
    "# print(f\"\\nThe number of documents in the corpus: {len(corpus)}\")\n",
    "# print(f\"Sample of the documents in the corpus:\\n {corpus}\")\n",
    "\n",
    "print(\"\\nThe number of documents: \" + str(len(slo_feature_list)))\n",
    "print(\"\\nThe number of words in the dictionary: \" + str(len(dictionary)))\n",
    "print(\"Sample of the words in the dictionary:\\n \" + str(dictionary[0:100]))\n",
    "print(\"\\nThe number of documents in the corpus: \" + str(len(corpus)))\n",
    "print(\"Sample of the documents in the corpus:\\n \" + str(corpus))\n",
    "\n",
    "# Visualize the dictionary of words.\n",
    "wordcloud = WordCloud(background_color='white').generate(' '.join(slo_feature_list))\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLength of the dictionary, corpus, document 0 in corpus, document 1 in corpus (in that order)\")\n",
    "print(len(dictionary), len(corpus), len(corpus[0]), len(corpus[1]))\n",
    "\n",
    "\"\"\"\n",
    "Modify the corpus of documents to store the index value of each word from the dictionary (vocabulary)\n",
    "rather than the words themselves.\n",
    "\"\"\"\n",
    "new_corpus = []\n",
    "for document in corpus:\n",
    "    new_document = []\n",
    "    for word in document:\n",
    "        word_index = vocab_index[word]\n",
    "        new_document.append(word_index)\n",
    "    new_corpus.append(new_document)\n",
    "\n",
    "print(\"\\nLength of the dictionary and corpus (as word dictionary index values (in that order))\")\n",
    "print(len(dictionary), len(new_corpus))\n",
    "\n",
    "print(\"\\nDocument 0 in the corpus as tokenized words:\")\n",
    "print(corpus[0][0:10])\n",
    "print(\"Document 0 in the corpus as tokenized word index values from the dictionary:\")\n",
    "print(new_corpus[0][0:10])\n",
    "\n",
    "print(\"\\nDocument 1 in the corpus as tokenized words:\")\n",
    "print(corpus[1][0:10])\n",
    "print(\"Document 1 in the corpus as tokenized word index values from the dictionary:\")\n",
    "print(new_corpus[1][0:10])\n",
    "\n",
    "print(\"\\nDocument 2 in the corpus as tokenized words:\")\n",
    "print(corpus[2][0:10])\n",
    "print(\"Document 2 in the corpus as tokenized word index values from the dictionary:\")\n",
    "print(new_corpus[2][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function contains the code specific to each topic modeling library we utilize.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_latent_dirichlet_allocation_topic_extraction():\n",
    "    \"\"\"\n",
    "    Function performs topic extraction on Tweets using the Gensim HDP model.\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    from hlda.sampler import HierarchicalLDA\n",
    "\n",
    "    # Set parameters.\n",
    "    n_samples = 500  # no of iterations for the sampler\n",
    "    alpha = 10.0  # smoothing over level distributions\n",
    "    gamma = 1.0  # CRP smoothing parameter; number of imaginary customers at next, as yet unused table\n",
    "    eta = 0.1  # smoothing over topic-word distributions\n",
    "    num_levels = 3  # the number of levels in the tree\n",
    "    display_topics = 50  # the number of iterations between printing a brief summary of the topics so far\n",
    "    n_words = 5  # the number of most probable words to print for each topic after model estimation\n",
    "    with_weights = False  # whether to print the words with the weights\n",
    "\n",
    "    # Train the model.\n",
    "    hlda = HierarchicalLDA(new_corpus, dictionary, alpha=alpha, gamma=gamma, eta=eta, num_levels=num_levels)\n",
    "    hlda.estimate(n_samples, display_topics=display_topics, n_words=n_words, with_weights=with_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we call the topic modeling function and train it on our Twitter dataset.  We record the time it takes to process the entire dataset and extract topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main function.  Execute the program.\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    my_start_time = time.time()\n",
    "    ################################################\n",
    "    \"\"\"\n",
    "    Perform the topic extraction.\n",
    "    \"\"\"\n",
    "    hierarchical_latent_dirichlet_allocation_topic_extraction()\n",
    "\n",
    "    ################################################\n",
    "    my_end_time = time.time()\n",
    "\n",
    "    time_elapsed_in_seconds = (my_end_time - my_start_time)\n",
    "    time_elapsed_in_minutes = (my_end_time - my_start_time) / 60.0\n",
    "    time_elapsed_in_hours = (my_end_time - my_start_time) / 60.0 / 60.0\n",
    "    # print(f\"Time taken to process dataset: {time_elapsed_in_seconds} seconds, \"\n",
    "    #       f\"{time_elapsed_in_minutes} minutes, {time_elapsed_in_hours} hours.\")\n",
    "    print(\"\\n\\nTime taken to process dataset: \" + str(time_elapsed_in_seconds) + \" seconds, \" +\n",
    "          str(time_elapsed_in_minutes) + \" minutes, \" + str(time_elapsed_in_hours) + \" hours.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Extraction Results on Twitter Dataset Tweet Text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution run with recursive depth of 1 takes almost 4 hours to complete and provides just the results below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "(run 1)\n",
    "(below with recursive tree depth of 1 only)\n",
    "\n",
    "..................................................50\n",
    "topic=0 level=0 (documents=654070): adani, coal, santos, bhp, $, 's, job, australia, rio, project, \n",
    "\n",
    "..................................................100\n",
    "topic=0 level=0 (documents=654070): adani, coal, santos, bhp, $, 's, job, australia, rio, project, \n",
    "\n",
    "\n",
    "\n",
    "Time taken to process dataset: 13477.575538873672 seconds, 224.62625898122786 minutes, 3.743770983020464 hours.\n",
    "\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution run with recursive depth of 2 is still in-progress.  Will await results over the weekend."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "(run 2)\n",
    "(below with recursive tree depth of 2 only)\n",
    "\n",
    "- still in-progress.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution run with recursive depth of 3 will not complete successfully.  This will require utilizing the Borg supercomputer with its much larger RAM capacity."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "(run 3)\n",
    "(below with recursive tree depth of 3 only)\n",
    "\n",
    "- it failed to finish to completion.\n",
    "\n",
    "Process finished with exit code 137 (interrupted by signal 9: SIGKILL)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library for this topic model is extremely memory intensive.  This makes sense as it is a recursive algorithm that creates a branching tree-like hierarchy of topics from the root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://pypi.org/project/hlda/\n",
    "    - The Python library we utilize for HLDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
