{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Non-Negative Matrix Factorization Topic Model Implementation on SLO Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize the Scikit-Learn Non-negative Matrix Factorization model.  This is not a statistical model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the requisite libraries, custom utility functions, and set the parameters for our various imported libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import logging as log\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Import custom utility functions.\n",
    "import topic_extraction_utility_functions as topic_util\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Pandas options.\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "pd.options.display.max_colwidth = 1000\n",
    "# Pandas float precision display.\n",
    "pd.set_option('precision', 12)\n",
    "# Don't output these types of warnings to terminal.\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "# Matplotlib log settings.\n",
    "mylog = log.getLogger(\"matplotlib\")\n",
    "mylog.setLevel(log.INFO)\n",
    "\n",
    "\"\"\"\n",
    "Turn debug log statements for various sections of code on/off.\n",
    "(adjust log level as necessary)\n",
    "\"\"\"\n",
    "log.basicConfig(level=log.INFO)\n",
    "log.disable(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process and Post-process Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "We preprocess our Twitter dataset as follows:<br>\n",
    "\n",
    "1) Downcase all text.<br>\n",
    "2) Check that there is text, otherwise convert to empty string.<br>\n",
    "3) Convert html chars. to unicode chars.<br>\n",
    "4) Remove \"RT\" tags.<br>\n",
    "5) Remove concatenated URL's.<br>\n",
    "6) Handle whitespaces by converting all/multiple whitespace characters to a single whitespace.<br>\n",
    "7) Remove URL's and replace with \"slo_url\".<br>\n",
    "8) Remove Tweet mentions and replace with \"slo_mention\".<br>\n",
    "9) Remove Tweet stock symbols and replace with \"slo_stock\".<br>\n",
    "10) Remove Tweet hashtags and replace with \"slo_hash\".<br>\n",
    "11) Remove Tweet cashtags and replace with \"slo_cash\".<br>\n",
    "12) Remove Tweet year and replace with \"slo_year\".<br>\n",
    "13) Remove Tweet time and replace with \"slo_time\".<br>\n",
    "14) Remove character elongations.<br>\n",
    "\n",
    "We postprocess our Twitter dataset as follows:<br>\n",
    "\n",
    "1) Remove the following irrelevant words specified in the List below:<br>\n",
    "\n",
    "    delete_list = [\"word_n\", \"auspol\", \"ausbiz\", \"tinto\", \"adelaide\", \"csg\", \"nswpol\",\n",
    "                   \"nsw\", \"lng\", \"don\", \"rio\", \"pilliga\", \"australia\", \"asx\", \"just\", \"today\", \"great\", \"says\", \"like\",\n",
    "                   \"big\", \"better\", \"rite\", \"would\", \"SCREEN_NAME\", \"mining\", \"former\", \"qldpod\", \"qldpol\", \"qld\", \"wr\",\n",
    "                   \"melbourne\", \"andrew\", \"fuck\", \"spadani\", \"greg\", \"th\", \"australians\", \"http\", \"https\", \"rt\",\n",
    "                   \"co\", \"amp\", \"carmichael\", \"abbot\", \"bill shorten\",\n",
    "                   \"slo_url\", \"slo_mention\", \"slo_hash\", \"slo_year\", \"slo_time\", \"slo_cash\", \"slo_stock\",\n",
    "                   \"adani\", \"bhp\", \"cuesta\", \"fotescue\", \"riotinto\", \"newmontmining\", \"santos\", \"oilsearch\",\n",
    "                   \"woodside\", \"ilukaresources\", \"whitehavencoal\",\n",
    "                   \"stopadani\", \"goadani\", \"bhpbilliton\", \"billiton\", \"cuestacoal\", \"cuests coal\", \"cqc\",\n",
    "                   \"fortescuenews\", \"fortescue metals\", \"rio tinto\", \"newmont\", \"newmont mining\", \"santosltd\",\n",
    "                   \"oilsearchltd\", \"oil search\", \"woodsideenergy\", \"woodside petroleum\", \"woodside energy\",\n",
    "                   \"iluka\", \"iluka resources\", \"whitehaven\", \"whitehaven coal\"]\n",
    "\n",
    "2) Remove all punctuation from the Tweet text.<br>\n",
    "3) Remove all English stop words from the Tweet text.<br>\n",
    "4) Lemmatize the words in the Tweet text.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize using our Twitter dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-with-irrelevant-tweets-excluded.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-topic-extraction-ready-tweet-text-with-hashtags-excluded-created-7-29-19.csv\",\n",
    "    \"text_derived\")\n",
    "\n",
    "# Tokenize using our Twitter dataset.\n",
    "tweet_dataset_preprocessor(\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-with-irrelevant-tweets-excluded.csv\",\n",
    "    \"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "    \"twitter-dataset-7-10-19-topic-extraction-ready-user-description-text-with-hashtags-excluded-created-7-29-19.csv\",\n",
    "    \"user_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The first parameter in our function call specifies the file path to the dataset to be preprocessed.  The second parameter specifies the location to save the CSV file to.  The 3rd parameter specifies the name of the column in the dataset that contains the original Tweet text.<br>\n",
    "\n",
    "\n",
    "Tweet preprocessing is done via a custom library imported as \"lda_util\" using \"topic_extraction_utility_functions.py\".<br>\n",
    "\n",
    "Refer to URL link for the codebase to the utility functions used above for data preprocessing and below for LDA topic extraction:<br>\n",
    "\n",
    "https://github.com/Calvin-CS/slo-classifiers/blob/master/topic/models/topic_extraction_utility_functions.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and prepare the preprocessed dataset for use in NMF topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the general format of insertion into a Pandas dataframe, isolating the column of interest, and generating a dictionary of words and corpus of documents.  Please refer to the code comments for details on the specific steps for the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import the dataset (absolute path).\n",
    "tweet_dataset_processed = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "                \"twitter-dataset-7-10-19-topic-extraction-ready-tweet-text-with-hashtags-excluded\"\n",
    "                \"-created-7-29-19-tokenized.csv\", sep=\",\")\n",
    "\n",
    "# # Import the dataset (test/debug).\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"D:/Dropbox/summer-research-2019/jupyter-notebooks/attribute-datasets/\"\n",
    "#                 \"twitter-dataset-7-10-19-topic-extraction-ready-tweet-text-with-hashtags-excluded\"\n",
    "#                 \"-created-7-30-19-test.csv\", sep=\",\")\n",
    "\n",
    "# Reindex and shuffle the data randomly.\n",
    "tweet_dataset_processed = tweet_dataset_processed.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed.index))\n",
    "\n",
    "# Generate a Pandas dataframe.\n",
    "tweet_text_dataframe = pd.DataFrame(tweet_dataset_processed)\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(f\"\\nThe shape of the Tweet text dataframe:\")\n",
    "log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "log.info(f\"\\nThe columns of the Tweet text dataframe:\")\n",
    "log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Drop any NaN or empty Tweet rows in dataframe (or else CountVectorizer will blow up).\n",
    "tweet_text_dataframe = tweet_text_dataframe.dropna()\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(f\"\\nThe shape of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(f\"{tweet_text_dataframe.shape}\\n\")\n",
    "log.info(f\"\\nThe columns of the Tweet text dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(f\"{tweet_text_dataframe.columns}\\n\")\n",
    "\n",
    "# Reindex everything.\n",
    "tweet_text_dataframe.index = pd.RangeIndex(len(tweet_text_dataframe.index))\n",
    "\n",
    "# Assign column names.\n",
    "tweet_text_dataframe_column_names = ['text_derived', 'text_derived_preprocessed', 'text_derived_postprocessed']\n",
    "\n",
    "# Rename column in dataframe.\n",
    "tweet_text_dataframe.columns = tweet_text_dataframe_column_names\n",
    "\n",
    "# Create input feature.\n",
    "selected_features = tweet_text_dataframe[['text_derived_postprocessed']]\n",
    "processed_features = selected_features.copy()\n",
    "\n",
    "# Check what we are using as inputs.\n",
    "log.info(f\"\\nA sample Tweet in our input feature:\")\n",
    "log.info(f\"{processed_features['text_derived_postprocessed'][0]}\\n\")\n",
    "\n",
    "# Create feature set.\n",
    "slo_feature_series = processed_features['text_derived_postprocessed']\n",
    "slo_feature_series = pd.Series(slo_feature_series)\n",
    "slo_feature_list = slo_feature_series.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function contains the code specific to each topic modeling library we utilize.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def non_negative_matrix_factorization_topic_extraction():\n",
    "    \"\"\"\n",
    "    Function performs topic extraction on Tweets using Scikit-Learn NMF model.\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import NMF\n",
    "\n",
    "    # Use tf-idf features for NMF.\n",
    "    print(\"\\nExtracting tf-idf features for NMF...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "    tfidf = tfidf_vectorizer.fit_transform(slo_feature_series)\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    # Run NMF using Frobenius norm.\n",
    "    nmf_frobenius = NMF(n_components=20, random_state=1,\n",
    "                        alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "    # Run NMF using generalized Kullback-Leibler divergence.\n",
    "    nmf_kullback_leibler = NMF(n_components=20, random_state=1,\n",
    "                               beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "                               l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Display the top words for each topic.\n",
    "    print(\"\\nTopics using NMF Frobenius norm:\")\n",
    "    topic_util.display_topics(nmf_frobenius, tfidf_feature_names, 10)\n",
    "\n",
    "    # Display the top words for each topic.\n",
    "    print(\"\\nTopics using generalized Kullback-Leibler divergence:\")\n",
    "    topic_util.display_topics(nmf_kullback_leibler, tfidf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we call the topic modeling function and train it on our Twitter dataset.  We record the time it takes to process the entire dataset and extract topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main function.  Execute the program.\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    my_start_time = time.time()\n",
    "    ################################################\n",
    "    \"\"\"\n",
    "    Perform exhaustive grid search.\n",
    "    \"\"\"\n",
    "    # FIXME - non functional unless we find a way to disable cross-validation \"cv\" parameter in GridSearchCV Class.\n",
    "    # What parameters do we search for?\n",
    "    lda_search_parameters = {\n",
    "        'vect__strip_accents': [None],\n",
    "        'vect__lowercase': [True],\n",
    "        'vect__stop_words': ['english'],\n",
    "        # 'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "        'vect__analyzer': ['word'],\n",
    "        'vect__min_df': [2],\n",
    "        'vect__max_df': [0.95],\n",
    "        'vect__max_features': [1000],\n",
    "        'clf__n_components': [5, 10, 20],\n",
    "        'clf__init': ['random', 'nndsvd', 'nndsvda', 'nndsvdar'],\n",
    "        'clf__solver': ['cd', 'mu'],\n",
    "        'clf__beta_loss': ['frobenius', 'kullback-leibler', 'itakura-saito'],\n",
    "        'clf__tol': [1e-2, 1e-4, 1e-6],\n",
    "        'clf__max_iter': [100, 200, 300],\n",
    "        'clf__alpha': [0],\n",
    "        'clf__l1_ratio': [0],\n",
    "        'clf__verbose': [False],\n",
    "        'clf__shuffle': [False],\n",
    "        'clf__random_state': [None],\n",
    "    }\n",
    "    # topic_util.non_negative_matrix_factorization_grid_search(slo_feature_series, lda_search_parameters)\n",
    "    \"\"\"\n",
    "    Perform exhaustive grid search on data subset.\n",
    "    \"\"\"\n",
    "    # data_subset = topic_util.dataframe_subset(tweet_dataset_processed, 50)\n",
    "    # topic_util.non_negative_matrix_factorization_grid_search(data_subset, lda_search_parameters)\n",
    "\n",
    "    \"\"\"\n",
    "    Perform the topic extraction.\n",
    "    \"\"\"\n",
    "    # non_negative_matrix_factorization_topic_extraction()\n",
    "\n",
    "    ################################################\n",
    "    my_end_time = time.time()\n",
    "\n",
    "    time_elapsed_in_seconds = (my_end_time - my_start_time)\n",
    "    time_elapsed_in_minutes = (my_end_time - my_start_time) / 60.0\n",
    "    time_elapsed_in_hours = (my_end_time - my_start_time) / 60.0 / 60.0\n",
    "    print(f\"Time taken to process dataset: {time_elapsed_in_seconds} seconds, \"\n",
    "          f\"{time_elapsed_in_minutes} minutes, {time_elapsed_in_hours} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Extraction Results on Twitter Dataset Tweet Text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution run 1."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "(run 1)\n",
    "\n",
    "Topics using NMF Frobenius norm:\n",
    "Topic 0:\n",
    "coal india build seam power world kill point mega port\n",
    "Topic 1:\n",
    "loan naif rail slo_cashn veto taxpayer line money billion dollar\n",
    "Topic 2:\n",
    "job 000 create 10 thousand tourism lie claim renewable cut\n",
    "Topic 3:\n",
    "tax pay haven slo_cash company year ato corporate chevron profit\n",
    "Topic 4:\n",
    "gas seam narrabri field plan pipeline barnaby forest export joyce\n",
    "Topic 5:\n",
    "stop land destroy vote people green culture win work native\n",
    "Topic 6:\n",
    "time thank late sentinel come dear conservative remind pl daily\n",
    "Topic 7:\n",
    "reef barrier kill save destroy coral trust protect fight bleach\n",
    "Topic 8:\n",
    "labor support green vote lnp shorten election alp oppose party\n",
    "Topic 9:\n",
    "½í ¼í beach ½í² ¼í¼ ¾í look slo_hash day think\n",
    "Topic 10:\n",
    "want people know billion dollar look mega govt future win\n",
    "Topic 11:\n",
    "climate change action policy risk future title native world fight\n",
    "Topic 12:\n",
    "project narrabri ahead shorten mega win bank halt announce finance\n",
    "Topic 13:\n",
    "need india thing destroy culture land know good planet federal\n",
    "Topic 14:\n",
    "government queensland australian turnbull federal court approval point plan cut\n",
    "Topic 15:\n",
    "iron ore fortescue price share year metal news group production\n",
    "Topic 16:\n",
    "water farmer unlimited basin free year land risk artesian licence\n",
    "Topic 17:\n",
    "new plan open london report ceo protest day point break\n",
    "Topic 18:\n",
    "fund bank public rule coalmine taxpayer govt wo westpac infrastructure\n",
    "Topic 19:\n",
    "slo_mention think company tell stand thousand good way thing risk\n",
    "\n",
    "Topics using generalized Kullback-Leibler divergence:\n",
    "Topic 0:\n",
    "coal build away indian massive environment india mean minister power\n",
    "Topic 1:\n",
    "tell say turnbull way ask make question happen try issue\n",
    "Topic 2:\n",
    "job 000 10 create claim lie thousand tourism renewable pm\n",
    "Topic 3:\n",
    "australian company oil face govt financial use fail write set\n",
    "Topic 4:\n",
    "project gas narrabri land forest farmer seam sign community field\n",
    "Topic 5:\n",
    "stop tax pay profit million corporate haven office rate island\n",
    "Topic 6:\n",
    "time thank good come late stand start leave week long\n",
    "Topic 7:\n",
    "reef barrier fight help kill destroy risk protect let save\n",
    "Topic 8:\n",
    "labor support green vote lnp win election shorten alp party\n",
    "Topic 9:\n",
    "½í beach ¼í read love ½í² slo_hash letter oh ¾í\n",
    "Topic 10:\n",
    "want people know billion future dollar listen industry mega video\n",
    "Topic 11:\n",
    "day action protest court group right join wrong native meet\n",
    "Topic 12:\n",
    "climate change energy bank clean policy demand chief carbon fact\n",
    "Topic 13:\n",
    "need money look india deal thing barnaby loan joyce slo_cashn\n",
    "Topic 14:\n",
    "government news break report point ceo business cut giant cost\n",
    "Topic 15:\n",
    "year plan work world power dam big resource china large\n",
    "Topic 16:\n",
    "water queensland basin farmer approval ahead galilee environmental free licence\n",
    "Topic 17:\n",
    "new loan rail line taxpayer naif poll minister open veto\n",
    "Topic 18:\n",
    "fund govt coalmine federal wo state rule huge bank decision\n",
    "Topic 19:\n",
    "slo_mention fortescue price company think share iron ore high sell\n",
    "\n",
    "\n",
    "Time taken to process dataset: 273.3467524051666 seconds, 4.555779206752777 minutes, 0.07592965344587961 hours.\n",
    "\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution run 2."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "(run 2)\n",
    "\n",
    "Topics using NMF Frobenius norm:\n",
    "Topic 0:\n",
    "coal india build seam power world kill point mega port\n",
    "Topic 1:\n",
    "loan naif rail slo_cashn veto taxpayer line money billion dollar\n",
    "Topic 2:\n",
    "job 000 create 10 thousand tourism lie claim renewable cut\n",
    "Topic 3:\n",
    "tax pay haven slo_cash company year ato corporate chevron profit\n",
    "Topic 4:\n",
    "gas seam narrabri field plan pipeline barnaby forest export joyce\n",
    "Topic 5:\n",
    "stop land destroy vote people green culture win work native\n",
    "Topic 6:\n",
    "time thank late sentinel come dear conservative remind pl daily\n",
    "Topic 7:\n",
    "reef barrier kill save destroy coral trust protect fight bleach\n",
    "Topic 8:\n",
    "labor support green vote lnp shorten election alp oppose party\n",
    "Topic 9:\n",
    "½í ¼í beach ½í² ¼í¼ ¾í look slo_hash day think\n",
    "Topic 10:\n",
    "want people know billion dollar look mega govt future win\n",
    "Topic 11:\n",
    "climate change action policy risk future title native world fight\n",
    "Topic 12:\n",
    "project narrabri ahead shorten mega win bank halt announce finance\n",
    "Topic 13:\n",
    "need india thing destroy culture land know good planet federal\n",
    "Topic 14:\n",
    "government queensland australian turnbull federal court approval point plan cut\n",
    "Topic 15:\n",
    "iron ore fortescue price share year metal news group production\n",
    "Topic 16:\n",
    "water farmer unlimited basin free year land risk artesian licence\n",
    "Topic 17:\n",
    "new plan open london report ceo protest day point break\n",
    "Topic 18:\n",
    "fund bank public rule coalmine taxpayer govt wo westpac infrastructure\n",
    "Topic 19:\n",
    "slo_mention think company tell stand thousand good way thing risk\n",
    "\n",
    "Topics using generalized Kullback-Leibler divergence:\n",
    "Topic 0:\n",
    "coal build away indian massive minister india power mean solar\n",
    "Topic 1:\n",
    "tell turnbull say way ask happen question make try council\n",
    "Topic 2:\n",
    "job 000 10 create claim lie thousand tourism pm renewable\n",
    "Topic 3:\n",
    "australian company oil face govt write fail financial use set\n",
    "Topic 4:\n",
    "project gas narrabri land forest farmer sign seam field community\n",
    "Topic 5:\n",
    "stop tax pay profit million corporate haven office rate island\n",
    "Topic 6:\n",
    "time thank good come late stand start leave week end\n",
    "Topic 7:\n",
    "reef barrier fight help kill destroy risk protect save let\n",
    "Topic 8:\n",
    "labor support vote lnp win green election alp party shorten\n",
    "Topic 9:\n",
    "½í beach ½í² ¼í read love slo_hash ¾í letter oh\n",
    "Topic 10:\n",
    "want people know billion future dollar mega listen industry video\n",
    "Topic 11:\n",
    "day action protest right group court join wrong local native\n",
    "Topic 12:\n",
    "climate change energy bank clean policy demand chief carbon fuel\n",
    "Topic 13:\n",
    "need money look india deal barnaby thing joyce slo_cashn loan\n",
    "Topic 14:\n",
    "government news break report point ceo business cut close giant\n",
    "Topic 15:\n",
    "year plan work world power dam big china resource large\n",
    "Topic 16:\n",
    "water queensland basin farmer approval ahead environmental galilee free licence\n",
    "Topic 17:\n",
    "new loan rail line taxpayer naif minister poll open veto\n",
    "Topic 18:\n",
    "fund coalmine govt federal green wo rule state bank decision\n",
    "Topic 19:\n",
    "slo_mention fortescue price company think share iron ore high sell\n",
    "\n",
    "\n",
    "Time taken to process dataset: 271.20036005973816 seconds, 4.520006000995636 minutes, 0.07533343334992727 hours.\n",
    "\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the fastest of our topic modeling algorithms.  Most of the time was spend generating term-frequencies for the entire dataset.  The output shows the results for two different types of beta-divergence Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "    - Scikit-Learn example of NMF and LDA.\n",
    "    \n",
    "    \n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF\n",
    "    - Scikit-Learn API documentation on NMF.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
