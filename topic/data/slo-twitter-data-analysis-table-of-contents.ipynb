{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {}
   },
   "source": [
    "# SLO Twitter Data Analysis - Table of Contents\n",
    "\n",
    "## Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "\n",
    "This collection of Jupyter Notebook files provide an analysis of Twitter data obtained by CSIRO Data61 from a period of time covering 2010 through 2018.  The Twitter API was used to extract the raw Tweet data.  The sections below provide a short summary and hyperlinks to individual Jupyter Notebook files that provide further details on our analysis of different attributes and combinations of attributes in our Twitter Dataset.\n",
    "\n",
    "Note: Irrelevant Tweets are dropped and not included as part of the dataset for this analysis.  This means any Tweet that is non-English as agreed upon by the Twitter API and \"spacy-langdetect\" language detection library was dropped from our dataset and any Tweet not associated with a SLO company was also dropped from our dataset.  \n",
    "\n",
    "We have 654,618 Tweets left in the dataset out of the total of 670,423.  We dropped 15,805 Tweets as irrelevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Raw Tweets are represented using an hierarchical JSON file structure that contains nested sub-objects for the \"entities\", \"user\", and \"retweeted_status\" each of which can, themselves, contain further nested sub-objects.  The attributes we currently consider of the utmost important to our research are as follows:\n",
    "\n",
    "- *retweeted_derived*: indicates whether the Tweet is a retweet.\n",
    "- *company_derived*: associates the Tweet with a company.\n",
    "- *text_derived*: the full Tweet text.\n",
    "- *tweet_url_link_derived*: hyperlink to the actual Tweet on Twitter.\n",
    "- *company_derived_designation*: single company Tweet is associated with or \"multiple\" for multi-company.\n",
    "- *tweet_text_length_derived*: character count of the Tweet text length.\n",
    "- *spaCy_language_detect*: language of the Tweet as determined by \"spacy-langdetect\" python library.\n",
    "- *tweet_created_at*: time-date stamp of the Tweet.\n",
    "- *tweet_id*: unique ID # of the Tweet.\n",
    "- *tweet_retweet_count*: the # of times the Tweet has been retweeted.\n",
    "- *user_id*: unique ID of the user (Tweet author).\n",
    "- *user_description*: short user written description of himself/herself.\n",
    "- *tweet_entities_hashtags*: hashtags present in the Tweet text.\n",
    "- *tweet_entities_user_mentions_id*: unique ID of the users mentioned in a Tweet.\n",
    "- *retweeted_status_full_text*: full text of the original retweeted Tweet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "For more details, see this [Twitter API Introduction](notebooks/slo-twitter-data-analysis-tweet-api-intro.ipynb#bookmark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Codebase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "\n",
    "\n",
    "The data analysis done in the following sections is based on a set of function libraries. These libraries include functions that process the raw JSON data and convert it into CSV format, libraries that load this processed dataset, and libraries that perform relevant data analysis.  It takes approximately 40 hours to create a processed CSV dataset from the raw JSON data.  About 2/3 of the required time is due to language detection using the \"spaCy-langdetect\" library.\n",
    "\n",
    "- `dataset_processor_adapted.py` reads in the raw JSON file structure by chunks and parses through it to derive and extract various attributes of the Tweets that we are interested in.  We export the results to a CSV dataset file. This is done once, at the very beginning of the analysis.\n",
    " \n",
    "- `slo_twitter_data_analysis.py` contains all the analysis code we use in this collection of Jupyter Notebook files.\n",
    "\n",
    "- `slo_twitter_data_analysis_utility_functions.py` contains utility functions used by the analysis functions. These functions include: graphing helper functions; functions that extract existing attributes and derive new attributes.  This allows us to readily manipulate our dataset in a computationally inexpensive manner (requires less time).\n",
    "\n",
    "Each of the following sections imports the necessary libraries, configures the settings for those libraries, loads the processed CSV format, and then performs the required data analysis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "\n",
    "For more details, see this [Codebase Introduction](notebooks/slo-twitter-data-analysis-codebase-intro.ipynb#bookmark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Tweet Language Statistics and Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "\n",
    "We accept as English anything marked as such by either Twitter or spaCy.  We reject as non-English anything marked as such by Twitter and spaCy.  Consequently, we keep 100.00% of our Tweets as English and discard 0.00% of our Tweets as non-English.  Non-English Tweets will only increase the difficulty of extracting coherent topics and as they comprise only a small portion of our dataset, we feel that dropping them from the dataset is appropriate.  The language detection results were obtained using the native Twitter API \"lang\" attribute field and the \"spacy-langdetect\" library built on top of \"spaCy\" and \"langdetect\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "For more details, see this [Language Statistics](notebooks/slo-twitter-data-analysis-language-statistics.ipynb#bookmark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Company Assignments for Tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "\n",
    "A feature is added to all tweets indicating the company that is the subject of the tweet. 98.75% of tweets designate a single company; 1.25% mention multiple companies. To these latter tweets, we assigned a \"multiple\" company subject. We decided to retain these multiple-company tweets because they contain useful information on mining in general. Admittedly, 1.49% of the multiple company tweets are not that interesting, e.g., \n",
    "\n",
    "> Rio Tinto slumped 2.7%, BHP dropped 2.2% and Woodside lost 1.4% in the 20 leaders, while outside the heavyweights, Iluka Resources sank dived 4.3%, Sandfire slumped 4.2%, Beach Energy sank 3.6%, Fortescue Metals dropped 2.9% and Santos fell 2.2%. #ASX\" stock info tweets for the energy sector.\n",
    "\n",
    "But others are more useful, e.g., \n",
    "\n",
    "> @43a6f0ce5dac4ea @takvera @billshortenmp @stopadani @GalileeBlockade @350Melbourne Nobody is making Noise about Rio Tinto, BHP, Gina who have Polluted &amp; Purged Australia for their Greed with Bribes to both Labor &amp; LNP\" individual tweets about the industry in general.\n",
    "    \n",
    "The analysis sections below will generally show analysis for aggregate, company-specific and multiple company tweets.  As a result of this analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "\n",
    "For more details, see this [Single v. Multi Company Associated Tweets](notebooks/slo-twitter-data-analysis-one-versus-multiple-companies-statistics.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Time Series Statistics and Analysis:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "\n",
    "\n",
    "64.84% of our Tweets are associated with \"Adani\" and 85.73% of those are in the 2017-2018 time period.  The Tweets for the other companies in our dataset are far less numerous.  Fortescue, Iluka, Newmont, and Woodside show a relatively even distribution across the years.  Multi-company Tweets, BHP, Oilsearch, and Riotinto show a distribution somewhat skewed to the right (indicating newer Tweets).  Cuesta and Whitehaven show a distribution somewhat skewed to the left (indicating older Tweets).  There are also some spikes during certain periods where many Tweets were made for a particular company, perhaps indicating some sort of event occurred.\n",
    "\n",
    "For now, we have lumped the time-date stamp across the years all together for our analysis though we could consider bucketing the data by time blocks to see a flow of topics over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "For more details, see this [Time Series Statistics](notebooks/slo-twitter-data-analysis-time-statistics.ipynb#bookmark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Retweet Statistics and Analysis:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "\n",
    "\n",
    "67.30% of our dataset are ReTweets while the other 32.70% are original tweets.  Most Tweets associated with \"Adani\" are ReTweets.  We see a more even distribution between ReTweets and non-ReTweets for the other companies.  Again, \"Adani\" Tweets heavily influence the distribution of our statistics and graphs.  Of the 440,548 ReTweets, we possess the orignal text of the ReTweeted Tweet for 439,913 of them.  We do not have the original text for 635 of them.  There are also a few orginal ReTweeted Tweets whose ReTweet counts are extremely high, with 98,886 being the highest.\n",
    "\n",
    "We hope to experiment with different strategies, e.g., remove all retweets; use retweet counts to determine \"influencers\"; compute votes for/against original posts via stance analysis of retweet sets for individual tweets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "For more details, see this [Retweet Statistics](notebooks/slo-twitter-data-analysis-retweet-statistics.ipynb#bookmark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## User (Tweet Author) Statistics and Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "\n",
    "\n",
    "At a unique author count of 36,637, we have far fewer unique authors than we do for the total of 654,618 Tweets in the dataset.  From the stats, we see that the Twitter users that are actually (neutral) news outlet or organizations are responsible for the majority of Tweets and ReTweets.  We also found that 371,363 of the 654,618 Tweet texts are over 140 characters long while only 251,980 user description texts are over 140 characters long.\n",
    "\n",
    "We will consider bucketing tweets based on rule-based stance assignments (see `autocoding-preprocessor.py`). We could throw out neutral tweets. We could to a network analysis of authors to compute \"influencers\", \"new\" ideas. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "For more details, see this [User (Tweet Author) Statistics](notebooks/slo-twitter-data-analysis-user-statistics.ipynb#bookmark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Tweet Statistics and Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "\n",
    "\n",
    "Many Tweet texts are longer than the max 140 characters; most are under 300 characters.  These extended-length tweets may be due to the fact that `dataset_processor_adapted.py` adds the full ReTweeted text to the \"text_derived\" field we derive for our dataset, or there may be encoding issues for non-English Tweets.  Further analysis is necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "For more details, see this [Tweet Text Statistics](notebooks/slo-twitter-data-analysis-tweet-text-statistics.ipynb#bookmark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## #Hashtag Statistics and Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "\n",
    "\n",
    "174,037 of the 424,437 (41.00%) of \"Adani\" Tweets do not have any hashtags while the rest have at least one hashtag.  359,715  (54.95%) Tweets have at least one hashtag.\n",
    "\n",
    "We leave the hashtags in the text for topic analysis because they can provide useful indicators as to the stance/sentiment of the Tweet towards the company.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "For more details, see this [Hashtags Statistics](notebooks/slo-twitter-data-analysis-hashtag-statistics.ipynb#bookmark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## @User Mentions Statistics and Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "\n",
    "    \n",
    "80.37% of all Tweets in our dataset have @user mentions.  Only 5.67% of all Tweets are replies to other Tweets in our dataset.  Most Tweets have at most a single user mention.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "For more details, see this [User Mentions Statistics](notebooks/slo-twitter-data-analysis-mentions-statistics.ipynb#bookmark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Tweet Stock Symbols, URL's, and Emojis Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "\n",
    "\n",
    "Only 10.52% of all the Tweets in our dataset possess stock symbols of some sort; 77.84% possess URL's; only 0.21% have emoticons of some sort.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "For more details, see this [Stock Symbols, URL's, Emojis - Statistics](notebooks/slo-twitter-data-analysis-stock-symbols-and-urls-emojis.ipynb#bookmark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Nan/Non-NaN Values Statistics and Analysis (will be deprecated in near-future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Very few fields in the dataset have null or NaN values. Those that do tend to be optional field, e.g., in-reply-to (only for replies); user location/description; hashtags; user mentions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "For more details, see this [Nan/Non-NaN Statistics](notebooks/slo-twitter-data-analysis-other-statistics.ipynb#bookmark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Resources Referenced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "\n",
    "\n",
    "Dataset File (obtained from CSIRO's real-time Twitter data feed): `dataset_slo_20100101-20180510.json` (not included in the project GitHub Repository)\n",
    "\n",
    "References:\n",
    "\n",
    "- [SLO-analysis.ipynb](notebooks/slo-analysis.ipynb): \n",
    "    original SLO Twitter data analysis file from Shuntaro Yada\n",
    "    \n",
    "- https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/intro-to-tweet-json: explanation of all data fields in JSON file format for Tweets\n",
    "\n",
    "- https://datatofish.com/export-dataframe-to-csv/\n",
    "\n",
    "- https://datatofish.com/export-pandas-dataframe-json/: \n",
    "    saving Pandas dataframe to CSV/JSON\n",
    "    \n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html: \n",
    "    Pandas to_datetime() function call\n",
    "    \n",
    "- https://www.machinelearningplus.com/plots/matplotlib-tutorial-complete-guide-python-plot-examples/: \n",
    "    plotting with matplotlib.\n",
    "    \n",
    "- https://stackoverflow.com/questions/49566007/jupyter-multiple-notebooks-using-same-data\n",
    "\n",
    "- https://stackoverflow.com/questions/16966280/reusing-code-from-different-ipython-notebooks: \n",
    "    sharing kernels and code across multiple Jupyter notebook files\n",
    "    \n",
    "- https://stackoverflow.com/questions/32370281/how-to-embed-image-or-picture-in-jupyter-notebook-either-from-a-local-machine-o: \n",
    "    displaying embedded images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
